<!DOCTYPE html>
<html lang="ko"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>ëª¨ë¸ ì°¸ì¡°(Model Reference) Â· ğŸ¦‰</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link href="../../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../../assets/custom.css" rel="stylesheet" type="text/css"/><link href="../../../assets/custom.css" rel="stylesheet" type="text/css"/><script src="/js/jquery-1.8.3.min.js"></script><script src="/js/jquery.word-break-keep-all.min.js"></script><script>$(document).ready(function() { $('p').wordBreakKeepAll(); });</script></head><body><nav class="toc"><h1>ğŸ¦‰</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../../">Home</a></li><li><span class="toctext">Flux âœ…</span><ul><li><a class="toctext" href="../../">Flux í™ˆ</a></li><li><span class="toctext">ëª¨ë¸ ë§Œë“¤ê¸°</span><ul><li><a class="toctext" href="../basics/">ê¸°ë³¸ì ì¸ ê²ƒ</a></li><li><a class="toctext" href="../recurrence/">ìˆœí™˜(Recurrence)</a></li><li><a class="toctext" href="../regularisation/">ì •ê·œí™”(Regularisation)</a></li><li class="current"><a class="toctext" href>ëª¨ë¸ ì°¸ì¡°(Model Reference)</a><ul class="internal"><li><a class="toctext" href="#ê¸°ë³¸-ë ˆì´ì–´-1">ê¸°ë³¸ ë ˆì´ì–´</a></li><li><a class="toctext" href="#Convolution-and-Pooling-Layers-1">Convolution and Pooling Layers</a></li><li><a class="toctext" href="#ìˆœí™˜-ë ˆì´ì–´(Recurrent-Layers)-1">ìˆœí™˜ ë ˆì´ì–´(Recurrent Layers)</a></li><li><a class="toctext" href="#Other-General-Purpose-Layers-1">Other General Purpose Layers</a></li><li><a class="toctext" href="#í™œì„±-í•¨ìˆ˜(Activation-Functions)-1">í™œì„± í•¨ìˆ˜(Activation Functions)</a></li><li><a class="toctext" href="#ì •ìƒí™”(Normalisation)-and-ì •ê·œí™”(Regularisation)-1">ì •ìƒí™”(Normalisation) &amp; ì •ê·œí™”(Regularisation)</a></li></ul></li></ul></li><li><span class="toctext">ëª¨ë¸ í›ˆë ¨ì‹œí‚¤ê¸°</span><ul><li><a class="toctext" href="../../training/optimisers/">ìµœì í™”</a></li><li><a class="toctext" href="../../training/training/">í›ˆë ¨ì‹œí‚¤ê¸°</a></li></ul></li><li><a class="toctext" href="../../data/onehot/">ì›-í•« ì¸ì½”ë”©</a></li><li><a class="toctext" href="../../gpu/">GPU ì§€ì›</a></li><li><a class="toctext" href="../../saving/">ì €ì¥ &amp; ë¶ˆëŸ¬ì˜¤ê¸°</a></li><li><a class="toctext" href="../../community/">ì»¤ë®¤ë‹ˆí‹°</a></li></ul></li><li><span class="toctext">DataFlow âœ…</span><ul><li><a class="toctext" href="../../../DataFlow/vertices/">DataFlow ë²„í‹°ìŠ¤(vertices)</a></li></ul></li><li><span class="toctext">Zygote â³</span><ul><li><a class="toctext" href="../../../Zygote/">Home</a></li><li><a class="toctext" href="../../../Zygote/adjoints/">Custom Adjoints</a></li><li><a class="toctext" href="../../../Zygote/utils/">Utilities</a></li><li><a class="toctext" href="../../../Zygote/complex/">Complex Differentiation</a></li><li><a class="toctext" href="../../../Zygote/flux/">Flux</a></li><li><a class="toctext" href="../../../Zygote/profiling/">Profiling</a></li><li><a class="toctext" href="../../../Zygote/internals/">Internals</a></li><li><a class="toctext" href="../../../Zygote/glossary/">Glossary</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Flux âœ…</li><li>ëª¨ë¸ ë§Œë“¤ê¸°</li><li><a href>ëª¨ë¸ ì°¸ì¡°(Model Reference)</a></li></ul><a class="edit-page" href="https://github.com/wookay/Owl.jl/blob/master/docs/src/Flux/models/layers.md"><span class="fa">ï‚›</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>ëª¨ë¸ ì°¸ì¡°(Model Reference)</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="ê¸°ë³¸-ë ˆì´ì–´-1" href="#ê¸°ë³¸-ë ˆì´ì–´-1">ê¸°ë³¸ ë ˆì´ì–´</a></h2><p>ê±°ì˜ ëª¨ë“  ì‹ ê²½ë§(neural networks)ì˜ í† ëŒ€ë¥¼ ë‹¤ìŒì˜ í•µì‹¬ ë ˆì´ì–´ë¡œ êµ¬ì„±í•œë‹¤.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.Chain" href="#Flux.Chain"><code>Flux.Chain</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Chain(layers...)</code></pre><p>Chain multiple layers / functions together, so that they are called in sequence on a given input.</p><pre><code class="language-julia">m = Chain(x -&gt; x^2, x -&gt; x+1)
m(5) == 26

m = Chain(Dense(10, 5), Dense(5, 2))
x = rand(10)
m(x) == m[2](m[1](x))</code></pre><p><code>Chain</code> also supports indexing and slicing, e.g. <code>m[2]</code> or <code>m[1:end-1]</code>. <code>m[1:3](x)</code> will calculate the output of the first three layers.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/basic.jl#L1-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.Dense" href="#Flux.Dense"><code>Flux.Dense</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Dense(in::Integer, out::Integer, Ïƒ = identity)</code></pre><p>Creates a traditional <code>Dense</code> layer with parameters <code>W</code> and <code>b</code>.</p><pre><code class="language-none">y = Ïƒ.(W * x .+ b)</code></pre><p>The input <code>x</code> must be a vector of length <code>in</code>, or a batch of vectors represented as an <code>in Ã— N</code> matrix. The out <code>y</code> will be a vector or batch of length <code>out</code>.</p><pre><code class="language-julia">julia&gt; d = Dense(5, 2)
Dense(5, 2)

julia&gt; d(rand(5))
Tracked 2-element Array{Float64,1}:
  0.00257447
  -0.00449443</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/basic.jl#L62-L81">source</a></section><h2><a class="nav-anchor" id="Convolution-and-Pooling-Layers-1" href="#Convolution-and-Pooling-Layers-1">Convolution and Pooling Layers</a></h2><p>These layers are used to build convolutional neural networks (CNNs).</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.Conv" href="#Flux.Conv"><code>Flux.Conv</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Conv(size, in=&gt;out)
Conv(size, in=&gt;out, relu)</code></pre><p>Standard convolutional layer. <code>size</code> should be a tuple like <code>(2, 2)</code>. <code>in</code> and <code>out</code> specify the number of input and output channels respectively.</p><p>Example: Applying Conv layer to a 1-channel input using a 2x2 window size,          giving us a 16-channel output. Output is activated with ReLU.</p><pre><code class="language-none">size = (2,2)
in = 1
out = 16 
Conv((2, 2), 1=&gt;16, relu)</code></pre><p>Data should be stored in WHCN order (width, height, # channels, # batches).  In other words, a 100Ã—100 RGB image would be a <code>100Ã—100Ã—3Ã—1</code> array,  and a batch of 50 would be a <code>100Ã—100Ã—3Ã—50</code> array.</p><p>Takes the keyword arguments <code>pad</code>, <code>stride</code> and <code>dilation</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/conv.jl#L5-L25">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.MaxPool" href="#Flux.MaxPool"><code>Flux.MaxPool</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">MaxPool(k)</code></pre><p>Max pooling layer. <code>k</code> stands for the size of the window for each dimension of the input.</p><p>Takes the keyword arguments <code>pad</code> and <code>stride</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/conv.jl#L202-L208">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.MeanPool" href="#Flux.MeanPool"><code>Flux.MeanPool</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">MeanPool(k)</code></pre><p>Mean pooling layer. <code>k</code> stands for the size of the window for each dimension of the input.</p><p>Takes the keyword arguments <code>pad</code> and <code>stride</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/conv.jl#L231-L237">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.DepthwiseConv" href="#Flux.DepthwiseConv"><code>Flux.DepthwiseConv</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">DepthwiseConv(size, in)
DepthwiseConv(size, in=&gt;mul)
DepthwiseConv(size, in=&gt;mul, relu)</code></pre><p>Depthwise convolutional layer. <code>size</code> should be a tuple like <code>(2, 2)</code>. <code>in</code> and <code>mul</code> specify the number of input channels and channel multiplier respectively. In case the <code>mul</code> is not specified it is taken as 1.</p><p>Data should be stored in WHCN order. In other words, a 100Ã—100 RGB image would be a <code>100Ã—100Ã—3</code> array, and a batch of 50 would be a <code>100Ã—100Ã—3Ã—50</code> array.</p><p>Takes the keyword arguments <code>pad</code> and <code>stride</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/conv.jl#L138-L151">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.ConvTranspose" href="#Flux.ConvTranspose"><code>Flux.ConvTranspose</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">ConvTranspose(size, in=&gt;out)
ConvTranspose(size, in=&gt;out, relu)</code></pre><p>Standard convolutional transpose layer. <code>size</code> should be a tuple like <code>(2, 2)</code>. <code>in</code> and <code>out</code> specify the number of input and output channels respectively. Data should be stored in WHCN order. In other words, a 100Ã—100 RGB image would be a <code>100Ã—100Ã—3</code> array, and a batch of 50 would be a <code>100Ã—100Ã—3Ã—50</code> array. Takes the keyword arguments <code>pad</code>, <code>stride</code> and <code>dilation</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/conv.jl#L71-L80">source</a></section><h2><a class="nav-anchor" id="ìˆœí™˜-ë ˆì´ì–´(Recurrent-Layers)-1" href="#ìˆœí™˜-ë ˆì´ì–´(Recurrent-Layers)-1">ìˆœí™˜ ë ˆì´ì–´(Recurrent Layers)</a></h2><p>ìœ„ì˜ í•µì‹¬ ë ˆì´ì–´ì™€ í•¨ê»˜, ì‹œí€€ìŠ¤ ë°ì´í„°(ë‹¤ë¥¸ ì¢…ë¥˜ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°)ë¥¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.RNN" href="#Flux.RNN"><code>Flux.RNN</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">RNN(in::Integer, out::Integer, Ïƒ = tanh)</code></pre><p>The most basic recurrent layer; essentially acts as a <code>Dense</code> layer, but with the output fed back into the input each time step.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/recurrent.jl#L105-L110">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.LSTM" href="#Flux.LSTM"><code>Flux.LSTM</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">LSTM(in::Integer, out::Integer)</code></pre><p>Long Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences.</p><p>See <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">this article</a> for a good overview of the internals.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/recurrent.jl#L150-L158">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.GRU" href="#Flux.GRU"><code>Flux.GRU</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">GRU(in::Integer, out::Integer)</code></pre><p>Gated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences.</p><p>See <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">this article</a> for a good overview of the internals.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/recurrent.jl#L191-L199">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.Recur" href="#Flux.Recur"><code>Flux.Recur</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Recur(cell)</code></pre><p><code>Recur</code> takes a recurrent cell and makes it stateful, managing the hidden state in the background. <code>cell</code> should be a model of the form:</p><pre><code class="language-none">h, y = cell(h, x...)</code></pre><p>For example, here&#39;s a recurrent network that keeps a running total of its inputs.</p><pre><code class="language-julia">accum(h, x) = (h+x, x)
rnn = Flux.Recur(accum, 0)
rnn(2) # 2
rnn(3) # 3
rnn.state # 5
rnn.(1:10) # apply to a sequence
rnn.state # 60</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/recurrent.jl#L7-L26">source</a></section><h2><a class="nav-anchor" id="Other-General-Purpose-Layers-1" href="#Other-General-Purpose-Layers-1">Other General Purpose Layers</a></h2><p>These are marginally more obscure than the Basic Layers. But in contrast to the layers described in the other sections are not readily grouped around a particular purpose (e.g. CNNs or RNNs).</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.Maxout" href="#Flux.Maxout"><code>Flux.Maxout</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Maxout(over)</code></pre><p><code>Maxout</code> is a neural network layer, which has a number of internal layers, which all have the same input, and the maxout returns the elementwise maximium of the internal layers&#39; outputs.</p><p>Maxout over linear dense layers satisfies the univeral approximation theorem.</p><p>Reference: Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.</p><ol><li>Maxout networks.</li></ol><p>In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML&#39;13), Sanjoy Dasgupta and David McAllester (Eds.), Vol. 28. JMLR.org III-1319-III-1327. https://arxiv.org/pdf/1302.4389.pdf</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/basic.jl#L146-L161">source</a></section><h2><a class="nav-anchor" id="í™œì„±-í•¨ìˆ˜(Activation-Functions)-1" href="#í™œì„±-í•¨ìˆ˜(Activation-Functions)-1">í™œì„± í•¨ìˆ˜(Activation Functions)</a></h2><p>ëª¨ë¸ì˜ ë ˆì´ì–´ ì¤‘ê°„ì— ë¹„ì„ í˜•ì„±(Non-linearities)ì„ ê°–ì„ ë•Œ ì‚¬ìš©í•œë‹¤. í•¨ìˆ˜ì˜ ëŒ€ë¶€ë¶„ì€ <a href="https://github.com/FluxML/NNlib.jl">NNlib</a>ì— ì •ì˜ë˜ì–´ ìˆê³  Fluxì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p><p>íŠ¹ë³„í•œ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ í™œì„± í•¨ìˆ˜ëŠ” ë³´í†µ ìŠ¤ì¹¼ë¼(scalars) ê°’ì„ ì²˜ë¦¬í•œë‹¤. ë°°ì—´ì— ì ìš©í•˜ë ¤ë©´ <code>Ïƒ.(xs)</code>, <code>relu.(xs)</code> ì²˜ëŸ¼ .ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŒ… í•´ ì£¼ì.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.Ïƒ" href="#NNlib.Ïƒ"><code>NNlib.Ïƒ</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">Ïƒ(x) = 1 / (1 + exp(-x))</code></pre><p>Classic <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> activation function.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/NNlib.jl/blob/c514d05a0be35d528b9e8df79c3b5b7c81fe3583/src/activation.jl#L4-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.relu" href="#NNlib.relu"><code>NNlib.relu</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">relu(x) = max(0, x)</code></pre><p><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/NNlib.jl/blob/c514d05a0be35d528b9e8df79c3b5b7c81fe3583/src/activation.jl#L38-L43">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.leakyrelu" href="#NNlib.leakyrelu"><code>NNlib.leakyrelu</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">leakyrelu(x) = max(0.01x, x)</code></pre><p>Leaky <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function. You can also specify the coefficient explicitly, e.g. <code>leakyrelu(x, 0.01)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/NNlib.jl/blob/c514d05a0be35d528b9e8df79c3b5b7c81fe3583/src/activation.jl#L47-L53">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.elu" href="#NNlib.elu"><code>NNlib.elu</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">elu(x, Î± = 1) =
  x &gt; 0 ? x : Î± * (exp(x) - 1)</code></pre><p>Exponential Linear Unit activation function. See <a href="https://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units</a>. You can also specify the coefficient explicitly, e.g. <code>elu(x, 1)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/NNlib.jl/blob/c514d05a0be35d528b9e8df79c3b5b7c81fe3583/src/activation.jl#L57-L64">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="NNlib.swish" href="#NNlib.swish"><code>NNlib.swish</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">swish(x) = x * Ïƒ(x)</code></pre><p>Self-gated actvation function. See <a href="https://arxiv.org/pdf/1710.05941.pdf">Swish: a Self-Gated Activation Function</a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/NNlib.jl/blob/c514d05a0be35d528b9e8df79c3b5b7c81fe3583/src/activation.jl#L82-L87">source</a></section><h2><a class="nav-anchor" id="ì •ìƒí™”(Normalisation)-and-ì •ê·œí™”(Regularisation)-1" href="#ì •ìƒí™”(Normalisation)-and-ì •ê·œí™”(Regularisation)-1">ì •ìƒí™”(Normalisation) &amp; ì •ê·œí™”(Regularisation)</a></h2><p>ì´ ë ˆì´ì–´ë“¤ì€ ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì¡°ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ í›ˆë ¨ ì‹œê°„(training times)ì˜ ê°œì„  ê·¸ë¦¬ê³  ì˜¤ë²„í”¼íŒ…(overfitting, ê³¼ì í•©)ì„ ì¤„ì—¬ ì¤€ë‹¤.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.testmode!" href="#Flux.testmode!"><code>Flux.testmode!</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">testmode!(m)
testmode!(m, false)</code></pre><p>Put layers like <a href="#Flux.Dropout"><code>Dropout</code></a> and <a href="#Flux.BatchNorm"><code>BatchNorm</code></a> into testing mode (or back to training mode with <code>false</code>).</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/normalise.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.BatchNorm" href="#Flux.BatchNorm"><code>Flux.BatchNorm</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">BatchNorm(channels::Integer, Ïƒ = identity;
          initÎ² = zeros, initÎ³ = ones,
          Ïµ = 1e-8, momentum = .1)</code></pre><p>Batch Normalization layer. The <code>channels</code> input should be the size of the channel dimension in your data (see below).</p><p>Given an array with <code>N</code> dimensions, call the <code>N-1</code>th the channel dimension. (For a batch of feature vectors this is just the data dimension, for <code>WHCN</code> images it&#39;s the usual channel dimension.)</p><p><code>BatchNorm</code> computes the mean and variance for each each <code>WÃ—HÃ—1Ã—N</code> slice and shifts them to have a new mean and variance (corresponding to the learnable, per-channel <code>bias</code> and <code>scale</code> parameters).</p><p>See <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.</p><p>Example:</p><pre><code class="language-julia">m = Chain(
  Dense(28^2, 64),
  BatchNorm(64, relu),
  Dense(64, 10),
  BatchNorm(10),
  softmax)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/normalise.jl#L99-L127">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.Dropout" href="#Flux.Dropout"><code>Flux.Dropout</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Dropout(p)</code></pre><p>A Dropout layer. For each input, either sets that input to <code>0</code> (with probability <code>p</code>) or scales it by <code>1/(1-p)</code>. This is used as a regularisation, i.e. it reduces overfitting during training.</p><p>Does nothing to the input once in <a href="#Flux.testmode!"><code>testmode!</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/normalise.jl#L15-L23">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.AlphaDropout" href="#Flux.AlphaDropout"><code>Flux.AlphaDropout</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">AlphaDropout(p)</code></pre><p>A dropout layer. It is used in Self-Normalizing Neural Networks.  (https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf) The AlphaDropout layer ensures that mean and variance of activations remains the same as before.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/normalise.jl#L46-L51">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.LayerNorm" href="#Flux.LayerNorm"><code>Flux.LayerNorm</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">LayerNorm(h::Integer)</code></pre><p>A <a href="https://arxiv.org/pdf/1607.06450.pdf">normalisation layer</a> designed to be used with recurrent hidden states of size <code>h</code>. Normalises the mean/stddev of each input before applying a per-neuron gain/bias.</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/normalise.jl#L77-L83">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Flux.GroupNorm" href="#Flux.GroupNorm"><code>Flux.GroupNorm</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><p>Group Normalization.  This layer can outperform Batch-Normalization and Instance-Normalization.</p><pre><code class="language-none">GroupNorm(chs::Integer, G::Integer, Î» = identity;
          initÎ² = (i) -&gt; zeros(Float32, i), initÎ³ = (i) -&gt; ones(Float32, i), 
          Ïµ = 1f-5, momentum = 0.1f0)</code></pre><p><span>$chs$</span> is the number of channels, the channel dimension of your input. For an array of N dimensions, the (N-1)th index is the channel dimension.</p><p><span>$G$</span> is the number of groups along which the statistics would be computed. The number of channels must be an integer multiple of the number of groups.</p><p>Example:</p><pre><code class="language-none">m = Chain(Conv((3,3), 1=&gt;32, leakyrelu;pad = 1),
          GroupNorm(32,16)) # 32 channels, 16 groups (G = 16), thus 2 channels per group used          </code></pre><p>Link : https://arxiv.org/pdf/1803.08494.pdf</p></div></div><a class="source-link" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/79534caca1b31012a4c6f3aa51507410003f4827/src/layers/normalise.jl#L290-L311">source</a></section><footer><hr/><a class="previous" href="../regularisation/"><span class="direction">ì´ì „ê¸€</span><span class="title">ì •ê·œí™”(Regularisation)</span></a><a class="next" href="../../training/optimisers/"><span class="direction">ë‹¤ìŒê¸€</span><span class="title">ìµœì í™”</span></a></footer></article></body></html>
