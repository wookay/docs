var documenterSearchIndex = {"docs":
[{"location":"#-1","page":"Home","title":"🦉","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"초보똥의 머싄러닁","category":"page"},{"location":"#","page":"Home","title":"Home","text":"번역에 대한 피드백은 https://github.com/wookay/Owl.jl/issues 이슈에 남겨 주세요","category":"page"},{"location":"Flux/#","page":"Flux 홈","title":"Flux 홈","text":"🦉 https://github.com/FluxML/Flux.jl 자료를 번역하는 곳입니당","category":"page"},{"location":"Flux/#Flux:-줄리아-머싄러닁-라이브러리-1","page":"Flux 홈","title":"Flux: 줄리아 머싄러닁 라이브러리","text":"","category":"section"},{"location":"Flux/#","page":"Flux 홈","title":"Flux 홈","text":"Flux는 머싄러닁을 위한 라이브러리이다. \"배터리-포함(batteries-included, 제품의 완전한 유용성을 위해 필요한 모든 부품을 함께 제공한다는 소프트웨어쪽 용어)\" 많은 유용한 도구를 제공한다. 줄리아 언어를 풀파워(full power)로 사용할 수 있다. 전체 스택을 줄리아 코드로 구현한다. GPU 커널도 가능하고, 개별 파트를 개인 취향에 맞게 조작할 수 있다.","category":"page"},{"location":"Flux/#설치하기-1","page":"Flux 홈","title":"설치하기","text":"","category":"section"},{"location":"Flux/#","page":"Flux 홈","title":"Flux 홈","text":"줄리아 0.6.0 이상, 아직 안깔았으면 설치하자.","category":"page"},{"location":"Flux/#","page":"Flux 홈","title":"Flux 홈","text":"Pkg.add(\"Flux\")\n# 선택인데 추천\nPkg.update() # 패키지를 최신 버전으로 업뎃\nPkg.test(\"Flux\") # 설치 똑바로 된건가 확인하기","category":"page"},{"location":"Flux/#","page":"Flux 홈","title":"Flux 홈","text":"기본적인 것 부터 시작하자. 모델 동물원(model zoo)은 여러가지 공통 모델을 다루는데 그걸로 시작해도 좋다.","category":"page"},{"location":"Flux/models/basics/#모델-만들기-기초-1","page":"기본적인 것","title":"모델 만들기 기초","text":"","category":"section"},{"location":"Flux/models/basics/#기울기(Gradients,-경사)-구하기-1","page":"기본적인 것","title":"기울기(Gradients, 경사) 구하기","text":"","category":"section"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"간단한 리니어 리그레션(linear regression, 직선 모양으로 그려지는 함수)을 생각해 보자. 이것은 입력 x에 대한 출력 배열 y를 예측한다. (줄리아 REPL에서 예제를 따라해보면 좋다)","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> W = rand(2, 5)\n2×5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = rand(2)\n2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> predict(x) = W*x .+ b\npredict (generic function with 1 method)\n\njulia> loss(x, y) = sum((predict(x) .- y).^2)\nloss (generic function with 1 method)\n\njulia> x, y = rand(5), rand(2) # 더미 데이터\n([0.496864, 0.947507, 0.874288, 0.251528, 0.192234], [0.901991, 0.0802404])\n\njulia> loss(x, y) # ~ 3\n3.1660692660286722","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"예측을 더 잘하기 위해 W와 b의 기울기를 구하자. loss function(손실, 예측 실패 함수)과 gradient descent(경사 하강, 내리막 기울기)를 해보면서. 직접 손으로 기울기를 계산할 수도 있지만 Flux에서는 W와 b를 훈련시키는 파라미터(parameters)로 둘 수 있다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> using Flux.Tracker\n\njulia> W = param(W)\nTracked 2×5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = param(b)\nTracked 2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> l = loss(x, y)\n3.1660692660286722 (tracked)\n\njulia> back!(l)\n","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"loss(x, y)는 방금 전과 같은 수(3.1660692660286722)를 리턴, 그런데 이제부터는 기울어지는 모양을 관찰 기록하여 값을 추적(tracked)  한다. back!을 호출하면 W와 b의 기울기를 계산한다. 기울기가 뭔지 알아냈으니 W를 고쳐가면서 모델을 훈련하자.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> W.grad\n2×5 Array{Float64,2}:\n 0.949491  1.81066  1.67074  0.480662  0.367352\n 1.49163   2.84449  2.62468  0.755107  0.577101\n\njulia> # 파라미터 업뎃\n       W.data .-= 0.1(W.grad)\n2×5 Array{Float64,2}:\n  0.762798   0.110647   0.0127989  0.890913  0.473484\n -0.0639541  0.693267  -0.0163046  0.385161  0.714602\n\njulia> loss(x, y) # ~ 2.5\n1.1327711929294395 (tracked)","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"예측 실패(loss)가 조금 줄어들었다. x 예측이 목표 대상(target) y에 좀 더 가까워졌다는 것을 의미한다. 데이터가 있으면 모델 훈련하기도 시도할 수 있다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"복잡한 딥러닝이 Flux에서는 이와 같은 예제처럼 단순해진다. 물론 모델의 파라미터 갯수가 백만개가 넘어가고 복잡한 제어 흐름을 갖게 되면 다른 모양을 갖겠지. 그리고 이러한 복잡성을 다루는 것에는 뭐가 있는지 한번 살펴보자.","category":"page"},{"location":"Flux/models/basics/#레이어-만들기-1","page":"기본적인 것","title":"레이어 만들기","text":"","category":"section"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"이제부터는 리니어 리그레션 보다 복잡한 모델을 만들어 보자. 예를 들어, 두 개의 리니어 레이어 사이에 시그모이드 (σ) 처럼 비선형(nonlinearity, 커브처럼 직선이 아닌 거)를 갖는 넘이 있을때, 위의 스타일은 아래와 같이 쓸 수 있다:","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> using Flux\n\njulia> W1 = param(rand(3, 5))\nTracked 3×5 Array{Float64,2}:\n 0.540422  0.680087  0.743124  0.0216563  0.377793\n 0.416939  0.51823   0.464998  0.419852   0.446143\n 0.260294  0.392582  0.46784   0.549495   0.373124\n\njulia> b1 = param(rand(3))\nTracked 3-element Array{Float64,1}:\n 0.213799\n 0.373862\n 0.243417\n\njulia> layer1(x) = W1 * x .+ b1\nlayer1 (generic function with 1 method)\n\njulia> W2 = param(rand(2, 3))\nTracked 2×3 Array{Float64,2}:\n 0.789744  0.389376  0.172613\n 0.472963  0.21518   0.220236\n\njulia> b2 = param(rand(2))\nTracked 2-element Array{Float64,1}:\n 0.121207\n 0.502486\n\njulia> layer2(x) = W2 * x .+ b2\nlayer2 (generic function with 1 method)\n\njulia> model(x) = layer2(σ.(layer1(x)))\nmodel (generic function with 1 method)\n\njulia> model(rand(5)) # => 2-엘러먼트 벡터\nTracked 2-element Array{Float64,1}:\n 1.06727\n 1.13835","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"작동은 하는데 중복 작업이 많아 보기에 좋지 않다 - 특히 레이어를 더 추가한다면. 리니어 레이어를 돌려주는 함수를 하나 만들어 이것들을 정리하자.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> function linear(in, out)\n         W = param(randn(out, in))\n         b = param(randn(out))\n         x -> W * x .+ b\n       end\nlinear (generic function with 1 method)\n\njulia> linear1 = linear(5, 3) # linear1.W 할 수 있닥 (익명함수 리턴)\n(::#3) (generic function with 1 method)\n\njulia> linear1.W\nTracked 3×5 Array{Float64,2}:\n -1.72011   -1.07297   0.396755  -0.117604   0.25952\n -0.16694    0.99327  -0.589717  -1.87123    0.141679\n -0.972281  -1.84836   2.55071   -0.136674  -0.147826\n\njulia> linear2 = linear(3, 2)\n(::#3) (generic function with 1 method)\n\njulia> model(x) = linear2(σ.(linear1(x)))\nmodel (generic function with 1 method)\n\njulia> model(x) # => 2-엘러먼트 벡터\nTracked 2-element Array{Float64,1}:\n 2.75582\n 0.416809","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"다른 방법으로는 struct로 타입을 만들어서 어파인(affine) 레이어를 명시적으로 표현하는 것이 있다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> struct Affine\n         W\n         b\n       end\n\njulia> Affine(in::Integer, out::Integer) =\n         Affine(param(randn(out, in)), param(randn(out)))\nAffine\n\njulia> # 오버로드 하면 객체를 함수처럼 호출할 수 있다\n       (m::Affine)(x) = m.W * x .+ m.b\n\njulia> a = Affine(10, 5)\nAffine(param([0.0252182 -1.99122 … -0.191235 0.294728; 1.13559 1.50226 … -2.43917 0.56976; … ; -0.735177 0.202646 … -0.301945 -0.183598; 1.05967 0.986786 … -1.57835 -0.0893871]), param([-0.39419, -1.26818, 0.757665, 0.941398, -0.783242]))\n\njulia> a(rand(10)) # => 5-엘러먼트 벡터\nTracked 5-element Array{Float64,1}:\n -0.945544\n -0.575674\n  2.93741\n  0.111253\n -0.843172","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"축하합니다! Flux에서 나오는 Dense 레이어 만들기 성공! Flux는 많은 재밌는 레이어들이 있는데, 그것들을 직접 만드는 것 또한 정말 쉽다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"(Dense와 다른 한가지 - 편의를 위해 활성(activation) 함수를 뒤에 추가할 수도 있다. Dense(10, 5, σ) 요런식으로.)","category":"page"},{"location":"Flux/models/basics/#이쁘게-쌓아보자-1","page":"기본적인 것","title":"이쁘게 쌓아보자","text":"","category":"section"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"다음과 같은 모델을 만드는 것은 흔하다: (layer1 이름이 겹치니 REPL을 새로 띄우자)","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> using Flux\n\njulia> layer1 = Dense(10, 5, σ)\nDense(10, 5, NNlib.σ)\n\njulia> # ...\n       model(x) = layer3(layer2(layer1(x)))\nmodel (generic function with 1 method)","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"기다랗게 연결(chains) 할라믄, 다음과 같이 레이어의 리스트를 만드는게 좀 더 직관적이다:","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> layers = [Dense(10, 5, σ), Dense(5, 2), softmax]\n3-element Array{Any,1}:\n Dense(10, 5, NNlib.σ)\n Dense(5, 2)\n NNlib.softmax\n\njulia> model(x) = foldl((x, m) -> m(x), x, layers)\nmodel (generic function with 1 method)\n\njulia> model(rand(10)) # => 2-엘러먼트 벡터\nTracked 2-element Array{Float64,1}:\n 0.593021\n 0.406979","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"편리하게 쓰라고 이것 역시 Flux에서 제공한다:","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> model2 = Chain(\n         Dense(10, 5, σ),\n         Dense(5, 2),\n         softmax)\nChain(Dense(10, 5, NNlib.σ), Dense(5, 2), NNlib.softmax)\n\njulia> model2(rand(10)) # => 2-엘러먼트 벡터\nTracked 2-element Array{Float64,1}:\n 0.172085\n 0.827915","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"고오급 딥러닝 라이브러리 같아 보인다; 어느만큼 간단하게 추상화 하는지 보았을 것이다. 줄리아 코드의 강력함을 놓치지 않았다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"이런 접근법의 좋은 점은 \"모델\"이 함수라는 것이다 (훈련가능한 파라미터와 함께), 함수 합성(∘) 또한 가능하다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> m = Dense(5, 2) ∘ Dense(10, 5, σ)\n(::#55) (generic function with 1 method)\n\njulia> m(rand(10))\nTracked 2-element Array{Float64,1}:\n -1.28749\n -0.202492","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"마찬가지로, Chain은 줄리아 함수와 이쁘게 동작한다.","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> m = Chain(x -> x^2, x -> x+1)\nChain(#3, #4)\n\njulia> m(5) # => 26\n26","category":"page"},{"location":"Flux/models/basics/#레이어-도우미들-1","page":"기본적인 것","title":"레이어 도우미들","text":"","category":"section"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"Flux는 사용자의 커스텀 레이어를 도와주는 함수를 제공한다. 다음과 같이 호출하면","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"julia> Flux.treelike(Affine)\nadapt (generic function with 1 method)","category":"page"},{"location":"Flux/models/basics/#","page":"기본적인 것","title":"기본적인 것","text":"Affine 레이어에 부가적인 유용한 기능이 추가된다, 파라미터 모으기(collecting)나 GPU에서 처리하기 같은 작업을 할 수 있다.","category":"page"},{"location":"Flux/models/recurrence/#순환-모델(Recurrent-Models)-1","page":"순환(Recurrence)","title":"순환 모델(Recurrent Models)","text":"","category":"section"},{"location":"Flux/models/recurrence/#기억-세포(Recurrent-Cells,-순환-셀,-뇌를-모방한-거)-1","page":"순환(Recurrence)","title":"기억 세포(Recurrent Cells, 순환 셀, 뇌를 모방한 거)","text":"","category":"section"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"단순한 피드포워드(feedforward, 사이클(cycle)이나 루프(loop)가 없는 네트워크) 경우, 모델 m은 여러 개의 입력 xᵢ에 대한 yᵢ를 예측하는 간단한 함수다. (예를 들어, x를 MNIST 숫자라 치면 y는 그것을 분류한 숫자.) 예측은 서로 완전히 독립적이며 x가 같으면 y도 언제나 동일하다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"y₁ = f(x₁)\ny₂ = f(x₂)\ny₃ = f(x₃)\n# ...","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"순환 네트워크는 히든 상태(hidden state, 숨겨논 상태)가 존재하며 모델을 돌릴 때 매번 그 상태를 다음으로 넘긴다. 그래서 이 모델은 그때마다 이전 h를 입력으로 받고, 새로운 h를 출력으로 내 놓는다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"h = # ... 초기 상태 ...\nh, y₁ = f(h, x₁)\nh, y₂ = f(h, x₂)\nh, y₃ = f(h, x₃)\n# ...","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"h에 저장한 정보는 다음번 예측을 위해 유지하고, 그래서 마치 함수의 메모리 같은 역할을 하게 한다. 이것은 x에 대한 예측이 이전까지 모델에 주입한 모든 입력으로부터 영향을 받는 것을 의미한다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"(요거는 중요한 거니까 예를 들어, x를 문장에서의 한 단어라 보자; 만약에 \"bank\"라는 영어 단어가 주어지면 모델은 이전 입력이 \"강 river\" 이면 강둑으로, \"투자 investment\"면 은행으로 해석해야 한다.)","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"Flux의 RNN 지원은 수학적 관점을 지니고 있다. 가장 기본이 되는 RNN은 표준 \"Dense\" 레이어를 따르고, 그 출력은 히든 상태이다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> using Flux\n\njulia> Wxh = randn(5, 10)\n5×10 Array{Float64,2}:\n -0.197167   0.0931036  -1.13283   …   0.426711   1.5678      0.488363\n -1.19948   -1.05618     1.057        -1.85708    2.05188    -0.732148\n -0.848823   0.147774    1.66139      -0.777346  -0.0650354   0.36015\n -0.380701   0.737349    0.426964      0.694122  -1.46597    -1.00572\n -0.789044  -0.374745   -0.996698      0.505453  -0.117276    1.35148\n\njulia> Whh = randn(5, 5)\n5×5 Array{Float64,2}:\n -1.12946    -0.523065   0.0547692  -0.305124  -0.105809\n -0.195351    0.588007   0.616959    0.779213  -0.145329\n -0.265139   -0.535485  -0.300887    2.13263   -1.53089\n -0.0537235  -1.47912   -0.883858    0.993426  -0.354738\n  0.486817    0.170843   0.0440353   0.177502   0.730423\n\njulia> b   = randn(5)\n5-element Array{Float64,1}:\n  0.982592\n -0.724775\n  0.118081\n  0.140369\n -1.07578\n\njulia> function rnn(h, x)\n         h = tanh.(Wxh * x .+ Whh * h .+ b)\n         return h, h\n       end\nrnn (generic function with 1 method)\n\njulia> x = rand(10) # 더미 데이터\n10-element Array{Float64,1}:\n 0.312436\n 0.384043\n 0.972045\n 0.194086\n 0.496317\n 0.654925\n 0.0311892\n 0.494105\n 0.338846\n 0.204689\n\njulia> h = rand(5)  # 초기 히든 상태\n5-element Array{Float64,1}:\n 0.861124\n 0.994686\n 0.560054\n 0.371721\n 0.159454\n\njulia> h, y = rnn(h, x)\n([-0.963817, -0.198195, 0.903936, -0.686608, -0.839093], [-0.963817, -0.198195, 0.903936, -0.686608, -0.839093])","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"마지막 rnn을 좀 더 돌려보면, 출력 y는 입력 x가 같은데도 조금씩 바뀌는 것을 알 수 있다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> h, y = rnn(h, x)\n([0.812906, -0.767065, 0.945139, 0.0198447, -0.996763], [0.812906, -0.767065, 0.945139, 0.0198447, -0.996763])\n\njulia> h, y = rnn(h, x)\n([-0.647084, -0.799032, 0.997557, 0.902798, -0.984697], [-0.647084, -0.799032, 0.997557, 0.902798, -0.984697])","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"앞서 언급한 rnn 함수는 명시적으로 상태를 관리하는 기억 세포(cells) 이다. 다양한 기억 세포가 존재하며 레이어 참조에 관련 내용이 있다. 위의 예제는 다음과 같이 바꿀 수 있다:","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> using Flux\n\njulia> rnn2 = Flux.RNNCell(10, 5)\nRNNCell(10, 5, tanh)\n\njulia> x = rand(10) # 더미 데이터\n10-element Array{Float64,1}:\n 0.142406\n 0.944597\n 0.973233\n 0.434782\n 0.715639\n 0.763562\n 0.280661\n 0.293604\n 0.496457\n 0.173372\n\njulia> h = rand(5)  # 초기 히든 상태\n5-element Array{Float64,1}:\n 0.602545\n 0.998396\n 0.558707\n 0.637564\n 0.0313308\n\njulia> h, y = rnn2(h, x)\n(param([-0.160217, -0.741263, 0.048164, 0.963063, 0.0301785]), param([-0.160217, -0.741263, 0.048164, 0.963063, 0.0301785]))","category":"page"},{"location":"Flux/models/recurrence/#상태를-갖는-모델-1","page":"순환(Recurrence)","title":"상태를 갖는 모델","text":"","category":"section"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"대부분의 경우, 히든 상태를 직접 관리하는 거는 귀찮으니까 모델이 상태를 갖게끔 처리할 수 있다. Flux는 Recur 래퍼를 제공한다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> x = rand(10)\n10-element Array{Float64,1}:\n 0.165593\n 0.502313\n 0.120926\n 0.505827\n 0.917068\n 0.557163\n 0.688472\n 0.791826\n 0.0838632\n 0.709302\n\njulia> h = rand(5)\n5-element Array{Float64,1}:\n 0.40008\n 0.48858\n 0.551568\n 0.0688404\n 0.0583865\n\njulia> m = Flux.Recur(rnn, h)\nRecur(rnn)\n\njulia> y = m(x)\n5-element Array{Float64,1}:\n  0.963414\n -0.999974\n  0.739107\n  0.976241\n  0.986023","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"Recur 래퍼는 m.state 필드에 상태를 저장한다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"RNN(10, 5) 생성자를 사용하면 - RNNCell과 대응하는 - 다음과 같이 이거는 단순히 래퍼 셀이다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> RNN(10, 5)\nRecur(RNNCell(10, 5, tanh))","category":"page"},{"location":"Flux/models/recurrence/#시퀀스(Sequences,-연속되는-값)-1","page":"순환(Recurrence)","title":"시퀀스(Sequences, 연속되는 값)","text":"","category":"section"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"종종 개별적인 x 보다는 연속되는 입력을 다루길 원한다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> seq = [rand(10) for i = 1:10]\n10-element Array{Array{Float64,1},1}:\n [0.443911, 0.955247, 0.980153, 0.313181, 0.0426581, 0.354755, 0.113961, 0.222873, 0.865114, 0.14094]\n [0.50466, 0.0204917, 0.890547, 0.574102, 0.301098, 0.944295, 0.95414, 0.36809, 0.341546, 0.474998]\n [0.474114, 0.152628, 0.364967, 0.601978, 0.212361, 0.66016, 0.12101, 0.944988, 0.417781, 0.715282]\n [0.0776375, 0.843099, 0.000618674, 0.352273, 0.977611, 0.801756, 0.550702, 0.311638, 0.285711, 0.0856441]\n [0.603498, 0.863035, 0.89494, 0.506224, 0.840984, 0.13453, 0.43549, 0.216554, 0.361081, 0.0965758]\n [0.236062, 0.407028, 0.357854, 0.875694, 0.0468227, 0.786622, 0.616748, 0.791976, 0.800668, 0.147169]\n [0.739452, 0.38329, 0.961215, 0.113691, 0.381309, 0.57526, 0.0170709, 0.403656, 0.445509, 0.051497]\n [0.956629, 0.624735, 0.14811, 0.202354, 0.484018, 0.250409, 0.0352729, 0.809209, 0.831828, 0.826355]\n [0.388553, 0.42596, 0.736068, 0.454156, 0.626974, 0.641246, 0.444018, 0.768584, 0.118879, 0.416568]\n [0.307721, 0.176393, 0.371934, 0.714272, 0.886859, 0.333667, 0.721609, 0.975586, 0.59609, 0.771424]","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"Recur로 모델을 시퀀스의 각 항목마다 쉽게 적용할 수 있다:","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> m.(seq) # 5-엘러먼트 벡터의 리스트를 돌려준다\n10-element Array{Array{Float64,1},1}:\n [0.958516, -0.996974, 0.640934, -0.440203, 0.991754]\n [0.998417, -0.998238, 0.988128, 0.924522, 0.999099]\n [0.943455, -0.999939, 0.94332, 0.638572, 0.999795]\n [0.997841, -0.999912, 0.414106, 0.705974, 0.999871]\n [0.9896, -0.96634, 0.903348, 0.805409, 0.949429]\n [0.990047, -0.999849, 0.991448, 0.950895, 0.999938]\n [0.980617, -0.988072, 0.978565, -0.785643, 0.985682]\n [0.98617, -0.99938, -0.791134, 0.603178, 0.0937938]\n [0.946547, -0.893022, 0.914559, 0.999905, 0.984556]\n [0.989439, -0.999979, 0.964896, 0.978421, 0.999834]","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"더 커다란 모델에 순환 레이어(recurrent layers)를 연쇄적(chain)으로 연결 할 수 있다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> m = Chain(LSTM(10, 15), Dense(15, 5))\nChain(Recur(LSTMCell(10, 60)), Dense(15, 5))\n\njulia> m.(seq)\n10-element Array{TrackedArray{…,Array{Float64,1}},1}:\n param([0.0779735, 0.0534096, -0.0245852, -0.0699291, -0.00650743])\n param([0.203825, -0.0307184, -0.0940759, -0.100437, 0.0523315])\n param([0.21071, -0.19635, -0.106985, -0.185204, 0.132647])\n param([0.314643, -0.205525, -0.00144219, -0.165195, 0.197256])\n param([0.351024, -0.116196, 0.00489051, -0.255343, 0.209503])\n param([0.370406, -0.125797, -0.0506301, -0.253045, 0.179001])\n param([0.349787, -0.091392, -0.0699977, -0.249944, 0.197391])\n param([0.370064, -0.21158, -0.00144108, -0.337597, 0.24153])\n param([0.396285, -0.240793, -0.0263459, -0.358695, 0.260678])\n param([0.464372, -0.316526, -0.0295575, -0.352548, 0.251627])","category":"page"},{"location":"Flux/models/recurrence/#기울기-계산-기록-잘라내기(Truncating-Gradients)-1","page":"순환(Recurrence)","title":"기울기 계산 기록 잘라내기(Truncating Gradients)","text":"","category":"section"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"기본적으로, 순환 레이어의 기울기를 계산하는 것은 전체 기록(history)을 내포한다. 예를 들어 100개의 입력을 가진 모델을 실행할 때, back!을 하면 100개에 대한 기울기를 계산한다. 그러고 다른 10개의 입력을 더 계산한다면 110개의 기울기를 계산해야 한다 - 이거는 누적되므로 빠르게 연산 비용이 증가한다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"이거를 막는 방법은 기울기 계산 기록을 잘라내어(truncate) 지워주는 것이다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> Flux.truncate!(m)\n","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"truncate!을 호출하면 깔끔이 청소해 준다. 그래서 더 많은 입력의 모델을 실행해도 비싼 기울기 연산 없이 해낼 수 있다.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"truncate!는 여러 개의 커다란 시퀀스 덩어리를 다룰 때 유용하지만, 서로 독립적인 시퀀스들을 다루고 싶을 때도 있다. 그 경우 히든 상태는 원래 값으로 완전히 초기화 되어 누적된 정보를 버린다. 그렇게 하고 싶으면 reset!을 해 주자.","category":"page"},{"location":"Flux/models/recurrence/#","page":"순환(Recurrence)","title":"순환(Recurrence)","text":"julia> Flux.reset!(m)\n","category":"page"},{"location":"Flux/models/regularisation/#정규화(Regularisation)-1","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"","category":"section"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"이번에는 모델 파라미터를 정규화 해 보자. vecnorm과 같은 정규화를 해주는 적절한 함수를 각 모델 파라미터에 적용하여 그 결과를 모든 loss에 더하도록 하자.","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"예를 들어, 다음과 같은 간단한 regression을 보자.","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"julia> using Flux\n\njulia> m = Dense(10, 5)\nDense(10, 5)\n\njulia> loss(x, y) = Flux.crossentropy(softmax(m(x)), y)\nloss (generic function with 1 method)","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"m.W와 m.b 파라미터에 L2 norm을 취하여 정규화 해보자.","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"julia> penalty() = vecnorm(m.W) + vecnorm(m.b)\npenalty (generic function with 1 method)\n\njulia> loss(x, y) = Flux.crossentropy(softmax(m(x)), y) + penalty()\nloss (generic function with 1 method)","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"레이어를 이용하는 경우, Flux는 params 함수를 제공하여 모든 파라미터를 한번에 가져올 수 있다. sum(vecnorm, params)를 사용하여 전체를 쉽게 적용할 수 있다.","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"julia> params(m)\n2-element Array{Any,1}:\n param([-0.61839 -0.556047 … -0.460808 -0.107646; 0.346293 -0.375076 … -0.608704 -0.181025; … ; -0.2226 -0.0992159 … 0.0707984 -0.429173; -0.331058 -0.291995 … 0.383368 0.156716])\n param([0.0, 0.0, 0.0, 0.0, 0.0])\n\njulia> sum(vecnorm, params(m))\n2.4130860599427706 (tracked)","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"좀 더 큰 규모의 예로, 멀티-레이어 퍼셉트론(perceptron)은 다음과 같다.","category":"page"},{"location":"Flux/models/regularisation/#","page":"정규화(Regularisation)","title":"정규화(Regularisation)","text":"julia> m = Chain(\n         Dense(28^2, 128, relu),\n         Dense(128, 32, relu),\n         Dense(32, 10), softmax)\nChain(Dense(784, 128, NNlib.relu), Dense(128, 32, NNlib.relu), Dense(32, 10), NNlib.softmax)\n\njulia> loss(x, y) = Flux.crossentropy(m(x), y) + sum(vecnorm, params(m))\nloss (generic function with 1 method)\n\njulia> loss(rand(28^2), rand(10))\n39.128892409412174 (tracked)","category":"page"},{"location":"Flux/models/layers/#기본-레이어-1","page":"모델 참조(Model Reference)","title":"기본 레이어","text":"","category":"section"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"거의 모든 신경망(neural networks)의 토대를 다음의 핵심 레이어로 구성한다.","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"Chain\nDense","category":"page"},{"location":"Flux/models/layers/#Flux.Chain","page":"모델 참조(Model Reference)","title":"Flux.Chain","text":"Chain(layers...)\n\nChain multiple layers / functions together, so that they are called in sequence on a given input.\n\nm = Chain(x -> x^2, x -> x+1)\nm(5) == 26\n\nm = Chain(Dense(10, 5), Dense(5, 2))\nx = rand(10)\nm(x) == m[2](m[1](x))\n\nChain also supports indexing and slicing, e.g. m[2] or m[1:end-1]. m[1:3](x) will calculate the output of the first three layers.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.Dense","page":"모델 참조(Model Reference)","title":"Flux.Dense","text":"Dense(in::Integer, out::Integer, σ = identity)\n\nCreates a traditional Dense layer with parameters W and b.\n\ny = σ.(W * x .+ b)\n\nThe input x must be a vector of length in, or a batch of vectors represented as an in × N matrix. The out y will be a vector or batch of length out.\n\njulia> d = Dense(5, 2)\nDense(5, 2)\n\njulia> d(rand(5))\nTracked 2-element Array{Float64,1}:\n  0.00257447\n  -0.00449443\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Convolution-and-Pooling-Layers-1","page":"모델 참조(Model Reference)","title":"Convolution and Pooling Layers","text":"","category":"section"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"These layers are used to build convolutional neural networks (CNNs).","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"Conv\nMaxPool\nMeanPool\nDepthwiseConv\nConvTranspose","category":"page"},{"location":"Flux/models/layers/#Flux.Conv","page":"모델 참조(Model Reference)","title":"Flux.Conv","text":"Conv(size, in=>out)\nConv(size, in=>out, relu)\n\nStandard convolutional layer. size should be a tuple like (2, 2). in and out specify the number of input and output channels respectively.\n\nExample: Applying Conv layer to a 1-channel input using a 2x2 window size,          giving us a 16-channel output. Output is activated with ReLU.\n\nsize = (2,2)\nin = 1\nout = 16 \nConv((2, 2), 1=>16, relu)\n\nData should be stored in WHCN order (width, height, # channels, # batches).  In other words, a 100×100 RGB image would be a 100×100×3×1 array,  and a batch of 50 would be a 100×100×3×50 array.\n\nTakes the keyword arguments pad, stride and dilation.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.MaxPool","page":"모델 참조(Model Reference)","title":"Flux.MaxPool","text":"MaxPool(k)\n\nMax pooling layer. k stands for the size of the window for each dimension of the input.\n\nTakes the keyword arguments pad and stride.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.MeanPool","page":"모델 참조(Model Reference)","title":"Flux.MeanPool","text":"MeanPool(k)\n\nMean pooling layer. k stands for the size of the window for each dimension of the input.\n\nTakes the keyword arguments pad and stride.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.DepthwiseConv","page":"모델 참조(Model Reference)","title":"Flux.DepthwiseConv","text":"DepthwiseConv(size, in)\nDepthwiseConv(size, in=>mul)\nDepthwiseConv(size, in=>mul, relu)\n\nDepthwise convolutional layer. size should be a tuple like (2, 2). in and mul specify the number of input channels and channel multiplier respectively. In case the mul is not specified it is taken as 1.\n\nData should be stored in WHCN order. In other words, a 100×100 RGB image would be a 100×100×3 array, and a batch of 50 would be a 100×100×3×50 array.\n\nTakes the keyword arguments pad and stride.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.ConvTranspose","page":"모델 참조(Model Reference)","title":"Flux.ConvTranspose","text":"ConvTranspose(size, in=>out)\nConvTranspose(size, in=>out, relu)\n\nStandard convolutional transpose layer. size should be a tuple like (2, 2). in and out specify the number of input and output channels respectively. Data should be stored in WHCN order. In other words, a 100×100 RGB image would be a 100×100×3 array, and a batch of 50 would be a 100×100×3×50 array. Takes the keyword arguments pad, stride and dilation.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#순환-레이어(Recurrent-Layers)-1","page":"모델 참조(Model Reference)","title":"순환 레이어(Recurrent Layers)","text":"","category":"section"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"위의 핵심 레이어와 함께, 시퀀스 데이터(다른 종류의 구조화된 데이터)를 처리할 때 사용할 수 있다.","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"RNN\nLSTM\nGRU\nFlux.Recur","category":"page"},{"location":"Flux/models/layers/#Flux.RNN","page":"모델 참조(Model Reference)","title":"Flux.RNN","text":"RNN(in::Integer, out::Integer, σ = tanh)\n\nThe most basic recurrent layer; essentially acts as a Dense layer, but with the output fed back into the input each time step.\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#Flux.LSTM","page":"모델 참조(Model Reference)","title":"Flux.LSTM","text":"LSTM(in::Integer, out::Integer)\n\nLong Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences.\n\nSee this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#Flux.GRU","page":"모델 참조(Model Reference)","title":"Flux.GRU","text":"GRU(in::Integer, out::Integer)\n\nGated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences.\n\nSee this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#Flux.Recur","page":"모델 참조(Model Reference)","title":"Flux.Recur","text":"Recur(cell)\n\nRecur takes a recurrent cell and makes it stateful, managing the hidden state in the background. cell should be a model of the form:\n\nh, y = cell(h, x...)\n\nFor example, here's a recurrent network that keeps a running total of its inputs.\n\naccum(h, x) = (h+x, x)\nrnn = Flux.Recur(accum, 0)\nrnn(2) # 2\nrnn(3) # 3\nrnn.state # 5\nrnn.(1:10) # apply to a sequence\nrnn.state # 60\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Other-General-Purpose-Layers-1","page":"모델 참조(Model Reference)","title":"Other General Purpose Layers","text":"","category":"section"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"These are marginally more obscure than the Basic Layers. But in contrast to the layers described in the other sections are not readily grouped around a particular purpose (e.g. CNNs or RNNs).","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"Maxout","category":"page"},{"location":"Flux/models/layers/#Flux.Maxout","page":"모델 참조(Model Reference)","title":"Flux.Maxout","text":"Maxout(over)\n\nMaxout is a neural network layer, which has a number of internal layers, which all have the same input, and the maxout returns the elementwise maximium of the internal layers' outputs.\n\nMaxout over linear dense layers satisfies the univeral approximation theorem.\n\nReference: Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\n\nMaxout networks.\n\nIn Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML'13), Sanjoy Dasgupta and David McAllester (Eds.), Vol. 28. JMLR.org III-1319-III-1327. https://arxiv.org/pdf/1302.4389.pdf\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#활성-함수(Activation-Functions)-1","page":"모델 참조(Model Reference)","title":"활성 함수(Activation Functions)","text":"","category":"section"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"모델의 레이어 중간에 비선형성(Non-linearities)을 갖을 때 사용한다. 함수의 대부분은 NNlib에 정의되어 있고 Flux에서 기본적으로 사용할 수 있다.","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"특별한 언급이 없으면 활성 함수는 보통 스칼라(scalars) 값을 처리한다. 배열에 적용하려면 σ.(xs), relu.(xs) 처럼 .으로 브로드캐스팅 해 주자.","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"σ\nrelu\nleakyrelu\nelu\nswish","category":"page"},{"location":"Flux/models/layers/#NNlib.σ","page":"모델 참조(Model Reference)","title":"NNlib.σ","text":"σ(x) = 1 / (1 + exp(-x))\n\nClassic sigmoid activation function.\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#NNlib.relu","page":"모델 참조(Model Reference)","title":"NNlib.relu","text":"relu(x) = max(0, x)\n\nRectified Linear Unit activation function.\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#NNlib.leakyrelu","page":"모델 참조(Model Reference)","title":"NNlib.leakyrelu","text":"leakyrelu(x) = max(0.01x, x)\n\nLeaky Rectified Linear Unit activation function. You can also specify the coefficient explicitly, e.g. leakyrelu(x, 0.01).\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#NNlib.elu","page":"모델 참조(Model Reference)","title":"NNlib.elu","text":"elu(x, α = 1) =\n  x > 0 ? x : α * (exp(x) - 1)\n\nExponential Linear Unit activation function. See Fast and Accurate Deep Network Learning by Exponential Linear Units. You can also specify the coefficient explicitly, e.g. elu(x, 1).\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#NNlib.swish","page":"모델 참조(Model Reference)","title":"NNlib.swish","text":"swish(x) = x * σ(x)\n\nSelf-gated actvation function. See Swish: a Self-Gated Activation Function.\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#정상화(Normalisation)-and-정규화(Regularisation)-1","page":"모델 참조(Model Reference)","title":"정상화(Normalisation) & 정규화(Regularisation)","text":"","category":"section"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"이 레이어들은 네트워크의 구조에는 영향을 주지 않으면서 훈련 시간(training times)의 개선 그리고 오버피팅(overfitting, 과적합)을 줄여 준다.","category":"page"},{"location":"Flux/models/layers/#","page":"모델 참조(Model Reference)","title":"모델 참조(Model Reference)","text":"Flux.testmode!\nBatchNorm\nDropout\nAlphaDropout\nLayerNorm\nGroupNorm","category":"page"},{"location":"Flux/models/layers/#Flux.testmode!","page":"모델 참조(Model Reference)","title":"Flux.testmode!","text":"testmode!(m)\ntestmode!(m, false)\n\nPut layers like Dropout and BatchNorm into testing mode (or back to training mode with false).\n\n\n\n\n\n","category":"function"},{"location":"Flux/models/layers/#Flux.BatchNorm","page":"모델 참조(Model Reference)","title":"Flux.BatchNorm","text":"BatchNorm(channels::Integer, σ = identity;\n          initβ = zeros, initγ = ones,\n          ϵ = 1e-8, momentum = .1)\n\nBatch Normalization layer. The channels input should be the size of the channel dimension in your data (see below).\n\nGiven an array with N dimensions, call the N-1th the channel dimension. (For a batch of feature vectors this is just the data dimension, for WHCN images it's the usual channel dimension.)\n\nBatchNorm computes the mean and variance for each each W×H×1×N slice and shifts them to have a new mean and variance (corresponding to the learnable, per-channel bias and scale parameters).\n\nSee Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\n\nExample:\n\nm = Chain(\n  Dense(28^2, 64),\n  BatchNorm(64, relu),\n  Dense(64, 10),\n  BatchNorm(10),\n  softmax)\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.Dropout","page":"모델 참조(Model Reference)","title":"Flux.Dropout","text":"Dropout(p)\n\nA Dropout layer. For each input, either sets that input to 0 (with probability p) or scales it by 1/(1-p). This is used as a regularisation, i.e. it reduces overfitting during training.\n\nDoes nothing to the input once in testmode!.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.AlphaDropout","page":"모델 참조(Model Reference)","title":"Flux.AlphaDropout","text":"AlphaDropout(p)\n\nA dropout layer. It is used in Self-Normalizing Neural Networks.  (https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf) The AlphaDropout layer ensures that mean and variance of activations remains the same as before.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.LayerNorm","page":"모델 참조(Model Reference)","title":"Flux.LayerNorm","text":"LayerNorm(h::Integer)\n\nA normalisation layer designed to be used with recurrent hidden states of size h. Normalises the mean/stddev of each input before applying a per-neuron gain/bias.\n\n\n\n\n\n","category":"type"},{"location":"Flux/models/layers/#Flux.GroupNorm","page":"모델 참조(Model Reference)","title":"Flux.GroupNorm","text":"Group Normalization.  This layer can outperform Batch-Normalization and Instance-Normalization.\n\nGroupNorm(chs::Integer, G::Integer, λ = identity;\n          initβ = (i) -> zeros(Float32, i), initγ = (i) -> ones(Float32, i), \n          ϵ = 1f-5, momentum = 0.1f0)\n\nchs is the number of channels, the channel dimension of your input. For an array of N dimensions, the (N-1)th index is the channel dimension.\n\nG is the number of groups along which the statistics would be computed. The number of channels must be an integer multiple of the number of groups.\n\nExample:\n\nm = Chain(Conv((3,3), 1=>32, leakyrelu;pad = 1),\n          GroupNorm(32,16)) # 32 channels, 16 groups (G = 16), thus 2 channels per group used          \n\nLink : https://arxiv.org/pdf/1803.08494.pdf\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#최적화-함수(Optimisers)-1","page":"최적화","title":"최적화 함수(Optimisers)","text":"","category":"section"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"간단한 리니어 리그레션에서 우리는 더미 데이터를 만든 후, 손실(loss)을 계산하고 역전파(backpropagate) 하여 파라미터 W와 b의 기울기를 계산하였다.","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"julia> using Flux\n\njulia> W = param(rand(2, 5))\nTracked 2×5 Array{Float64,2}:\n 0.215021  0.22422   0.352664  0.11115   0.040711\n 0.180933  0.769257  0.361652  0.783197  0.545495\n\njulia> b = param(rand(2))\nTracked 2-element Array{Float64,1}:\n 0.205216\n 0.150938\n\njulia> predict(x) = W*x .+ b\npredict (generic function with 1 method)\n\njulia> loss(x, y) = sum((predict(x) .- y).^2)\nloss (generic function with 1 method)\n\njulia> x, y = rand(5), rand(2) # 더미 데이터\n([0.153473, 0.927019, 0.40597, 0.783872, 0.392236], [0.261727, 0.00917161])\n\njulia> l = loss(x, y) # ~ 3\n3.6352060699201565 (tracked)\n\njulia> Flux.back!(l)\n","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"기울기를 사용하여 파라미터를 업데이트 하고자 한다. 손실을 줄이려고 말이다. 여기서 한가지 방법은:","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"function update()\n  η = 0.1 # 학습하는 속도(Learning Rate)\n  for p in (W, b)\n    p.data .-= η .* p.grad # 업데이트 적용\n    p.grad .= 0            # 기울기 0으로 clear\n  end\nend","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"update를 호출하면 파라미터 W와 b는 바뀌고 손실(loss)은 내려간다.","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"두가지는 짚고 넘어가자: 모델에서 훈련할 파라미터의 목록 (여기서는 [W, b]), 그리고 업데이트 진행 속도. 여기서의 업데이트는 간단한 gradient descent(경사 하강, x .-= η .* Δ) 였지만, 모멘텀(momentum)을 추가하는 것처럼 보다 어려운 것도 해보고 싶을 것이다.","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"여기서 변수를 얻는 것은 아무것도 아니지만, 레이어를 복잡하게 쌓는다면 골치 좀 아플 것이다.","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"julia> m = Chain(\n         Dense(10, 5, σ),\n         Dense(5, 2), softmax)\nChain(Dense(10, 5, NNlib.σ), Dense(5, 2), NNlib.softmax)","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"[m[1].W, m[1].b, ...] 이렇게 작성하는 것 대신, Flux에서 제공하는 params(m) 함수를 이용해 모델의 모든 파라미터의 목록을 구할 것이다.","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"julia> opt = SGD([W, b], 0.1) # Gradient descent(경사 하강)을 learning rate(학습 속도) 0.1 으로 한다\n(::#71) (generic function with 1 method)\n\njulia> opt() # `W`와 `b`를 변경하며 업데이트를 수행한다\n","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"최적화 함수는 파라미터 목록을 받아 위의 update와 같은 함수를 돌려준다. opt나 update를 훈련 루프(training loop)에 넘겨줄 수 있는데, 매번 데이터의 미니-배치(mini-batch)를 한 후에 최적화를 수행할 것이다.","category":"page"},{"location":"Flux/training/optimisers/#최적화-함수-참고-1","page":"최적화","title":"최적화 함수 참고","text":"","category":"section"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"모든 최적화 함수는 넘겨받은 파라미터를 업데이트 하는 함수를 돌려준다.","category":"page"},{"location":"Flux/training/optimisers/#","page":"최적화","title":"최적화","text":"Descent\nMomentum\nNesterov\nRMSProp\nADAM\nAdaMax\nADAGrad\nADADelta\nAMSGrad\nNADAM\nADAMW","category":"page"},{"location":"Flux/training/optimisers/#Flux.Optimise.Descent","page":"최적화","title":"Flux.Optimise.Descent","text":"Descent(η)\n\nClassic gradient descent optimiser with learning rate η. For each parameter p and its gradient δp, this runs p -= η*δp.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.Momentum","page":"최적화","title":"Flux.Optimise.Momentum","text":"Momentum(params, η = 0.01; ρ = 0.9)\n\nGradient descent with learning rate η and momentum ρ.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.Nesterov","page":"최적화","title":"Flux.Optimise.Nesterov","text":"Nesterov(eta, ρ = 0.9)\n\nGradient descent with learning rate  η and Nesterov momentum ρ.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.RMSProp","page":"최적화","title":"Flux.Optimise.RMSProp","text":"RMSProp(η = 0.001, ρ = 0.9)\n\nRMSProp optimiser. Parameters other than learning rate don't need tuning. Often a good choice for recurrent networks.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.ADAM","page":"최적화","title":"Flux.Optimise.ADAM","text":"ADAM(η = 0.001, β = (0.9, 0.999))\n\nADAM optimiser.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.AdaMax","page":"최적화","title":"Flux.Optimise.AdaMax","text":"AdaMax(params, η = 0.001; β1 = 0.9, β2 = 0.999, ϵ = 1e-08)\n\nAdaMax optimiser. Variant of ADAM based on the ∞-norm.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.ADAGrad","page":"최적화","title":"Flux.Optimise.ADAGrad","text":"ADAGrad(η = 0.1; ϵ = 1e-8)\n\nADAGrad optimiser. Parameters don't need tuning.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.ADADelta","page":"최적화","title":"Flux.Optimise.ADADelta","text":"ADADelta(ρ = 0.9, ϵ = 1e-8)\n\nADADelta optimiser. Parameters don't need tuning.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.AMSGrad","page":"최적화","title":"Flux.Optimise.AMSGrad","text":"AMSGrad(η = 0.001, β = (0.9, 0.999))\n\nAMSGrad optimiser. Parameters don't need tuning.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.NADAM","page":"최적화","title":"Flux.Optimise.NADAM","text":"NADAM(η = 0.001, β = (0.9, 0.999))\n\nNADAM optimiser. Parameters don't need tuning.\n\n\n\n\n\n","category":"type"},{"location":"Flux/training/optimisers/#Flux.Optimise.ADAMW","page":"최적화","title":"Flux.Optimise.ADAMW","text":"ADAMW((η = 0.001, β = (0.9, 0.999), decay = 0)\n\nADAMW fixing weight decay regularization in Adam.\n\n\n\n\n\n","category":"function"},{"location":"Flux/training/training/#훈련시키키(Training)-1","page":"훈련시키기","title":"훈련시키키(Training)","text":"","category":"section"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"모델을 훈련시키려면 세 가지가 필요하다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"목표 함수(objective function), 주어진 데이터를 얼만큼 잘 평가할 것인가.\n데이터 포인트의 묶음(A collection of data points)을 목표 함수에 넘겨줄 것이다.\n최적화 함수로 모델 파라미터를 적절하게 업데이트 할 것이다.","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"그리하여 Flux.train!는 다음과 같이 호출한다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"Flux.train!(objective, data, opt)","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"모델 동물원(model zoo)에 여러가지 예제가 있다.","category":"page"},{"location":"Flux/training/training/#손실-함수(Loss-Functions)-1","page":"훈련시키기","title":"손실 함수(Loss Functions)","text":"","category":"section"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"목표 함수는 반드시 모델과 대상(target)의 차이를 나타내는 숫자를 돌려주어야 한다 - 모델의 loss. 기초에서 정의한 loss 함수가 목표(an objective)로서 작동할 것이다. 모델의 관점에서 목표를 정의할 수도 있다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"julia> using Flux\n\njulia> m = Chain(\n         Dense(784, 32, σ),\n         Dense(32, 10), softmax)\nChain(Dense(784, 32, NNlib.σ), Dense(32, 10), NNlib.softmax)\n\njulia> loss(x, y) = Flux.mse(m(x), y)\nloss (generic function with 1 method)\n\n# 나중에\njulia> Flux.train!(loss, data, opt)","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"목표는 항상 m(x)의 예측과 대상 y의 거리를 측정하는 비용 함수(cost function)의 관점에서 정의된다. Flux는 mean squared error를 구하는 mse나, cross entropy loss를 구하는 crossentropy 같은 비용 함수를 내장하고 있다. 원한다면 직접 계산해 볼 수도 있다.","category":"page"},{"location":"Flux/training/training/#데이터세트(Datasets)-1","page":"훈련시키기","title":"데이터세트(Datasets)","text":"","category":"section"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"data 인자는 훈련할 데이터(보통 입력 x와 target 출력 y)의 묶음을 제공한다. 예를 들어, 딱 하나 있는 더미 데이터 세트는 다음과 같다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"x = rand(784)\ny = rand(10)\ndata = [(x, y)]","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"Flux.train!은 loss(x, y)을 호출하고, 기울기를 계산하며, 가중치(weights)를 업데이트하고 다음 데이터 포인트로 이동한다. 같은 데이터를 세 번 훈련시킬 수 있다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"data = [(x, y), (x, y), (x, y)]\n# 또는 아래와 같이\ndata = Iterators.repeated((x, y), 3)","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"x와 y는 별도로 읽어들어는 것이 보통이다. 이럴 경우에 zip을 쓸 수 있다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"xs = [rand(784), rand(784), rand(784)]\nys = [rand( 10), rand( 10), rand( 10)]\ndata = zip(xs, ys)","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"기본적으로 train!은 데이터를 오직 한번만 순회한다 (한 세대, a single \"epoch\"). 여러 세대를 돌리는 @epochs 매크로를 제공하고 있으니 REPL에서 다음과 같이 해 보자.","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"julia> using Flux: @epochs\n\njulia> @epochs 2 println(\"hello\")\nINFO: Epoch 1\nhello\nINFO: Epoch 2\nhello\n\njulia> @epochs 2 Flux.train!(...)\n# 두 세대에 걸쳐 훈련한다","category":"page"},{"location":"Flux/training/training/#컬백(Callbacks)-1","page":"훈련시키기","title":"컬백(Callbacks)","text":"","category":"section"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"train!은 cb 인자를 추가적으로 받는데, 컬백 함수를 줘서 훈련 과정을 지켜볼 수 있다. 예를 들면:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"train!(objective, data, opt, cb = () -> println(\"training\"))","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"컬백은 훈련 데이터의 배치(batch) 마다 호출된다. 좀더 적게 호출하려면 Flux.throttle(f, timeout)를 주어 f가 매 timeout 초 이상 호출되는 것을 막는다.","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"컬백을 사용하는 전형적인 방식은 다음과 같다:","category":"page"},{"location":"Flux/training/training/#","page":"훈련시키기","title":"훈련시키기","text":"test_x, test_y = # ... 테스트 데이터의 단일 배치(single batch) 만들기 ...\nevalcb() = @show(loss(test_x, test_y))\n\nFlux.train!(objective, data, opt,\n            cb = throttle(evalcb, 5))","category":"page"},{"location":"Flux/data/onehot/#원-핫-인코딩(One-Hot-Encoding)-1","page":"원-핫 인코딩","title":"원-핫 인코딩(One-Hot Encoding)","text":"","category":"section"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"참true, 거짓false 혹은 고양이cat, 강아지dog 와 같은 범주형 변수(categorical variables)로 인코딩 해 보자. \"one-of-k\" 또는 \"one-hot\" 형식이 되고 Flux는 onehot 함수로 쉽게 할 수 있다.","category":"page"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"julia> using Flux: onehot\n\njulia> onehot(:b, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n  true\n false\n\njulia> onehot(:c, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n false\n  true","category":"page"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"역함수는 argmax (불리언 이나 일반 확률 분포(general probability distribution)를 인자로 받는다) 이다.","category":"page"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"julia> argmax(ans, [:a, :b, :c])\n:c\n\njulia> argmax([true, false, false], [:a, :b, :c])\n:a\n\njulia> argmax([0.3, 0.2, 0.5], [:a, :b, :c])\n:c","category":"page"},{"location":"Flux/data/onehot/#배치(Batches)-1","page":"원-핫 인코딩","title":"배치(Batches)","text":"","category":"section"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"onehotbatch는 원-핫 벡터의 배치(batch, 매트릭스)를 만들어 준다. argmax는 매트릭스를 배치로 취급한다.","category":"page"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"julia> using Flux: onehotbatch\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3×3 Flux.OneHotMatrix:\n false   true  false\n  true  false   true\n false  false  false\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Array{Symbol,1}:\n  :b\n  :a\n  :b","category":"page"},{"location":"Flux/data/onehot/#","page":"원-핫 인코딩","title":"원-핫 인코딩","text":"위의 연산은 Array 대신 OneHotVector와 OneHotMatrix를 돌려준다. OneHotVector는 일반적인 벡터처럼 동작하는데 정수 인덱스를 바로 사용하여 불필요한 계산 비용이 들지 않도록 처리한다. 예를 들어 매트릭스와 원-핫 벡터을 곱하는 경우, 내부적으로는 매트릭스에서 관련된 행만을 잘라내는 식으로 처리한다.","category":"page"},{"location":"Flux/gpu/#GPU-지원-1","page":"GPU 지원","title":"GPU 지원","text":"","category":"section"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"GPU 같이 하드웨어 백엔드로 하는 배열 연산의 지원은 CuArrays와 같은 외부 패키지를 제공한다. Flux는 배열의 타입을 정하지 않았기에(agnostic) 모델 가중치(weights)와 데이터를 GPU에 옮겨주면 Flux가 이를 다룰 수 있다.","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"예를 들어, CuArrays (cu 컨버터로 변환)를 사용하여 기본 예제를 NVIDIA GPU에서 돌릴 수 있다.","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"using CuArrays\n\nW = cu(rand(2, 5)) # 2×5 CuArray\nb = cu(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = cu(rand(5)), cu(rand(2)) # 더미 데이터\nloss(x, y) # ~ 3","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"파라미터 (W, b)와 데이터 세트 (x, y)를 cuda 배열로 변환하였다. 도함수(derivatives)와 훈련 값은 전과 동일하다.","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"Dense 레이어나 Chain 같은 조립 모델(structured model)를 정의하였으면, 내부 파라미터를 변환시켜야 한다. Flux에서 제공하는 mapleaves 함수로 모델의 모든 파라미터를 한꺼번에 변경할 수 있다.","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"d = Dense(10, 5, σ)\nd = mapleaves(cu, d)\nd.W # Tracked CuArray\nd(cu(rand(10))) # CuArray output\n\nm = Chain(Dense(10, 5, σ), Dense(5, 2), softmax)\nm = mapleaves(cu, m)\nd(cu(rand(10)))","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"편의상 Flux는 gpu 함수를 제공하여 GPU가 이용 가능한 경우 모델과 데이터를 GPU로 변환하게 한다. 그냥은 암것도 안하지만 CuArrays 를 로딩(using CuArrays)한 경우는 데이터를 GPU에 옮겨준다.","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"julia> using Flux, CuArrays\n\njulia> m = Dense(10,5) |> gpu\nDense(10, 5)\n\njulia> x = rand(10) |> gpu\n10-element CuArray{Float32,1}:\n 0.800225\n ⋮\n 0.511655\n\njulia> m(x)\nTracked 5-element CuArray{Float32,1}:\n -0.30535\n ⋮\n -0.618002","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"비슷한 용도로 cpu는 모델과 데이터를 GPU에서 그만돌리게 한다.","category":"page"},{"location":"Flux/gpu/#","page":"GPU 지원","title":"GPU 지원","text":"julia> x = rand(10) |> gpu\n10-element CuArray{Float32,1}:\n 0.235164\n ⋮\n 0.192538\n\njulia> x |> cpu\n10-element Array{Float32,1}:\n 0.235164\n ⋮\n 0.192538","category":"page"},{"location":"Flux/saving/#모델을-저장하고-불러오기-1","page":"저장 & 불러오기","title":"모델을 저장하고 불러오기","text":"","category":"section"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"모델을 저장하고는 차후에 이를 불러들여 실행하고 싶은가. 가장 쉬운 방법은 BSON.jl 이다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"모델을 저장하자:","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"julia> using Flux\n\njulia> model = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" model","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"불러오기:","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"julia> using Flux\n\njulia> using BSON: @load\n\njulia> @load \"mymodel.bson\" model\n\njulia> model\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"모델은 보통의 줄리아 타입이다. 따라서 줄리아 저장 포맷이면 어느 것이라도 사용할 수 있다. BSON.jl은 특히 잘 지원하며 앞으로도 되도록 호환을 유지한다 (지금 저장한 모델이 Flux의 차후 버전에서도 불러들일 수 있게).","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"note: Note\nGPU에 모델의 가중치를 저장하였으면, GPU 지원이 안되는 경우에는 이를 불러 들일 수 없다. 저장하기 전에 모델을 CPU로 돌려놓기 에서의 cpu(model)를 해주는게 가장 좋은 방법이다.","category":"page"},{"location":"Flux/saving/#모델-가중치-저장하기-1","page":"저장 & 불러오기","title":"모델 가중치 저장하기","text":"","category":"section"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"어떤 경우는 저장은 모델 파라미터만 하고 코드에서 모델 아키텍처를 재구성하는게 유용한 방법일 수 있다. params(model)로 모델 파라미터를 구할 수 있다. data.(params)을 하면 추적 내역 데이터를 지울 수 있다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"julia> using Flux\n\njulia> model = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> weights = Tracker.data.(params(model));\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" weights","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"Flux.loadparams!로 쉽게 모델에 파라미터를 불러들일 수 있다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"julia> using Flux\n\njulia> model = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> using BSON: @load\n\njulia> @load \"mymodel.bson\" weights\n\njulia> Flux.loadparams!(model, weights)","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"새로 뜬 model은 전에 파라미터 저장한 것과 일치한다.","category":"page"},{"location":"Flux/saving/#체크포인팅-1","page":"저장 & 불러오기","title":"체크포인팅","text":"","category":"section"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"장시간 훈련에 있어 주기적으로 모델을 저장하는 것은 참 좋은 생각이다. 그러면 훈련이 중단되어도 (파워가 나가는 등등의 이유로) 다시 재개할 수 있다. 그러기 위해서는 train!의 컬백 함수에서 모델을 저장하면 된다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"using Flux: throttle\nusing BSON: @save\n\nm = Chain(Dense(10,5,relu),Dense(5,2),softmax)\n\nevalcb = throttle(30) do\n  # loss 보기\n  @save \"model-checkpoint.bson\" model\nend","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"이러면 \"model-checkpoint.bson\" 파일을 30초 마다 업데이트 한다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"훈련시키는 동안에 모델을 연달아 저장하는 까리한 방법도 있는데 예를 들면","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"@save \"model-$(now()).bson\" model","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"이렇게 하면 \"model-2018-03-06T02:57:10.41.bson\"과 같이 연달아서 모델이 저장된다. 현 테스트 세트 loss도 저장할 수 있어서, 오버피팅 시작한다 싶으면 이전 사본의 모델로 복구를 쉽게 할 수 있다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"@save \"model-$(now()).bson\" model loss = testloss()","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"모델의 최적화 상태까지도 저장할 수 있으니, 정확하게 중단된 지점부터 이어 훈련을 재개할 수 있다.","category":"page"},{"location":"Flux/saving/#","page":"저장 & 불러오기","title":"저장 & 불러오기","text":"opt = ADAM(params(model))\n@save \"model-$(now()).bson\" model opt","category":"page"},{"location":"Flux/community/#커뮤니티-1","page":"커뮤니티","title":"커뮤니티","text":"","category":"section"},{"location":"Flux/community/#","page":"커뮤니티","title":"커뮤니티","text":"모든 Flux 사용자는 커뮤니티 참여에 적극 환영한다. Julia 포럼, 슬랙 (채널 #machine-learning), 또는 Flux의 Gitter 를 이용하자. 질문이나 이슈에 대해 도울 것이다.","category":"page"},{"location":"Flux/community/#","page":"커뮤니티","title":"커뮤니티","text":"Flux를 해킹하는데 관심이 있으면, 소스 코드는 열려 있고 이해하기 쉽다 – 일반적인 줄리아 코드로 되어 있다. intro 이슈 부터 관심있게 살펴보고 시작해 보자.","category":"page"},{"location":"Flux/community/#","page":"커뮤니티","title":"커뮤니티","text":"🦉 번역 완료 2018-03-12","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"🦉  https://github.com/MikeInnes/DataFlow.jl/blob/master/docs/vertices.md 번역","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"DataFlow가 하는 두가지:","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"그래프 데이터 구조(a graph data structure)\n그래프를 기술하기 위한 공통 문법(a common syntax for describing graphs)","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"서로 얶매여 있지 않으니 편하게 쓰면 된다;  예를 들어, 이 문법으로 만든 그래프를 인접 행렬(an adjacency matrix)로 변환하여 처리한다거나 DataFlow의 공통 그래프 연산 라이브러리를 활용하여 다른 방식으로 그래프를 생성할 수 있다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"DataFlow는 명시적으로 데이터 구조를 단순하게 유지하고, 다른 어떠한 의미도 덧붙이지 않는다. 그래프는 반듯한 줄리아 프로그램, 베이시안 네트워크, 또는 전기 회로와 같은 것을 표현할 수 있다. (Flux와 같은) DataFlow를 사용하는 라이브러리는 문법을 확장할 수 있기를 원하며 응용 프로그램에 적절한 코드를 생성시키는 그래프를 다룰 것이다.","category":"page"},{"location":"DataFlow/vertices/#데이터-구조-1","page":"DataFlow 버티스(vertices)","title":"데이터 구조","text":"","category":"section"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"DataFlow는 DVertex와 IVertex라는 두 개의 데이터 구조가 있다.  둘은 그래프의 노드(nodes)가 다른 노드와  입력/출력(inputs/outputs)이 어디에/어디에서(to/from) 이뤄질 것인지 표현한다. IVertex는 링크드 리스트(a linked list) 처럼 입력-연결(input-linked) 이다 - 입력으로   사용되는 노드에 대한 참조(a reference)를 유지한다. DVertex는 이중으로 연결한 것으로 더블-링크드 리스트(doubly-linked list)에 해당한다 - 입력과 이것을 입력으로 갖는 모든 노드를 참조한다. DVertex는 기술적으론 표현력이 더 좋지만 작업하기도 더 빡세니까, 가능하다면 입력-연결(input-linked)로 바꿔서 쓰는게 최선이다 (DataFlow.il()로 할 수 있다).","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"# src/graph/graph.jl\nabstract type Vertex{T} end\n\n# src/graph/ilgraph.jl\nstruct IVertex{T} <: Vertex{T}\n  value::T\n  inputs::Vector{IVertex{T}}\n  # outputs::Set{IVertex{T}} # DVertex는 요걸 추가\nend","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"IVertex는 줄리아의 Expr 객체와 유사하다. 예를 들어, 다음과 같이 표현식 x+length(xs) 를 비슷한 방식으로서 저장한다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> using DataFlow\n\njulia> x = 2\n2\n\njulia> Expr(:call, :+, x, Expr(:call, :length, :xs))\n:(2 + length(xs))\n\njulia> IVertex(:+, IVertex(:x), IVertex(:length, IVertex(:xs)))\nIVertex{Symbol}(x() + length(xs()))","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"주요한 차이는 객체 아이덴티티(object identity)가 DataFlow 그래프에서는 중요하다는 것이다. 다음과 같이 구문 표현 트리(an expression tree)를 만들면:","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> foo = Expr(:call, :length, :xs)\n:(length(xs))\n\njulia> Expr(:call, :+, foo, foo)\n:(length(xs) + length(xs))","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"length(xs) 표현식을 재사용 했음에도 length(xs)+length(xs)를 출력한다. DataFlow의 재사용은 큰 차이가 있다:","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> g = IVertex{Any}\nIVertex\n\njulia> g(:+, g(:length, g(:xs)), g(:length, g(:xs)))\nIVertex(length(xs()) + length(xs()))\n\njulia> foo = g(:length, g(:xs))\nIVertex(length(xs()))\n\njulia> g(:+, foo, foo)\nIVertex(\neland = length(xs())\neland + eland)","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"재사용 한 것이 프로그램 그래프에 인코드 되었다. 위의 데이터 구조에서는 \"변수\" 개념이 없는데 데이터의 흐름이 직접 표현되었기 때문이다; 대신 문법 변환에서 변수가 필요해지면 그 때 생성될 것이다.","category":"page"},{"location":"DataFlow/vertices/#알고리즘-1","page":"DataFlow 버티스(vertices)","title":"알고리즘","text":"","category":"section"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"DataFlow 그래프를 다루는 기본 접근 방식은 함수형 프로그래밍에서 트리(tree)를 다루는 테크닉과 같은 것을 사용한다. 그러니까, 재귀적으로(recursively) 이전 것을 밟아나가며 새로운 그래프를 생성하는 알고리즘을 만들도록 하자. 그래프에 있는 각 노드에 특정 함수를 적용(apply) 시키는 함수로서, prewalk 와 postwalk 같은게 패키지에 들어 있다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"예를 보자:","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> using DataFlow: postwalk, value\n\njulia> foo = g(:+, g(:length, g(:xs)), g(:length, g(:ys)))\nIVertex(length(xs()) + length(ys()))\n\njulia> postwalk(foo) do v\n         value(v) == :length && value(v[1]) == :xs ? g(:lx) : v\n       end\nIVertex(lx() + length(ys()))","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"(pre- 와 postwalk의 차이는 순회(traversal)하는 순서에 있다. @show 를 통해서 볼 수 있다.) 이 방법으로 그래프에서 찾기(find), 바꾸기(replace) 같은 것을 하거나, 더욱 복잡한 구조 변환에 적용할 수 있다. 그럼 이제 공통 부분 표현식 제거(cse, common subexpression elimination)를 하는 기본적인 방법을 은연 중에 터득했으니 한번 구현해 보자:","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> cse(v::IVertex, cache = Dict()) =\n         postwalk(x -> get!(cache, x, x), v)\ncse (generic function with 2 methods)","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"(역주: get!은 사전(Dict)에 없는 키를 저장한다.)","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> d = Dict(\"a\"=>1, \"b\"=>2, \"c\"=>3);\n\njulia> get!(d, \"a\", 5)\n1\n\njulia> get!(d, \"d\", 4)\n4\n\njulia> d\nDict{String,Int64} with 4 entries:\n  \"c\" => 3\n  \"b\" => 2\n  \"a\" => 1\n  \"d\" => 4","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"그래프의 각 노드가 사전(Dict) 타입에 끌어들이고 값들은 자기 자신을 참조(refer) 한다. 이것으로 결과 그래프의 어느 값이든 ==는 ===와 마찬가지인게 (===는 identical 비교) 확실해지며, 공통 표현식은 재사용된다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> foo = @flow length(xs)+length(xs)\nIVertex(length(xs) + length(xs))\n\njulia> cse(foo)\nIVertex(\neland = length(xs)\neland + eland)","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"일반적으로 DataFlow의 postwalk와 같은 고급 연산에 능숙해야 하지만, 어떤 경우에는 처음부터 재귀 알고리즘을 직접 짜야 할 때도 있다. 트리(a tree)에 적용하는 알고리즘과 같아 보이지만 주의 사항이 있는데 (1) 동일한 노드(identical nodes)가 트리에서 여러 번 도달할 수 있다. (2) 재귀하다 그래프에서 사이클(cyle)이 발생하여 무한 루프에 빠질 수 있다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"잘못하면 악몽처럼 보이지만 사실은 일석이조인 것이; 함수를 memoize 하여 노드를 반복해서 방문하면 재귀를 끝내게 하자. 그리고 재귀하기 전에는 현재 호출의 결과를 꼭 캐시(cache) 하도록 한다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"julia> using DataFlow: value, inputs, thread!\n\njulia> function replace_xs(g, cache = ObjectIdDict())\n         # 이 노드를 이미 처리한 경우에는 빠른 종료\n         haskey(cache, g) && return cache[g]\n         # 새로운 (비어있는) 노드를 만들고 캐시 해 둠\n         cache[g] = g′ = typeof(g)(value(g) == :xs ? :foo : value(g))\n         # 원래 노드의 입력은 처리하고 결과를 새로운 노드에 추가\n         thread!(g′, (replace_xs(v, cache) for v in inputs(g))...)\n       end\nreplace_xs (generic function with 2 methods)\n\njulia> foo = DataFlow.cse(@flow length(xs)+length(xs))\nIVertex(\nalligator = length(xs)\nalligator + alligator)\n\njulia> replace_xs(foo)\nIVertex(\nalligator = length(xs)\nalligator + alligator)","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"여기서는 캐시하는 것을 잊어도 length(foo)+length(foo) 하다 망하진 않는데, 다른 경우에는 도중에 멈출 수 있다.","category":"page"},{"location":"DataFlow/vertices/#","page":"DataFlow 버티스(vertices)","title":"DataFlow 버티스(vertices)","text":"🦉  번역 완료 2018-03-15","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"🦉  https://github.com/FluxML/Zygote.jl 자료를 번역하는 곳입니당","category":"page"},{"location":"Zygote/#Zygote-1","page":"Home","title":"Zygote","text":"","category":"section"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Welcome! Zygote extends the Julia language to support differentiable programming. With Zygote you can write down any Julia code you feel like – including using existing Julia packages – then get gradients and optimise your program. Deep learning, ML and probabilistic programming are all different kinds of differentiable programming that you can do with Zygote.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"At least, that's the idea. We're still in beta so expect some adventures.","category":"page"},{"location":"Zygote/#Setup-1","page":"Home","title":"Setup","text":"","category":"section"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Zygote is still moving quickly and it's best to work from the development branches. Run this in a Julia session:","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"using Pkg; pkg\"add Zygote#master\"","category":"page"},{"location":"Zygote/#Taking-Gradients-1","page":"Home","title":"Taking Gradients","text":"","category":"section"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Zygote is easy to understand since, at its core, it has a one-function API (forward), along with a few simple conveniences. Before explaining forward, we'll look at the higher-level function gradient.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"gradient calculates derivatives. For example, the derivative of 3x^2 + 2x + 1 is 6x + 2, so when x = 5, dx = 32.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> using Zygote\n\njulia> gradient(x -> 3x^2 + 2x + 1, 5)\n(32,)","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"gradient returns a tuple, with a gradient for each argument to the function.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> gradient((a, b) -> a*b, 2, 3)\n(3, 2)","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"This will work equally well if the arguments are arrays, structs, or any other Julia type, but the function should return a scalar (like a loss or objective l, if you're doing optimisation / ML).","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> W = rand(2, 3); x = rand(3);\n\njulia> gradient(W -> sum(W*x), W)[1]\n2×3 Array{Float64,2}:\n 0.0462002  0.817608  0.979036\n 0.0462002  0.817608  0.979036\n\njulia> gradient(x -> 3x^2 + 2x + 1, 1//4)\n(7//2,)","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Control flow is fully supported, including recursion.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> function pow(x, n)\n         r = 1\n         for i = 1:n\n           r *= x\n         end\n         return r\n       end\npow (generic function with 1 method)\n\njulia> gradient(x -> pow(x, 3), 5)\n(75,)\n\njulia> pow2(x, n) = n <= 0 ? 1 : x*pow2(x, n-1)\npow2 (generic function with 1 method)\n\njulia> gradient(x -> pow2(x, 3), 5)\n(75,)","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Data structures are also supported, including mutable ones like dictionaries. Arrays are currently immutable, though this may change in future.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> d = Dict()\nDict{Any,Any} with 0 entries\n\njulia> gradient(5) do x\n         d[:x] = x\n         d[:x] * d[:x]\n       end\n(10,)\n\njulia> d[:x]\n5","category":"page"},{"location":"Zygote/#Structs-and-Types-1","page":"Home","title":"Structs and Types","text":"","category":"section"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Julia makes it easy to work with custom types, and Zygote makes it easy to differentiate them. For example, given a simple Point type:","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"import Base: +, -\n\nstruct Point\n  x::Float64\n  y::Float64\nend\n\na::Point + b::Point = Point(a.x + b.x, a.y + b.y)\na::Point - b::Point = Point(a.x - b.x, a.y - b.y)\ndist(p::Point) = sqrt(p.x^2 + p.y^2)","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> a = Point(1, 2)\nPoint(1.0, 2.0)\n\njulia> b = Point(3, 4)\nPoint(3.0, 4.0)\n\njulia> dist(a + b)\n7.211102550927978\n\njulia> gradient(a -> dist(a + b), a)[1]\n(x = 0.5547001962252291, y = 0.8320502943378437)","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Zygote's default representation of the \"point adjoint\" is a named tuple with gradients for both fields, but this can of course be customised too.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"This means we can do something very powerful: differentiating through Julia libraries, even if they weren't designed for this. For example, colordiff might be a smarter loss function on colours than simple mean-squared-error:","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> using Colors\n\njulia> colordiff(RGB(1, 0, 0), RGB(0, 1, 0))\n86.60823557376344\n\njulia> gradient(colordiff, RGB(1, 0, 0), RGB(0, 1, 0))\n((r = 0.4590887719632896, g = -9.598786801605689, b = 14.181383399012862), (r = -1.7697549557037275, g = 28.88472330558805, b = -0.044793892637761346))","category":"page"},{"location":"Zygote/#Gradients-of-ML-models-1","page":"Home","title":"Gradients of ML models","text":"","category":"section"},{"location":"Zygote/#","page":"Home","title":"Home","text":"It's easy to work with even very large and complex models, and there are few ways to do this. Autograd-style models pass around a collection of weights.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> linear(θ, x) = θ[:W] * x .+ θ[:b]\nlinear (generic function with 1 method)\n\njulia> x = rand(5);\n\njulia> θ = Dict(:W => rand(2, 5), :b => rand(2))\nDict{Any,Any} with 2 entries:\n  :b => [0.0430585, 0.530201]\n  :W => [0.923268 … 0.589691]\n\n# Alternatively, use a named tuple or struct rather than a dict.\n# θ = (W = rand(2, 5), b = rand(2))\n\njulia> θ̄ = gradient(θ -> sum(linear(θ, x)), θ)[1]\nDict{Any,Any} with 2 entries:\n  :b => [1.0, 1.0]\n  :W => [0.628998 … 0.433006]","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"An extension of this is the Flux-style model in which we use call overloading to combine the weight object with the forward pass (equivalent to a closure).","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> struct Linear\n         W\n         b\n       end\n\njulia> (l::Linear)(x) = l.W * x .+ l.b\n\njulia> model = Linear(rand(2, 5), rand(2))\nLinear([0.267663 … 0.334385], [0.0386873, 0.0203294])\n\njulia> dmodel = gradient(model -> sum(model(x)), model)[1]\n(W = [0.652543 … 0.683588], b = [1.0, 1.0])","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"Zygote also support one more way to take gradients, via implicit parameters – this is a lot like autograd-style gradients, except we don't have to thread the parameter collection through all our code.","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"julia> W = rand(2, 5); b = rand(2);\n\njulia> linear(x) = W * x .+ b\nlinear (generic function with 2 methods)\n\njulia> grads = gradient(() -> sum(linear(x)), Params([W, b]))\nGrads(...)\n\njulia> grads[W], grads[b]\n([0.652543 … 0.683588], [1.0, 1.0])","category":"page"},{"location":"Zygote/#","page":"Home","title":"Home","text":"However, implicit parameters exist mainly for compatibility with Flux's current AD; it's recommended to use the other approaches unless you need this.","category":"page"},{"location":"Zygote/adjoints/#Custom-Adjoints-1","page":"Custom Adjoints","title":"Custom Adjoints","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"The @adjoint macro is an important part of Zygote's interface; customising your backwards pass is not only possible but widely used and encouraged. While there are specific utilities available for common things like gradient clipping, understanding adjoints will give you the most flexibility. We first give a bit more background on what these pullback things are.","category":"page"},{"location":"Zygote/adjoints/#Pullbacks-1","page":"Custom Adjoints","title":"Pullbacks","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"gradient is really just syntactic sugar around the more fundamental function forward.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> y, back = Zygote.forward(sin, 0.5);\n\njulia> y\n0.479425538604203","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"forward gives two outputs: the result of the original function, sin(0.5), and a pullback, here called back. back implements the gradient computation for sin, accepting a derivative and producing a new one. In mathematical terms, it implements a vector-Jacobian product. Where y = f(x) and the gradient fracpartial lpartial x is written barx, the pullback mathcalB_y computes:","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"barx = fracpartial lpartial x = fracpartial lpartial y fracpartial ypartial x = mathcalB_y(bary)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"To make this concrete, take the function y = sin(x). fracpartial ypartial x = cos(x), so the pullback is bary cos(x). In other words forward(sin, x) behaves the same as","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"dsin(x) = sin(x), ȳ -> (ȳ * cos(x),)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"gradient takes a function l = f(x) and assumes l = fracpartial lpartial l = 1 and feeds this in to the pullback. In the case of sin,","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> function gradsin(x)\n         _, back = dsin(x)\n         back(1)\n       end\ngradsin (generic function with 1 method)\n\njulia> gradsin(0.5)\n(0.8775825618903728,)\n\njulia> cos(0.5)\n0.8775825618903728","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"More generally","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> function mygradient(f, x...)\n         _, back = Zygote.forward(f, x...)\n         back(1)\n       end\nmygradient (generic function with 1 method)\n\njulia> mygradient(sin, 0.5)\n(0.8775825618903728,)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"The rest of this section contains more technical detail. It can be skipped if you only need an intuition for pullbacks; you generally won't need to worry about it as a user.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"If x and y are vectors, fracpartial ypartial x becomes a Jacobian. Importantly, because we are implementing reverse mode we actually left-multiply the Jacobian, i.e. v'J, rather than the more usual J*v. Transposing v to a row vector and back (v'J)' is equivalent to J'v so our gradient rules actually implement the adjoint of the Jacobian. This is relevant even for scalar code: the adjoint for y = sin(x) is x̄ = sin(x)'*ȳ; the conjugation is usually moot but gives the correct behaviour for complex code. \"Pullbacks\" are therefore sometimes called \"vector-Jacobian products\" (VJPs), and we refer to the reverse mode rules themselves as \"adjoints\".","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"Zygote has many adjoints for non-mathematical operations such as for indexing and data structures. Though these can still be seen as linear functions of vectors, it's not particularly enlightening to implement them with an actual matrix multiply. In these cases it's easiest to think of the adjoint as a kind of inverse. For example, the gradient of a function that takes a tuple to a struct (e.g. y = Complex(a, b)) will generally take a struct to a tuple ((ȳ.re, ȳ.im)). The gradient of a getindex y = x[i...] is a setindex! x̄[i...] = ȳ, etc.","category":"page"},{"location":"Zygote/adjoints/#Custom-Adjoints-2","page":"Custom Adjoints","title":"Custom Adjoints","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"We can extend Zygote to a new function with the @adjoint function.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> mul(a, b) = a*b\n\njulia> using Zygote: @adjoint\n\njulia> @adjoint mul(a, b) = mul(a, b), c̄ -> (c̄*b, c̄*a)\n\njulia> gradient(mul, 2, 3)\n(3, 2)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"It might look strange that we write mul(a, b) twice here. In this case we want to call the normal mul function for the forward pass, but you may also want to modify the forward pass (for example, to capture intermediate results in the pullback).","category":"page"},{"location":"Zygote/adjoints/#Custom-Types-1","page":"Custom Adjoints","title":"Custom Types","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"One good use for custom adjoints is to customise how your own types behave during differentiation. For example, in our Point example we noticed that the adjoint is a named tuple, rather than another point.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"import Base: +, -\n\nstruct Point\n  x::Float64\n  y::Float64\nend\n\nwidth(p::Point) = p.x\nheight(p::Point) = p.y\n\na::Point + b::Point = Point(width(a) + width(b), height(a) + height(b))\na::Point - b::Point = Point(width(a) - width(b), height(a) - height(b))\ndist(p::Point) = sqrt(width(p)^2 + height(p)^2)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> gradient(a -> dist(a), Point(1, 2))[1]\n(x = 0.5547001962252291, y = 0.8320502943378437)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"Fundamentally, this happens because of Zygote's default adjoint for getfield.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> gradient(a -> a.x, Point(1, 2))\n((x = 1, y = nothing),)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"We can overload this by modifying the getters height and width.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> @adjoint width(p::Point) = p.x, x̄ -> (Point(x̄, 0),)\n\njulia> @adjoint height(p::Point) = p.y, ȳ -> (Point(0, ȳ),)\n\njulia> Zygote.refresh() # currently needed when defining new adjoints\n\njulia> gradient(a -> height(a), Point(1, 2))\n(Point(0.0, 1.0),)\n\njulia> gradient(a -> dist(a), Point(1, 2))[1]\nPoint(0.4472135954999579, 0.8944271909999159)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"If you do this you should also overload the Point constructor, so that it can handle a Point gradient (otherwise this function will error).","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> @adjoint Point(a, b) = Point(a, b), p̄ -> (p̄.x, p̄.y)\n\njulia> gradient(x -> dist(Point(x, 1)), 1)\n(0.7071067811865475,)","category":"page"},{"location":"Zygote/adjoints/#Advanced-Adjoints-1","page":"Custom Adjoints","title":"Advanced Adjoints","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"We usually use custom adjoints to add gradients that Zygote can't derive itself (for example, because they ccall to BLAS). But there are some more advanced and fun things we can to with @adjoint.","category":"page"},{"location":"Zygote/adjoints/#Gradient-Hooks-1","page":"Custom Adjoints","title":"Gradient Hooks","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> hook(f, x) = x\nhook (generic function with 1 method)\n\njulia> @adjoint hook(f, x) = x, x̄ -> (nothing, f(x̄))","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"hook doesn't seem that interesting, as it doesn't do anything. But the fun part is in the adjoint; it's allowing us to apply a function f to the gradient of x.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> gradient((a, b) -> hook(-, a)*b, 2, 3)\n(-3, 2)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"We could use this for debugging or modifying gradients (e.g. gradient clipping).","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> gradient((a, b) -> hook(ā -> @show(ā), a)*b, 2, 3)\nā = 3\n(3, 2)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"Zygote provides both hook and @showgrad so you don't have to write these yourself.","category":"page"},{"location":"Zygote/adjoints/#Checkpointing-1","page":"Custom Adjoints","title":"Checkpointing","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"A more advanced example is checkpointing, in which we save memory by re-computing the forward pass of a function during the backwards pass. To wit:","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> checkpoint(f, x) = f(x)\ncheckpoint (generic function with 1 method)\n\njulia> @adjoint checkpoint(f, x) = f(x), ȳ -> Zygote._forward(f, x)[2](ȳ)\n\njulia> gradient(x -> checkpoint(sin, x), 1)\n(0.5403023058681398,)","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"If a function has side effects we'll see that the forward pass happens twice, as expected.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> foo(x) = (println(x); sin(x))\nfoo (generic function with 1 method)\n\njulia> gradient(x -> checkpoint(foo, x), 1)\n1\n1\n(0.5403023058681398,)","category":"page"},{"location":"Zygote/adjoints/#Gradient-Reflection-1","page":"Custom Adjoints","title":"Gradient Reflection","text":"","category":"section"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"It's easy to check whether the code we're running is currently being differentiated.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"isderiving() = false\n\n@adjoint isderiving() = true, _ -> nothing","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"A more interesting example is to actually detect how many levels of nesting are going on.","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"nestlevel() = 0\n\n@adjoint nestlevel() = nestlevel()+1, _ -> nothing","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"Demo:","category":"page"},{"location":"Zygote/adjoints/#","page":"Custom Adjoints","title":"Custom Adjoints","text":"julia> function f(x)\n         println(nestlevel(), \" levels of nesting\")\n         return x\n       end\nf (generic function with 1 method)\n\njulia> grad(f, x) = gradient(f, x)[1]\ngrad (generic function with 1 method)\n\njulia> f(1);\n0 levels of nesting\n\njulia> grad(f, 1);\n1 levels of nesting\n\njulia> grad(x -> x*grad(f, x), 1);\n2 levels of nesting","category":"page"},{"location":"Zygote/utils/#Utilities-1","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"Zygote/utils/#","page":"Utilities","title":"Utilities","text":"Zygote provides a set of helpful utilities. These are all \"user-level\" tools – in other words you could have written them easily yourself, but they live in Zygote for convenience.","category":"page"},{"location":"Zygote/utils/#","page":"Utilities","title":"Utilities","text":"Zygote.@showgrad\nZygote.hook\nZygote.dropgrad\nZygote.hessian\nZygote.Buffer\nZygote.forwarddiff","category":"page"},{"location":"Zygote/utils/#Zygote.@showgrad","page":"Utilities","title":"Zygote.@showgrad","text":"@showgrad(x) -> x\n\nMuch like @show, but shows the gradient about to accumulate to x. Useful for debugging gradients.\n\njulia> gradient(2, 3) do a, b\n         @showgrad(a)*b\n       end\n∂(a) = 3\n(3, 2)\n\nNote that the gradient depends on how the output of @showgrad is used, and is not the overall gradient of the variable a. For example:\n\njulia> gradient(2) do a\n     @showgrad(a)*a\n   end\n∂(a) = 2\n(4,)\n\njulia> gradient(2, 3) do a, b\n         @showgrad(a) # not used, so no gradient\n         a*b\n       end\n∂(a) = nothing\n(3, 2)\n\n\n\n\n\n","category":"macro"},{"location":"Zygote/utils/#Zygote.hook","page":"Utilities","title":"Zygote.hook","text":"hook(x̄ -> ..., x) -> x\n\nGradient hooks. Allows you to apply an arbitrary function to the gradient for x.\n\njulia> gradient(2, 3) do a, b\n         hook(ā -> @show(ā), a)*b\n       end\nā = 3\n(3, 2)\n\njulia> gradient(2, 3) do a, b\n         hook(-, a)*b\n       end\n(-3, 2)\n\n\n\n\n\n","category":"function"},{"location":"Zygote/utils/#Zygote.dropgrad","page":"Utilities","title":"Zygote.dropgrad","text":"dropgrad(x) -> x\n\nDrop the gradient of x.\n\njulia> gradient(2, 3) do a, b\n     dropgrad(a)*b\n   end\n(nothing, 2)\n\n\n\n\n\n","category":"function"},{"location":"Zygote/utils/#Zygote.hessian","page":"Utilities","title":"Zygote.hessian","text":"hessian(f, x)\n\nConstruct the Hessian of f, where x is a real or real array and f(x) is a real.\n\njulia> hessian(((a, b),) -> a*b, [2, 3])\n2×2 Array{Int64,2}:\n 0  1\n 1  0\n\n\n\n\n\n","category":"function"},{"location":"Zygote/utils/#Zygote.Buffer","page":"Utilities","title":"Zygote.Buffer","text":"Buffer(xs, ...)\n\nBuffer is an array-like type which is mutable when taking gradients. You can construct a Buffer with the same syntax as similar (e.g. Buffer(xs, 5)) and then use normal indexing. Finally, use copy to get back a normal array.\n\nFor example:\n\njulia> function vstack(xs)\n           buf = Buffer(xs, length(xs), 5)\n           for i = 1:5\n             buf[:, i] = xs\n           end\n           return copy(buf)\n         end\nvstack (generic function with 1 method)\n\njulia> vstack([1, 2, 3])\n3×5 Array{Int64,2}:\n 1  1  1  1  1\n 2  2  2  2  2\n 3  3  3  3  3\n\njulia> gradient(x -> sum(vstack(x)), [1, 2, 3])\n([5.0, 5.0, 5.0],)\n\nBuffer is not an AbstractArray and can't be used for linear algebra operations like matrix multiplication. This prevents it from being captured by pullbacks.\n\ncopy is a semantic copy, but does not allocate memory. Instead the Buffer is made immutable after copying.\n\n\n\n\n\n","category":"type"},{"location":"Zygote/utils/#Zygote.forwarddiff","page":"Utilities","title":"Zygote.forwarddiff","text":"forwarddiff(f, x) -> f(x)\n\nRuns f(x) as usual, but instructs Zygote to differentiate f using forward mode, rather than the usual reverse mode.\n\nForward mode takes time linear in length(x) but only has constant memory overhead, and is very efficient for scalars, so in some cases this can be a useful optimisation.\n\njulia> function pow(x, n)\n         r = one(x)\n         for i = 1:n\n           r *= x\n         end\n         return r\n       end\npow (generic function with 1 method)\n\njulia> gradient(5) do x\n         forwarddiff(x) do x\n           pow(x, 2)\n         end\n       end\n(10,)\n\nNote that the function f will drop gradients for any closed-over values.\n\njulia> gradient(2, 3) do a, b\n         forwarddiff(a) do a\n           a*b\n         end\n       end\n(3, nothing)\n\nThis can be rewritten by explicitly passing through b, i.e.\n\ngradient(2, 3) do a, b\n  forwarddiff([a, b]) do (a, b)\n    a*b\n  end\nend\n\n\n\n\n\n","category":"function"},{"location":"Zygote/complex/#Complex-Differentiation-1","page":"Complex Differentiation","title":"Complex Differentiation","text":"","category":"section"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"Complex numbers add some difficulty to the idea of a \"gradient\". To talk about gradient(f, x) here we need to talk a bit more about f.","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"If f returns a real number, things are fairly straightforward. For c = x + yi and  z = f(c), we can define the adjoint bar c = fracpartial zpartial x + fracpartial zpartial yi = bar x + bar y i (note that bar c means gradient, and c means conjugate). It's exactly as if the complex number were just a pair of reals (re, im). This works out of the box.","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"julia> gradient(c -> abs2(c), 1+2im)\n(2 + 4im,)","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"However, while this is a very pragmatic definition that works great for gradient descent, it's not quite aligned with the mathematical notion of the derivative: i.e. f(c + epsilon) approx f(c) + bar c epsilon. In general, such a bar c is not possible for complex numbers except when f is holomorphic (or analytic). Roughly speaking this means that the function is defined over c as if it were a normal real number, without exploiting its complex structure – it can't use real, imag, conj, or anything that depends on these like abs2 (abs2(x) = x*x'). (This constraint also means there's no overlap with the Real case above; holomorphic functions always return complex numbers for complex input.) But most \"normal\" numerical functions – exp, log, anything that can be represented by a Taylor series – are fine.","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"Fortunately it's also possible to get these derivatives; they are the conjugate of the gradients for the real part.","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"julia> gradient(x -> real(log(x)), 1+2im)[1] |> conj\n0.2 - 0.4im","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"We can check that this function is holomorphic – and thus that the gradient we got out is sensible – by checking the Cauchy-Riemann equations. In other words this should give the same answer:","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"julia> -im*gradient(x -> imag(log(x)), 1+2im)[1] |> conj\n0.2 - 0.4im","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"Notice that this fails in a non-holomorphic case, f(x) = log(x'):","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"julia> gradient(x -> real(log(x')), 1+2im)[1] |> conj\n0.2 - 0.4im\n\njulia> -im*gradient(x -> imag(log(x')), 1+2im)[1] |> conj\n-0.2 + 0.4im","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"In cases like these, all bets are off. The gradient can only be described with more information; either a 2x2 Jacobian (a generalisation of the Real case, where the second column is now non-zero), or by the two Wirtinger derivatives (a generalisation of the holomorphic case, where frac f z is now non-zero). To get these efficiently, as we would a Jacobian, we can just call the backpropagators twice.","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"function jacobi(f, x)\n  y, back = Zygote.forward(f, x)\n  back(1)[1], back(im)[1]\nend\n\nfunction wirtinger(f, x)\n  du, dv = jacobi(f, x)\n  (du' + im*dv')/2, (du + im*dv)/2\nend","category":"page"},{"location":"Zygote/complex/#","page":"Complex Differentiation","title":"Complex Differentiation","text":"julia> wirtinger(x -> 3x^2 + 2x + 1, 1+2im)\n(8.0 + 12.0im, 0.0 + 0.0im)\n\njulia> wirtinger(x -> abs2(x), 1+2im)\n(1.0 - 2.0im, 1.0 + 2.0im)","category":"page"},{"location":"Zygote/flux/#Flux-1","page":"Flux","title":"Flux","text":"","category":"section"},{"location":"Zygote/flux/#","page":"Flux","title":"Flux","text":"It's easy to use Zygote in place of Flux's default AD, Tracker, just by changing Tracker.gradient to Zygote.gradient. The API is otherwise the same.","category":"page"},{"location":"Zygote/flux/#","page":"Flux","title":"Flux","text":"julia> using Flux, Zygote\n\njulia> m = Chain(Dense(10, 5, relu), Dense(5, 2))\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2))\n\njulia> x = rand(10);\n\njulia> gs = gradient(() -> sum(m(x)), params(m))\nGrads(...)\n\njulia> gs[m[1].W]\n5×10 Array{Float32,2}:\n -0.255175  -1.2295   ...","category":"page"},{"location":"Zygote/flux/#","page":"Flux","title":"Flux","text":"You can use optimisers and update gradients as usual.","category":"page"},{"location":"Zygote/flux/#","page":"Flux","title":"Flux","text":"julia> opt = ADAM();\n\njulia> Flux.Optimise.update!(opt, params(m), gs)","category":"page"},{"location":"Zygote/profiling/#Debugging-in-Time-and-Space-1","page":"Profiling","title":"Debugging in Time and Space","text":"","category":"section"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"Because Zygote generates Julia code for the backwards pass, many of Julia's normal profiling and performance debugging tools work well on it out of the box.","category":"page"},{"location":"Zygote/profiling/#Performance-Profiling-1","page":"Profiling","title":"Performance Profiling","text":"","category":"section"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"Julia's sampling profiler is useful for understanding performance. We recommend running the profiler in Juno, but the terminal or ProfileView.jl also work well.","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"(Image: )","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"The bars indicate time taken in both the forwards and backwards passes at that line. The canopy chart on the right shows us each function call as a block, arranged so that when f calls g, g gets a block just below f, which is bigger the longer it took to run. If we dig down the call stack we'll eventually find the adjoints for things like matmul, which we can click on to view.","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"(Image: )","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"The trace inside the adjoint can be used to distinguish time taken by the forwards and backwards passes.","category":"page"},{"location":"Zygote/profiling/#Memory-Profiling-1","page":"Profiling","title":"Memory Profiling","text":"","category":"section"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"Reverse-mode AD typically uses memory proportional to the number of operations in the program, so long-running programs can also suffer memory usage issues. Zygote includes a space profiler to help debug these issues. Like the time profiler, it shows a canopy chart, but this time hovering over it displays the number of bytes stored by each line of the program.","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"(Image: )","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"Note that this currently only works inside Juno.","category":"page"},{"location":"Zygote/profiling/#Reflection-1","page":"Profiling","title":"Reflection","text":"","category":"section"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"Julia's code and type inference reflection tools can also be useful, though Zygote's use of closures can make the output noisy. To see the code Julia runs you should use the low-level _forward method and the pullback it returns. This will directly show either the derived adjoint code or the code for a custom adjoint, if there is one.","category":"page"},{"location":"Zygote/profiling/#","page":"Profiling","title":"Profiling","text":"julia> using Zygote: Context, _forward\n\njulia> add(a, b) = a+b\n\njulia> @code_typed _forward(Context(), add, 1, 2)\nCodeInfo(\n1 ─ %1 = (Base.getfield)(args, 1)::Int64\n│   %2 = (Base.getfield)(args, 2)::Int64\n│   %3 = (Base.add_int)(%1, %2)::Int64\n│   %4 = (Base.tuple)(%3, $(QuoteNode(∂(add))))::PartialTuple(Tuple{Int64,typeof(∂(add))}, Any[Int64, Const(∂(add), false)])\n└──      return %4\n) => Tuple{Int64,typeof(∂(add))}\n\njulia> y, back = _forward(Context(), add, 1, 2)\n(3, ∂(add))\n\njulia> @code_typed back(1)\nCodeInfo(\n1 ─ %1 = (Base.mul_int)(Δ, 1)::Int64\n│   %2 = (Base.mul_int)(Δ, 1)::Int64\n│   %3 = (Zygote.tuple)(nothing, %1, %2)::PartialTuple(Tuple{Nothing,Int64,Int64}, Any[Const(nothing, false), Int64, Int64])\n└──      return %3\n) => Tuple{Nothing,Int64,Int64}","category":"page"},{"location":"Zygote/internals/#Internals-1","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"Zygote/internals/#What-Zygote-Does-1","page":"Internals","title":"What Zygote Does","text":"","category":"section"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"These notebooks and the Zygote paper provide useful background on Zygote's transform; this page is particularly focused on implementation details.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Given a function like","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"function foo(x)\n  a = bar(x)\n  b = baz(a)\n  return b\nend","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"how do we differentiate it? The key is that we can differentiate foo if we can differentiate bar and baz. If we assume we can get pullbacks for those functions, the pullback for foo looks as follows.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"function J(::typeof(foo), x)\n  a, da = J(bar, x)\n  b, db = J(baz, a)\n  return b, function(b̄)\n    ā = db(b̄)\n    x̄ = da(ā)\n    return x̄\n  end\nend","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Thus, where the forward pass calculates x -> a -> b, the backwards takes b̄ -> ā -> x̄ via the pullbacks. The AD transform is recursive; we'll differentiate bar and baz in the same way, until we hit a case where gradient is explicitly defined.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Here's a working example that illustrates the concepts.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"J(::typeof(sin), x) = sin(x), ȳ -> ȳ*cos(x)\nJ(::typeof(cos), x) = cos(x), ȳ -> -ȳ*sin(x)\n\nfoo(x) = sin(cos(x))\n\nfunction J(::typeof(foo), x)\n  a, da = J(sin, x)\n  b, db = J(cos, a)\n  return b, function(b̄)\n    ā = db(b̄)\n    x̄ = da(ā)\n    return x̄\n  end\nend\n\ngradient(f, x) = J(f, x)[2](1)\n\ngradient(foo, 1)","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Now, clearly this is a mechanical transformation, so the only remaining thing is to automate it – a small matter of programming.","category":"page"},{"location":"Zygote/internals/#Closures-1","page":"Internals","title":"Closures","text":"","category":"section"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"The J function here corresponds to forward in Zygote. However, forward actually a wrapper around the lower level _forward function.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"julia> y, back = Zygote._forward(sin, 0.5);\n\njulia> back(1)\n(nothing, 0.8775825618903728)","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Why the extra nothing here? This actually represents the gradient of the function sin. This is often nothing, but when we have closures the function contains data we need gradients for.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"julia> f = let a = 3; x -> x*a; end\n#19 (generic function with 1 method)\n\njulia> y, back = Zygote._forward(f, 2);\n\njulia> back(1)\n((a = 2,), 3)","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"This is a minor point for the most part, but _forward will come up in future examples.","category":"page"},{"location":"Zygote/internals/#Entry-Points-1","page":"Internals","title":"Entry Points","text":"","category":"section"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"We could do this transform with a macro, but don't want to require that all differentiable code is annotated. Instead a generated function gets us much of the power of a macro without this annotation, because we can use it to get lowered code for a function. We can then modify the code as we please and return it to implement J(foo, x).","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"julia> foo(x) = baz(bar(x))\nfoo (generic function with 1 method)\n\njulia> @code_lowered foo(1)\nCodeInfo(\n1 ─ %1 = (Main.bar)(x)\n│   %2 = (Main.baz)(%1)\n└──      return %2","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"We convert the code to SSA form using Julia's built-in IR data structure, after which it looks like this.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"julia> Zygote.@code_ir foo(1)\n1 1 ─ %1 = (Main.bar)(_2)::Any\n  │   %2 = (Main.baz)(%1)::Any\n  └──      return %2    ","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"(There isn't much difference unless there's some control flow.)","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"The code is then differentiated by the code in compiler/reverse.jl. You can see the output with @code_adjoint.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"julia> Zygote.@code_adjoint foo(1)\n1 1 ─ %1  = (Zygote._forward)(_2, Zygote.unwrap, Main.bar)::Any\n  │   %2  = (Base.getindex)(%1, 1)::Any\n  │         (Base.getindex)(%1, 2)::Any\n  │   %4  = (Zygote._forward)(_2, %2, _4)::Any\n  │   %5  = (Base.getindex)(%4, 1)::Any\n  │         (Base.getindex)(%4, 2)::Any\n  │   %7  = (Zygote._forward)(_2, Zygote.unwrap, Main.baz)::Any\n  │   %8  = (Base.getindex)(%7, 1)::Any\n  │         (Base.getindex)(%7, 2)::Any\n  │   %10 = (Zygote._forward)(_2, %8, %5)::Any\n  │   %11 = (Base.getindex)(%10, 1)::Any\n  │         (Base.getindex)(%10, 2)::Any\n  └──       return %11\n  1 ─ %1  = Δ()::Any\n1 │   %2  = (@12)(%1)::Any\n  │   %3  = (Zygote.gradindex)(%2, 1)::Any\n  │   %4  = (Zygote.gradindex)(%2, 2)::Any\n  │         (@9)(%3)::Any\n  │   %6  = (@6)(%4)::Any\n  │   %7  = (Zygote.gradindex)(%6, 1)::Any\n  │   %8  = (Zygote.gradindex)(%6, 2)::Any\n  │         (@3)(%7)::Any\n  │   %10 = (Zygote.tuple)(nothing, %8)::Any\n  └──       return %10\n, [1])","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"This code is quite verbose, mainly due to all the tuple unpacking (gradindex is just like getindex, but handles nothing gracefully). The are two pieces of IR here, one for the modified forward pass and one for the pullback closure. The @ nodes allow the closure to refer to values from the forward pass, and the Δ() represents the incoming gradient ȳ. In essence, this is just what we wrote above by hand for J(::typeof(foo), x).","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"compiler/emit.jl lowers this code into runnable IR (e.g. by turning @ references into getfields and stacks), and it's then turned back into lowered code for Julia to run.","category":"page"},{"location":"Zygote/internals/#Closure-Conversion-1","page":"Internals","title":"Closure Conversion","text":"","category":"section"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"There are no closures in lowered Julia code, so we can't actually emit one directly in lowered code. To work around this we have a trick: we have a generic struct like","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"struct Pullback{F}\n  data\nend","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"We can put whatever we want in data, and the F will be the signature for the original call, like Tuple{typeof(foo),Int}. When the pullback gets called it hits another generated function which emits the pullback code.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"In hand written code this would look like:","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"struct Pullback{F}\n  data\nend\n\nfunction J(::typeof(foo), x)\n  a, da = J(sin, x)\n  b, db = J(cos, a)\n  return b, Pullback{typeof(foo)}((da, db))\nend\n\nfunction(p::Pullback{typeof(foo)})(b̄)\n  da, db = p.data[1], p.data[2]\n  ā = db(b̄)\n  x̄ = da(ā)\n  return x̄\nend","category":"page"},{"location":"Zygote/internals/#Debugging-1","page":"Internals","title":"Debugging","text":"","category":"section"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Say some of our code is throwing an error.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"bad(x) = x\n\nZygote.@adjoint bad(x) = x, _ -> error(\"bad\")\n\nfoo(x) = bad(sin(x))\n\ngradient(foo, 1) # error!","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Zygote can usually give a stacktrace pointing right to the issue here, but in some cases there are compiler crashes that make this harder. In these cases it's best to (a) use _forward and (b) take advantage of Zygote's recursion to narrow down the problem function.","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"julia> y, back = Zygote._forward(foo, 1);\n\njulia> back(1) # just make up a value here, it just needs to look similar to `y`\nERROR: bad\n\n# Ok, so we try functions that foo calls\n\njulia> y, back = Zygote._forward(sin, 1);\n\njulia> back(1)\n(nothing, 0.5403023058681398)\n\n# Looks like that's fine\n\njulia> y, back = Zygote._forward(bad, 1);\n\njulia> back(1) # ok, here's our issue. Lather, rinse, repeat.\nERROR: bad","category":"page"},{"location":"Zygote/internals/#","page":"Internals","title":"Internals","text":"Of course, our goal is that you never have to do this, but until Zygote is more mature it can be a useful way to narrow down test cases.","category":"page"},{"location":"Zygote/glossary/#Glossary-1","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Differentiation is a minefield of conflicting and overlapping terminology, partly because the ideas have been re-discovered in many different fields (e.g. calculus and differential geometry, the traditional AD community, deep learning, finance, etc.) Many of these terms are not well-defined and others may disagree on the details. Nevertheless, we aim to at least say how we use these terms, which will be helpful when reading over Zygote issues, discussions and source code.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"The list is certainly not complete; if you see new terms you'd like defined, or would like to add one yourself, please do open an issue or PR.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Adjoint: See pullback. Used when defining new pullbacks (i.e. the @adjoint macro) since this involves defining the adjoint of the Jacobian, in most cases.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Backpropagation: Essentially equivalent to \"reverse-mode AD\". Used particularly in the machine learning world to refer to simple chains of functions f(g(h(x))), but has generalised beyond that.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Derivative: Given a scalar function y = f(x), the derivative is fracpartial ypartial x. \"Partial\" is taken for granted in AD; there's no interesting distinction between partial and total derivatives for our purposes. It's all in the eye of the beholder.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Differential: Given a function f(x), the linearisation partial f such that f(x + epsilon) approx f(x) + partial f epsilon. This is a generalisation of the derivative since it applies to, for example, vector-to-vector functions (partial f is a Jacobian) and holomorphic complex functions (partial f is the first Wirtinger derivative). This is not, in general, what Zygote calculates, though differentials can usually be derived from gradients.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"IR: Intermediate Representation. Essentially source code, but usually lower level – e.g. control flow constructs like loops and branches have all been replaced by gotos. The idea is that it's harder for humans to read/write but easier to manipulate programmatically. Worth looking at SSA form as a paradigmatic example.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Gradient: See sensitivity. There is no technical difference in Zygote's view, though \"gradient\" sometimes distinguishes the sensitivity we actually want from e.g. the internal ones that Zygote produces as it backpropagates.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Graph: ML people tend to think of models as \"computation graphs\", but this is no more true than any program is a graph. In fact, pretty much anything is a graph if you squint hard enough. This also refers to the data structure that e.g. TensorFlow and PyTorch build to represent your model, but see trace for that.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Pullback: Given y = f(x) the function bar x = back(bar y). In other words, the function back in y, back = Zygote.forward(f, x).","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Sensitivity: Used to refer to the gradient bar x = fracpartial lpartial x with some scalar loss l. In other words, you have a value x (which need not be scalar) at some point in your program, and bar x tells you how you should change that value to decrease the loss. In the AD world, sometimes used to refer to adjoint rules.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Source to Source Differentiation: Or Source Code Transformation (SCT). As opposed to tracing programs to simplify them, an alternative is to operate directly on a language's source code or IR, generating new source code for pullbacks. This describes Zygote, Swift for TensorFlow, Tapenade and a few other old ADs that worked on C source files. Zygote and Swift are unusual in that they work on in-memory IR rather than text source.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"To an extent, tracing ADs can be viewed as source transform of a Wengert list / trace. The key difference is that the trace is a lossy representation of the original semantics, which causes problems with e.g. control flow. Systems which can preserve some of those semantics (e.g. autograph) begin to blur the line here, though they are still not nearly as expressive as language IRs.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Symbolic Differentiation: Used to refer to differentiation of \"mathematical expressions\", that is, things like 3x^2 + sin(x). Often distinguished from AD, though this is somewhat arbitrary; you can happily produce a symbolic adjoint for a Wengert list, the only difference being that you're allowed to make variable bindings. So it's really just a special case of AD on an unusually limited language.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Tape: This term can refer to pretty much any part of an AD implementation. In particular confusion is caused by conflating the trace with the set of values sometimes closed over by a pullback. Autograd has a combined trace/closure data structure which is usually described as the tape. On the other hand, PyTorch described their implementation as tape-free because the trace/closure is stored as a DAG rather than a vector, so basically all bets are off here.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Trace: A recording of each mathematical operation used by a program, made at runtime and usually forming a Wengert list. Traces may or may not also record actual runtime values (e.g. PyTorch vs. TensorFlow). They can often be treated as an IR and compiled, but are distinguished from true IRs in that they unroll and inline all control flow, functions and data structures. The tracing process can be thought of as a kind of partial evaluation, though tracers are typically much less worried about losing information.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"vector-Jacobian product: see pullback. So called because all pullbacks are linear functions that can be represented by (left) multiplication with the Jacobian matrix.","category":"page"},{"location":"Zygote/glossary/#","page":"Glossary","title":"Glossary","text":"Wengert List: A set of simple variable assignments and mathematical expressions, forming a directed graph. Can be thought of as a limited programming language with variable bindings and numerical functions but no control flow or data structures. If you trace a program for AD it will typically take this form.","category":"page"}]
}
