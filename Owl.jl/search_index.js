var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Home",
    "title": "Home",
    "category": "page",
    "text": ""
},

{
    "location": "#-1",
    "page": "Home",
    "title": "ðŸ¦‰",
    "category": "section",
    "text": "ë°¥ ë¨¹ê³  ë˜¥ ì‹¸ëŠ” ê³³ìž…ë‹ˆë‹¤ "
},

{
    "location": "Flux/#",
    "page": "Flux í™ˆ",
    "title": "Flux í™ˆ",
    "category": "page",
    "text": "https://github.com/FluxML/Flux.jl ìžë£Œë¥¼ ë²ˆì—­í•˜ëŠ” ê³³ìž…ë‹ˆë‹¹"
},

{
    "location": "Flux/#Flux:-ì¤„ë¦¬ì•„-ë¨¸ì‹ -ëŸ¬ë‹-ë¼ì´ë¸ŒëŸ¬ë¦¬-1",
    "page": "Flux í™ˆ",
    "title": "Flux: ì¤„ë¦¬ì•„ ë¨¸ì‹  ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬",
    "category": "section",
    "text": "FluxëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬. \"ë°°í„°ë¦¬-í¬í•¨(batteries-included, ì œí’ˆì˜ ì™„ì „í•œ ìœ ìš©ì„±ì„ ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ë¶€í’ˆì„ í•¨ê»˜ ì œê³µí•œë‹¤ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ìª½ ìš©ì–´)\" ë§Žì€ ìœ ìš©í•œ ë„êµ¬ë¥¼ ì œê³µ. ì¤„ë¦¬ì•„ ì–¸ì–´ë¥¼ í’€íŒŒì›Œ(full power)ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŒ. ì „ì²´ ìŠ¤íƒì„ ì¤„ë¦¬ì•„ ì½”ë“œë¡œ êµ¬í˜„í•¨. GPU ì»¤ë„ë„ ê°€ëŠ¥í•˜ê³ , ê°œë³„ íŒŒíŠ¸ë¥¼ ê°œì¸ ì·¨í–¥ì— ë§žê²Œ ì¡°ìž‘í•  ìˆ˜ ìžˆìŒ."
},

{
    "location": "Flux/#ì„¤ì¹˜-1",
    "page": "Flux í™ˆ",
    "title": "ì„¤ì¹˜",
    "category": "section",
    "text": "ì¤„ë¦¬ì•„ 0.6.0 ì´ìƒ, ì•„ì§ ì•ˆê¹”ì•˜ìœ¼ë©´ ì„¤ì¹˜.Pkg.add(\"Flux\")\n# ì„ íƒì¸ë° ì¶”ì²œ\nPkg.update() # íŒ¨í‚¤ì§€ë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ëŽƒ\nPkg.test(\"Flux\") # ì„¤ì¹˜ ë˜‘ë°”ë¡œ ëœê±´ê°€ í™•ì¸í•´ ë´„ê¸°ë³¸ì ì¸ ê²ƒ ë¶€í„° ì‹œìž‘í•˜ìž. ë™ë¬¼ì› ëª¨ë¸(model zoo)ì€ ì—¬ëŸ¬ê°€ì§€ ê³µí†µ ëª¨ë¸ì„ ë‹¤ë£¨ëŠ”ë° ê·¸ê±¸ë¡œ ì‹œìž‘í•´ë„ ì¢‹ìŒ."
},

{
    "location": "Flux/models/basics/#",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/basics/#ëª¨ë¸-ë§Œë“¤ê¸°-ê¸°ì´ˆ-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ëª¨ë¸ ë§Œë“¤ê¸° ê¸°ì´ˆ",
    "category": "section",
    "text": ""
},

{
    "location": "Flux/models/basics/#ê¸°ìš¸ê¸°(Gradients,-ê²½ì‚¬)-êµ¬í•˜ê¸°-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ê¸°ìš¸ê¸°(Gradients, ê²½ì‚¬) êµ¬í•˜ê¸°",
    "category": "section",
    "text": "ê°„ë‹¨í•œ linear regression(ë¦¬ë‹ˆì–´ ë¦¬ê·¸ë ˆì…˜, ì§ì„  ëª¨ì–‘ìœ¼ë¡œ ê·¸ë ¤ì§€ëŠ” í•¨ìˆ˜)ì„ ìƒê°í•´ ë³´ìž. ì´ê²ƒì€ ìž…ë ¥ xì— ëŒ€í•´ ì¶œë ¥ ë°°ì—´ yê°€ ì–´ë–¤ ê°’ì´ ë‚˜ì˜¬ì§€ ì˜ˆì¸¡í•˜ëŠ” ê±°ìž„. (ì¤„ë¦¬ì•„ REPLì—ì„œ ì˜ˆì œë¥¼ ë”°ë¼í•´ë³´ë©´ ì¢‹ìŒ)julia> W = rand(2, 5)\n2Ã—5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = rand(2)\n2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> predict(x) = W*x .+ b\npredict (generic function with 1 method)\n\njulia> loss(x, y) = sum((predict(x) .- y).^2)\nloss (generic function with 1 method)\n\njulia> x, y = rand(5), rand(2) # ë”ë¯¸ ë°ì´í„°\n([0.496864, 0.947507, 0.874288, 0.251528, 0.192234], [0.901991, 0.0802404])\n\njulia> loss(x, y) # ~ 3\n3.1660692660286722ì˜ˆì¸¡ì„ ë” ìž˜í•˜ê¸° ìœ„í•´ Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ìž. loss function(ì†ì‹¤, ì˜ˆì¸¡ ì‹¤íŒ¨ìœ¨ í•¨ìˆ˜)ê³¼ gradient descent(ê²½ì‚¬ í•˜ê°•, ë‚´ë¦¬ë§‰ ê¸°ìš¸ê¸°)ë¥¼ í•´ë³´ë©´ì„œ. ì§ì ‘ ì†ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìžˆì§€ë§Œ Fluxì—ì„œëŠ” Wì™€ bë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„°(parameters)ë¡œ ë‘˜ ìˆ˜ ìžˆìŒ.julia> using Flux.Tracker\n\njulia> W = param(W)\nTracked 2Ã—5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = param(b)\nTracked 2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> l = loss(x, y)\n3.1660692660286722 (tracked)\n\njulia> back!(l)\nloss(x, y)ëŠ” ë°©ê¸ˆ ì „ê³¼ ê°™ì€ ìˆ˜(3.1660692660286722)ë¥¼ ë¦¬í„´, ê·¸ëŸ°ë° ì´ì œë¶€í„°ëŠ” ê¸°ìš¸ì–´ì§€ëŠ” ëª¨ì–‘ì„ ê´€ì°° ê¸°ë¡í•˜ì—¬ ê°’ì„ ì¶”ì (tracked) í•¨. back!ì„ í˜¸ì¶œí•˜ë©´ Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´. ê¸°ìš¸ê¸°ê°€ ë­”ì§€ ì•Œì•„ëƒˆìœ¼ë‹ˆ Wë¥¼ ê³ ì³ê°€ë©´ì„œ ëª¨ë¸ì„ í›ˆë ¨í•˜ìž.julia> W.grad\n2Ã—5 Array{Float64,2}:\n 0.949491  1.81066  1.67074  0.480662  0.367352\n 1.49163   2.84449  2.62468  0.755107  0.577101\n\njulia> # íŒŒë¼ë¯¸í„° ì—…ëŽƒ\n       W.data .-= 0.1(W.grad)\n2Ã—5 Array{Float64,2}:\n  0.762798   0.110647   0.0127989  0.890913  0.473484\n -0.0639541  0.693267  -0.0163046  0.385161  0.714602\n\njulia> loss(x, y) # ~ 2.5\n1.1327711929294395 (tracked)ì˜ˆì¸¡ ì‹¤íŒ¨(loss)ê°€ ì¡°ê¸ˆ ì¤„ì–´ë“¤ì—ˆìŒ, x ì˜ˆì¸¡ì´ ëª©í‘œ íƒ€ê²Ÿ yì— ì¢€ ë” ê°€ê¹Œì›Œì¡Œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨. ë°ì´í„°ê°€ ìžˆìœ¼ë©´ ëª¨ë¸ í›ˆë ¨í•˜ê¸°ë„ ì‹œë„í•  ìˆ˜ ìžˆìŒ.ë³µìž¡í•œ ë”¥ëŸ¬ë‹ì´ Fluxì—ì„œëŠ” ì´ì™€ ê°™ì€ ì˜ˆì œì²˜ëŸ¼ ë‹¨ìˆœí•´ ì§. ë¬¼ë¡  ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°¯ìˆ˜ê°€ ë°±ë§Œê°œê°€ ë„˜ì–´ê°€ê³  ë³µìž¡í•œ ì œì–´ íë¦„ì„ ê°–ê²Œ ë˜ë©´ ë‹¤ë¥¸ ëª¨ì–‘ì„ ê°–ê² ì§€. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë³µìž¡ì„±ì„ ë‹¤ë£¨ëŠ” ë²•ì´ ìžˆìŒ. ê·¸ëŸ° ê²ƒì´ ë­ê°€ ìžˆëŠ”ì§€ í•œë²ˆ ì‚´íŽ´ë³´ê² ìŒ."
},

{
    "location": "Flux/models/basics/#ë ˆì´ì–´-ë§Œë“¤ê¸°-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ë ˆì´ì–´ ë§Œë“¤ê¸°",
    "category": "section",
    "text": "ì´ì œë¶€í„°ëŠ” linear regression ë³´ë‹¤ ë³µìž¡í•œ ëª¨ë¸ì„ ë§Œë“¦. ì˜ˆë¥¼ ë“¤ì–´, ë‘ ê°œì˜ linear ë ˆì´ì–´ ì‚¬ì´ì— ì‹œê·¸ëª¨ì´ë“œ (Ïƒ) ì²˜ëŸ¼ nonlinearity(ë¹„ì„ í˜•, ì»¤ë¸Œì²˜ëŸ¼ ì§ì„ ì´ ì•„ë‹Œ ê±°)ë¥¼ ê°–ëŠ” ë„˜ì´ ìžˆì„ë•Œ. ìœ„ì˜ ìŠ¤íƒ€ì¼ì€ ì•„ëž˜ì™€ ê°™ì´ ì“¸ ìˆ˜ ìžˆìŒ:julia> using Flux\n\njulia> W1 = param(rand(3, 5))\nTracked 3Ã—5 Array{Float64,2}:\n 0.540422  0.680087  0.743124  0.0216563  0.377793\n 0.416939  0.51823   0.464998  0.419852   0.446143\n 0.260294  0.392582  0.46784   0.549495   0.373124\n\njulia> b1 = param(rand(3))\nTracked 3-element Array{Float64,1}:\n 0.213799\n 0.373862\n 0.243417\n\njulia> layer1(x) = W1 * x .+ b1\nlayer1 (generic function with 1 method)\n\njulia> W2 = param(rand(2, 3))\nTracked 2Ã—3 Array{Float64,2}:\n 0.789744  0.389376  0.172613\n 0.472963  0.21518   0.220236\n\njulia> b2 = param(rand(2))\nTracked 2-element Array{Float64,1}:\n 0.121207\n 0.502486\n\njulia> layer2(x) = W2 * x .+ b2\nlayer2 (generic function with 1 method)\n\njulia> model(x) = layer2(Ïƒ.(layer1(x)))\nmodel (generic function with 1 method)\n\njulia> model(rand(5)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 1.06727\n 1.13835ìž‘ë™ì€ í•˜ëŠ”ë° ì¤‘ë³µ ìž‘ì—…ì´ ë§Žì•„ ë³´ê¸°ì— ì¢‹ì§€ ì•Šë‹¤ - íŠ¹ížˆ ë ˆì´ì–´ë¥¼ ë” ì¶”ê°€í•œë‹¤ë©´. linear ë ˆì´ì–´ë¥¼ ëŒë ¤ì£¼ëŠ” í•¨ìˆ˜ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì´ê²ƒë“¤ì„ ì •ë¦¬í•¨.julia> function linear(in, out)\n         W = param(randn(out, in))\n         b = param(randn(out))\n         x -> W * x .+ b\n       end\nlinear (generic function with 1 method)\n\njulia> linear1 = linear(5, 3) # linear1.W í•  ìˆ˜ ìžˆìŒ (x -> W * x .+ b ìµëª…í•¨ìˆ˜ ë¦¬í„´)\n(::#3) (generic function with 1 method)\n\njulia> linear1.W\nTracked 3Ã—5 Array{Float64,2}:\n -1.72011   -1.07297   0.396755  -0.117604   0.25952\n -0.16694    0.99327  -0.589717  -1.87123    0.141679\n -0.972281  -1.84836   2.55071   -0.136674  -0.147826\n\njulia> linear2 = linear(3, 2)\n(::#3) (generic function with 1 method)\n\njulia> model(x) = linear2(Ïƒ.(linear1(x)))\nmodel (generic function with 1 method)\n\njulia> model(x) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 2.75582\n 0.416809ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” structë¡œ íƒ€ìž…ì„ ë§Œë“¤ì–´ì„œ affine(ì–´íŒŒì¸) ë ˆì´ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ìžˆìŒ.julia> struct Affine\n         W\n         b\n       end\n\njulia> Affine(in::Integer, out::Integer) =\n         Affine(param(randn(out, in)), param(randn(out)))\nAffine\n\njulia> # ì˜¤ë²„ë¡œë“œ í•˜ë©´ ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•  ìˆ˜ ìžˆìŒ\n       (m::Affine)(x) = m.W * x .+ m.b\n\njulia> a = Affine(10, 5)\nAffine(param([0.0252182 -1.99122 â€¦ -0.191235 0.294728; 1.13559 1.50226 â€¦ -2.43917 0.56976; â€¦ ; -0.735177 0.202646 â€¦ -0.301945 -0.183598; 1.05967 0.986786 â€¦ -1.57835 -0.0893871]), param([-0.39419, -1.26818, 0.757665, 0.941398, -0.783242]))\n\njulia> a(rand(10)) # => 5-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 5-element Array{Float64,1}:\n -0.945544\n -0.575674\n  2.93741\n  0.111253\n -0.843172ì¶•í•˜ë“œë ¤ì—¼! Fluxì—ì„œ ë‚˜ì˜¤ëŠ” Dense ë ˆì´ì–´ ë§Œë“¤ê¸° ì„±ê³µí•œ ê±°ìž„. FluxëŠ” ë§Žì€ ìž¬ë°ŒëŠ” ë ˆì´ì–´ë“¤ì´ ìžˆëŠ”ë°, ê·¸ê²ƒë“¤ì„ ì§ì ‘ ë§Œë“œëŠ” ê²ƒ ì—­ì‹œ ì•„ì£¼ ì‰¬ì›€.(Denseì™€ ë‹¤ë¥¸ í•œê°€ì§€ - íŽ¸ì˜ë¥¼ ìœ„í•´ activation(í™œì„±) í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ëŠ” ê±°ë„ ë¨. Dense(10, 5, Ïƒ) ìš”ëŸ°ì‹ìœ¼ë¡œ.)"
},

{
    "location": "Flux/models/basics/#ì´ì˜ê²Œ-ìŒ“ì•„ë³´ìž-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ì´ì˜ê²Œ ìŒ“ì•„ë³´ìž",
    "category": "section",
    "text": "ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì€ í”í•œ ì¼ìž„: (layer1 ì´ë¦„ì´ ê²¹ì¹˜ë‹ˆ REPL ìƒˆë¡œ ë„ìš°ìž)julia> using Flux\n\njulia> layer1 = Dense(10, 5, Ïƒ)\nDense(10, 5, NNlib.Ïƒ)\n\njulia> # ...\n       model(x) = layer3(layer2(layer1(x)))\nmodel (generic function with 1 method)ê¸°ë‹¤ëž—ê²Œ ì—°ê²°(chains) í• ë¼ë¯„, ë‹¤ìŒê³¼ ê°™ì´ ë ˆì´ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ”ê²Œ ì¢€ ë” ì§ê´€ì ìž„:julia> layers = [Dense(10, 5, Ïƒ), Dense(5, 2), softmax]\n3-element Array{Any,1}:\n Dense(10, 5, NNlib.Ïƒ)\n Dense(5, 2)\n NNlib.softmax\n\njulia> model(x) = foldl((x, m) -> m(x), x, layers)\nmodel (generic function with 1 method)\n\njulia> model(rand(10)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 0.593021\n 0.406979íŽ¸ë¦¬í•˜ê²Œ ì“°ë¼ê³  ì´ê²ƒ ì—­ì‹œ Fluxì—ì„œ ì œê³µí•¨:julia> model2 = Chain(\n         Dense(10, 5, Ïƒ),\n         Dense(5, 2),\n         softmax)\nChain(Dense(10, 5, NNlib.Ïƒ), Dense(5, 2), NNlib.softmax)\n\njulia> model2(rand(10)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 0.172085\n 0.827915ê³ ì˜¤ê¸‰ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°™ì•„ ë³´ì´ëŠ” êµ°; ì–´ëŠë§Œí¼ ê°„ë‹¨í•˜ê²Œ ì¶”ìƒí™” í•˜ëŠ”ì§€ ë´¤ì„ ê±°ìž„. ì¤„ë¦¬ì•„ ì½”ë“œì˜ ê°•ë ¥í•¨ì„ ìžƒì§€ ì•Šì•˜ìŒ.ì´ëŸ° ì ‘ê·¼ë²•ì˜ ì¢‹ì€ ì ì€ \"ëª¨ë¸\"ì€ ê·¸ëƒ¥ í•¨ìˆ˜ë¼ëŠ”ê±° (ì•„ë§ˆë„ í›ˆë ¨ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜), ê·¸ë§ˆì €ë„ í•¨ìˆ˜ í•©ì„±(âˆ˜)ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ í•  ìˆ˜ ìžˆìŒ.julia> m = Dense(5, 2) âˆ˜ Dense(10, 5, Ïƒ)\n(::#55) (generic function with 1 method)\n\njulia> m(rand(10))\nTracked 2-element Array{Float64,1}:\n -1.28749\n -0.202492ë§ˆì°¬ê°€ì§€ë¡œ, Chainì€ ì¤„ë¦¬ì•„ í•¨ìˆ˜ì™€ ì´ì˜ê²Œ ë™ìž‘í•¨.julia> m = Chain(x -> x^2, x -> x+1)\nChain(#3, #4)\n\njulia> m(5) # => 26\n26"
},

{
    "location": "Flux/models/basics/#ë ˆì´ì–´-ë„ìš°ë¯¸ë“¤-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ë ˆì´ì–´ ë„ìš°ë¯¸ë“¤",
    "category": "section",
    "text": "FluxëŠ” ì‚¬ìš©ìžì˜ ì»¤ìŠ¤í…€ ë ˆì´ì–´ë¥¼ ë„ì™€ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì œê³µí•¨, ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œí•˜ë©´julia> Flux.treelike(Affine)\nadapt (generic function with 1 method)Affine ë ˆì´ì–´ì— ë¶€ê°€ì ì¸ ìœ ìš©í•œ ê¸°ëŠ¥ì´ ì¶”ê°€ë¨, íŒŒë¼ë¯¸í„° ëª¨ìœ¼ê¸°(collecting)ë‚˜ GPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ê°™ì€ ê±°."
},

{
    "location": "Flux/models/recurrence/#",
    "page": "Recurrence",
    "title": "Recurrence",
    "category": "page",
    "text": "ë˜¥ì‹¸ê³  ë‚˜ì¤‘ì— ë²ˆì—­í•¨"
},

{
    "location": "Flux/models/recurrence/#Recurrent-Models-1",
    "page": "Recurrence",
    "title": "Recurrent Models",
    "category": "section",
    "text": ""
},

{
    "location": "Flux/models/recurrence/#Recurrent-Cells-1",
    "page": "Recurrence",
    "title": "Recurrent Cells",
    "category": "section",
    "text": "In the simple feedforward case, our model m is a simple function from various inputs xáµ¢ to predictions yáµ¢. (For example, each x might be an MNIST digit and each y a digit label.) Each prediction is completely independent of any others, and using the same x will always produce the same y.yâ‚ = f(xâ‚)\nyâ‚‚ = f(xâ‚‚)\nyâ‚ƒ = f(xâ‚ƒ)\n# ...Recurrent networks introduce a hidden state that gets carried over each time we run the model. The model now takes the old h as an input, and produces a new h as output, each time we run it.h = # ... initial state ...\nh, yâ‚ = f(h, xâ‚)\nh, yâ‚‚ = f(h, xâ‚‚)\nh, yâ‚ƒ = f(h, xâ‚ƒ)\n# ...Information stored in h is preserved for the next prediction, allowing it to function as a kind of memory. This also means that the prediction made for a given x depends on all the inputs previously fed into the model.(This might be important if, for example, each x represents one word of a sentence; the model's interpretation of the word \"bank\" should change if the previous input was \"river\" rather than \"investment\".)Flux's RNN support closely follows this mathematical perspective. The most basic RNN is as close as possible to a standard Dense layer, and the output is also the hidden state.Wxh = randn(5, 10)\nWhh = randn(5, 5)\nb   = randn(5)\n\nfunction rnn(h, x)\n  h = tanh.(Wxh * x .+ Whh * h .+ b)\n  return h, h\nend\n\nx = rand(10) # dummy data\nh = rand(5)  # initial hidden state\n\nh, y = rnn(h, x)If you run the last line a few times, you'll notice the output y changing slightly even though the input x is the same.We sometimes refer to functions like rnn above, which explicitly manage state, as recurrent cells. There are various recurrent cells available, which are documented in the layer reference. The hand-written example above can be replaced with:using Flux\n\nrnn2 = Flux.RNNCell(10, 5)\n\nx = rand(10) # dummy data\nh = rand(5)  # initial hidden state\n\nh, y = rnn2(h, x)"
},

{
    "location": "Flux/models/recurrence/#Stateful-Models-1",
    "page": "Recurrence",
    "title": "Stateful Models",
    "category": "section",
    "text": "For the most part, we don't want to manage hidden states ourselves, but to treat our models as being stateful. Flux provides the Recur wrapper to do this.x = rand(10)\nh = rand(5)\n\nm = Flux.Recur(rnn, h)\n\ny = m(x)The Recur wrapper stores the state between runs in the m.state field.If you use the RNN(10, 5) constructor â€“ as opposed to RNNCell â€“ you'll see that it's simply a wrapped cell.julia> RNN(10, 5)\nRecur(RNNCell(Dense(15, 5)))"
},

{
    "location": "Flux/models/recurrence/#Sequences-1",
    "page": "Recurrence",
    "title": "Sequences",
    "category": "section",
    "text": "Often we want to work with sequences of inputs, rather than individual xs.seq = [rand(10) for i = 1:10]With Recur, applying our model to each element of a sequence is trivial:m.(seq) # returns a list of 5-element vectorsThis works even when we've chain recurrent layers into a larger model.m = Chain(LSTM(10, 15), Dense(15, 5))\nm.(seq)"
},

{
    "location": "Flux/models/recurrence/#Truncating-Gradients-1",
    "page": "Recurrence",
    "title": "Truncating Gradients",
    "category": "section",
    "text": "By default, calculating the gradients in a recurrent layer involves the entire history. For example, if we call the model on 100 inputs, calling back! will calculate the gradient for those 100 calls. If we then calculate another 10 inputs we have to calculate 110 gradients â€“ this accumulates and quickly becomes expensive.To avoid this we can truncate the gradient calculation, forgetting the history.truncate!(m)Calling truncate! wipes the slate clean, so we can call the model with more inputs without building up an expensive gradient computation.truncate! makes sense when you are working with multiple chunks of a large sequence, but we may also want to work with a set of independent sequences. In this case the hidden state should be completely reset to its original value, throwing away any accumulated information. reset! does this for you."
},

{
    "location": "Flux/models/regularisation/#",
    "page": "Regularisation",
    "title": "Regularisation",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/regularisation/#Regularisation-1",
    "page": "Regularisation",
    "title": "Regularisation",
    "category": "section",
    "text": "Applying regularisation to model parameters is straightforward. We just need to apply an appropriate regulariser, such as norm, to each model parameter and add the result to the overall loss.For example, say we have a simple regression.m = Dense(10, 5)\nloss(x, y) = crossentropy(softmax(m(x)), y)We can regularise this by taking the (L2) norm of the parameters, m.W and m.b.penalty() = norm(m.W) + norm(m.b)\nloss(x, y) = crossentropy(softmax(m(x)), y) + penalty()When working with layers, Flux provides the params function to grab all parameters at once. We can easily penalise everything with sum(norm, params).julia> params(m)\n2-element Array{Any,1}:\n param([0.355408 0.533092; â€¦ 0.430459 0.171498])\n param([0.0, 0.0, 0.0, 0.0, 0.0])\n\njulia> sum(norm, params(m))\n26.01749952921026 (tracked)Here's a larger example with a multi-layer perceptron.m = Chain(\n  Dense(28^2, 128, relu),\n  Dense(128, 32, relu),\n  Dense(32, 10), softmax)\n\nloss(x, y) = crossentropy(m(x), y) + sum(norm, params(m))\n\nloss(rand(28^2), rand(10))"
},

{
    "location": "Flux/models/layers/#",
    "page": "Model Reference",
    "title": "Model Reference",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/layers/#Basic-Layers-1",
    "page": "Model Reference",
    "title": "Basic Layers",
    "category": "section",
    "text": "These core layers form the foundation of almost all neural networks.Chain\nDense\nConv2D"
},

{
    "location": "Flux/models/layers/#Recurrent-Layers-1",
    "page": "Model Reference",
    "title": "Recurrent Layers",
    "category": "section",
    "text": "Much like the core layers above, but can be used to process sequence data (as well as other kinds of structured data).RNN\nLSTM\nFlux.Recur"
},

{
    "location": "Flux/models/layers/#Activation-Functions-1",
    "page": "Model Reference",
    "title": "Activation Functions",
    "category": "section",
    "text": "Non-linearities that go between layers of your model. Most of these functions are defined in NNlib but are available by default in Flux.Note that, unless otherwise stated, activation functions operate on scalars. To apply them to an array you can call Ïƒ.(xs), relu.(xs) and so on.Ïƒ\nrelu\nleakyrelu\nelu\nswish"
},

{
    "location": "Flux/models/layers/#Normalisation-and-Regularisation-1",
    "page": "Model Reference",
    "title": "Normalisation & Regularisation",
    "category": "section",
    "text": "These layers don't affect the structure of the network but may improve training times or reduce overfitting.Flux.testmode!\nBatchNorm\nDropout\nLayerNorm"
},

{
    "location": "Flux/training/optimisers/#",
    "page": "Optimisers",
    "title": "Optimisers",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/training/optimisers/#Optimisers-1",
    "page": "Optimisers",
    "title": "Optimisers",
    "category": "section",
    "text": "Consider a simple linear regression. We create some dummy data, calculate a loss, and backpropagate to calculate gradients for the parameters W and b.W = param(rand(2, 5))\nb = param(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = rand(5), rand(2) # Dummy data\nl = loss(x, y) # ~ 3\nback!(l)We want to update each parameter, using the gradient, in order to improve (reduce) the loss. Here's one way to do that:function update()\n  Î· = 0.1 # Learning Rate\n  for p in (W, b)\n    p.data .-= Î· .* p.grad # Apply the update\n    p.grad .= 0            # Clear the gradient\n  end\nendIf we call update, the parameters W and b will change and our loss should go down.There are two pieces here: one is that we need a list of trainable parameters for the model ([W, b] in this case), and the other is the update step. In this case the update is simply gradient descent (x .-= Î· .* Î”), but we might choose to do something more advanced, like adding momentum.In this case, getting the variables is trivial, but you can imagine it'd be more of a pain with some complex stack of layers.m = Chain(\n  Dense(10, 5, Ïƒ),\n  Dense(5, 2), softmax)Instead of having to write [m[1].W, m[1].b, ...], Flux provides a params function params(m) that returns a list of all parameters in the model for you.For the update step, there's nothing whatsoever wrong with writing the loop above â€“ it'll work just fine â€“ but Flux provides various optimisers that make it more convenient.opt = SGD([W, b], 0.1) # Gradient descent with learning rate 0.1\n\nopt() # Carry out the update, modifying `W` and `b`.An optimiser takes a parameter list and returns a function that does the same thing as update above. We can pass either opt or update to our training loop, which will then run the optimiser after every mini-batch of data."
},

{
    "location": "Flux/training/optimisers/#Optimiser-Reference-1",
    "page": "Optimisers",
    "title": "Optimiser Reference",
    "category": "section",
    "text": "All optimisers return a function that, when called, will update the parameters passed to it.SGD\nMomentum\nNesterov\nADAM"
},

{
    "location": "Flux/training/training/#",
    "page": "Training",
    "title": "Training",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/training/training/#Training-1",
    "page": "Training",
    "title": "Training",
    "category": "section",
    "text": "To actually train a model we need three things:A objective function, that evaluates how well a model is doing given some input data.\nA collection of data points that will be provided to the objective function.\nAn optimiser that will update the model parameters appropriately.With these we can call Flux.train!:Flux.train!(objective, data, opt)There are plenty of examples in the model zoo."
},

{
    "location": "Flux/training/training/#Loss-Functions-1",
    "page": "Training",
    "title": "Loss Functions",
    "category": "section",
    "text": "The objective function must return a number representing how far the model is from its target â€“ the loss of the model. The loss function that we defined in basics will work as an objective. We can also define an objective in terms of some model:m = Chain(\n  Dense(784, 32, Ïƒ),\n  Dense(32, 10), softmax)\n\nloss(x, y) = Flux.mse(m(x), y)\n\n# later\nFlux.train!(loss, data, opt)The objective will almost always be defined in terms of some cost function that measures the distance of the prediction m(x) from the target y. Flux has several of these built in, like mse for mean squared error or crossentropy for cross entropy loss, but you can calculate it however you want."
},

{
    "location": "Flux/training/training/#Datasets-1",
    "page": "Training",
    "title": "Datasets",
    "category": "section",
    "text": "The data argument provides a collection of data to train with (usually a set of inputs x and target outputs y). For example, here's a dummy data set with only one data point:x = rand(784)\ny = rand(10)\ndata = [(x, y)]Flux.train! will call loss(x, y), calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:data = [(x, y), (x, y), (x, y)]\n# Or equivalently\ndata = Iterators.repeated((x, y), 3)It's common to load the xs and ys separately. In this case you can use zip:xs = [rand(784), rand(784), rand(784)]\nys = [rand( 10), rand( 10), rand( 10)]\ndata = zip(xs, ys)Note that, by default, train! only loops over the data once (a single \"epoch\"). A convenient way to run multiple epochs from the REPL is provided by @epochs.julia> using Flux: @epochs\n\njulia> @epochs 2 println(\"hello\")\nINFO: Epoch 1\nhello\nINFO: Epoch 2\nhello\n\njulia> @epochs 2 Flux.train!(...)\n# Train for two epochs"
},

{
    "location": "Flux/training/training/#Callbacks-1",
    "page": "Training",
    "title": "Callbacks",
    "category": "section",
    "text": "train! takes an additional argument, cb, that's used for callbacks so that you can observe the training process. For example:train!(objective, data, opt, cb = () -> println(\"training\"))Callbacks are called for every batch of training data. You can slow this down using Flux.throttle(f, timeout) which prevents f from being called more than once every timeout seconds.A more typical callback might look like this:test_x, test_y = # ... create single batch of test data ...\nevalcb() = @show(loss(test_x, test_y))\n\nFlux.train!(objective, data, opt,\n            cb = throttle(evalcb, 5))"
},

{
    "location": "Flux/data/onehot/#",
    "page": "One-Hot Encoding",
    "title": "One-Hot Encoding",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/data/onehot/#One-Hot-Encoding-1",
    "page": "One-Hot Encoding",
    "title": "One-Hot Encoding",
    "category": "section",
    "text": "It's common to encode categorical variables (like true, false or cat, dog) in \"one-of-k\" or \"one-hot\" form. Flux provides the onehot function to make this easy.julia> using Flux: onehot\n\njulia> onehot(:b, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n  true\n false\n\njulia> onehot(:c, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n false\n  trueThe inverse is argmax (which can take a general probability distribution, as well as just booleans).julia> argmax(ans, [:a, :b, :c])\n:c\n\njulia> argmax([true, false, false], [:a, :b, :c])\n:a\n\njulia> argmax([0.3, 0.2, 0.5], [:a, :b, :c])\n:c"
},

{
    "location": "Flux/data/onehot/#Batches-1",
    "page": "One-Hot Encoding",
    "title": "Batches",
    "category": "section",
    "text": "onehotbatch creates a batch (matrix) of one-hot vectors, and argmax treats matrices as batches.julia> using Flux: onehotbatch\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3Ã—3 Flux.OneHotMatrix:\n false   true  false\n  true  false   true\n false  false  false\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Array{Symbol,1}:\n  :b\n  :a\n  :bNote that these operations returned OneHotVector and OneHotMatrix rather than Arrays. OneHotVectors behave like normal vectors but avoid any unnecessary cost compared to using an integer index directly. For example, multiplying a matrix with a one-hot vector simply slices out the relevant row of the matrix under the hood."
},

{
    "location": "Flux/gpu/#",
    "page": "GPU Support",
    "title": "GPU Support",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/gpu/#GPU-Support-1",
    "page": "GPU Support",
    "title": "GPU Support",
    "category": "section",
    "text": "Support for array operations on other hardware backends, like GPUs, is provided by external packages like CuArrays and CLArrays. Flux doesn't care what array type you use, so we can just plug these in without any other changes.For example, we can use CuArrays (with the cu converter) to run our basic example on an NVIDIA GPU.using CuArrays\n\nW = cu(rand(2, 5)) # a 2Ã—5 CuArray\nb = cu(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = cu(rand(5)), cu(rand(2)) # Dummy data\nloss(x, y) # ~ 3Note that we convert both the parameters (W, b) and the data set (x, y) to cuda arrays. Taking derivatives and training works exactly as before.If you define a structured model, like a Dense layer or Chain, you just need to convert the internal parameters. Flux provides mapleaves, which allows you to alter all parameters of a model at once.d = Dense(10, 5, Ïƒ)\nd = mapleaves(cu, d)\nd.W # Tracked CuArray\nd(cu(rand(10))) # CuArray output\n\nm = Chain(Dense(10, 5, Ïƒ), Dense(5, 2), softmax)\nm = mapleaves(cu, m)\nd(cu(rand(10)))The mnist example contains the code needed to run the model on the GPU; just uncomment the lines after using CuArrays."
},

{
    "location": "Flux/community/#",
    "page": "Community",
    "title": "Community",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/community/#Community-1",
    "page": "Community",
    "title": "Community",
    "category": "section",
    "text": "All Flux users are welcome to join our community on the Julia forum, the slack (channel #machine-learning), or Flux's Gitter. If you have questions or issues we'll try to help you out.If you're interested in hacking on Flux, the source code is open and easy to understand â€“ it's all just the same Julia code you work with normally. You might be interested in our intro issues to get started."
},

]}
