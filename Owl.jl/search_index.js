var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Home",
    "title": "Home",
    "category": "page",
    "text": ""
},

{
    "location": "#-1",
    "page": "Home",
    "title": "ğŸ¦‰",
    "category": "section",
    "text": "ì´ˆë³´ë˜¥ì˜ ë¨¸ì‹„ëŸ¬ë‹ë²ˆì—­ì— ëŒ€í•œ í”¼ë“œë°±ì€ https://github.com/wookay/Owl.jl ì´ìŠˆì— ë‚¨ê²¨ ë‹¬ë¼"
},

{
    "location": "Flux/#",
    "page": "Flux í™ˆ",
    "title": "Flux í™ˆ",
    "category": "page",
    "text": "ğŸ¦‰ https://github.com/FluxML/Flux.jl ìë£Œë¥¼ ë²ˆì—­í•˜ëŠ” ê³³ì…ë‹ˆë‹¹"
},

{
    "location": "Flux/#Flux:-ì¤„ë¦¬ì•„-ë¨¸ì‹„ëŸ¬ë‹-ë¼ì´ë¸ŒëŸ¬ë¦¬-1",
    "page": "Flux í™ˆ",
    "title": "Flux: ì¤„ë¦¬ì•„ ë¨¸ì‹„ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬",
    "category": "section",
    "text": "FluxëŠ” ë¨¸ì‹„ëŸ¬ë‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. \"ë°°í„°ë¦¬-í¬í•¨(batteries-included, ì œí’ˆì˜ ì™„ì „í•œ ìœ ìš©ì„±ì„ ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ë¶€í’ˆì„ í•¨ê»˜ ì œê³µí•œë‹¤ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ìª½ ìš©ì–´)\" ë§ì€ ìœ ìš©í•œ ë„êµ¬ë¥¼ ì œê³µí•œë‹¤. ì¤„ë¦¬ì•„ ì–¸ì–´ë¥¼ í’€íŒŒì›Œ(full power)ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì „ì²´ ìŠ¤íƒì„ ì¤„ë¦¬ì•„ ì½”ë“œë¡œ êµ¬í˜„í•œë‹¤. GPU ì»¤ë„ë„ ê°€ëŠ¥í•˜ê³ , ê°œë³„ íŒŒíŠ¸ë¥¼ ê°œì¸ ì·¨í–¥ì— ë§ê²Œ ì¡°ì‘í•  ìˆ˜ ìˆë‹¤."
},

{
    "location": "Flux/#ì„¤ì¹˜í•˜ê¸°-1",
    "page": "Flux í™ˆ",
    "title": "ì„¤ì¹˜í•˜ê¸°",
    "category": "section",
    "text": "ì¤„ë¦¬ì•„ 0.6.0 ì´ìƒ, ì•„ì§ ì•ˆê¹”ì•˜ìœ¼ë©´ ì„¤ì¹˜í•˜ì.Pkg.add(\"Flux\")\n# ì„ íƒì¸ë° ì¶”ì²œ\nPkg.update() # íŒ¨í‚¤ì§€ë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ëƒ\nPkg.test(\"Flux\") # ì„¤ì¹˜ ë˜‘ë°”ë¡œ ëœê±´ê°€ í™•ì¸í•˜ê¸°ê¸°ë³¸ì ì¸ ê²ƒ ë¶€í„° ì‹œì‘í•˜ì. ëª¨ë¸ ë™ë¬¼ì›(model zoo)ì€ ì—¬ëŸ¬ê°€ì§€ ê³µí†µ ëª¨ë¸ì„ ë‹¤ë£¨ëŠ”ë° ê·¸ê±¸ë¡œ ì‹œì‘í•´ë„ ì¢‹ë‹¤."
},

{
    "location": "Flux/models/basics/#",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/basics/#ëª¨ë¸-ë§Œë“¤ê¸°-ê¸°ì´ˆ-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ëª¨ë¸ ë§Œë“¤ê¸° ê¸°ì´ˆ",
    "category": "section",
    "text": ""
},

{
    "location": "Flux/models/basics/#ê¸°ìš¸ê¸°(Gradients,-ê²½ì‚¬)-êµ¬í•˜ê¸°-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ê¸°ìš¸ê¸°(Gradients, ê²½ì‚¬) êµ¬í•˜ê¸°",
    "category": "section",
    "text": "ê°„ë‹¨í•œ ë¦¬ë‹ˆì–´ ë¦¬ê·¸ë ˆì…˜(linear regression, ì§ì„  ëª¨ì–‘ìœ¼ë¡œ ê·¸ë ¤ì§€ëŠ” í•¨ìˆ˜)ì„ ìƒê°í•´ ë³´ì. ì´ê²ƒì€ ì…ë ¥ xì— ëŒ€í•œ ì¶œë ¥ ë°°ì—´ yë¥¼ ì˜ˆì¸¡í•œë‹¤. (ì¤„ë¦¬ì•„ REPLì—ì„œ ì˜ˆì œë¥¼ ë”°ë¼í•´ë³´ë©´ ì¢‹ë‹¤)julia> W = rand(2, 5)\n2Ã—5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = rand(2)\n2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> predict(x) = W*x .+ b\npredict (generic function with 1 method)\n\njulia> loss(x, y) = sum((predict(x) .- y).^2)\nloss (generic function with 1 method)\n\njulia> x, y = rand(5), rand(2) # ë”ë¯¸ ë°ì´í„°\n([0.496864, 0.947507, 0.874288, 0.251528, 0.192234], [0.901991, 0.0802404])\n\njulia> loss(x, y) # ~ 3\n3.1660692660286722ì˜ˆì¸¡ì„ ë” ì˜í•˜ê¸° ìœ„í•´ Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ì. loss function(ì†ì‹¤, ì˜ˆì¸¡ ì‹¤íŒ¨ í•¨ìˆ˜)ê³¼ gradient descent(ê²½ì‚¬ í•˜ê°•, ë‚´ë¦¬ë§‰ ê¸°ìš¸ê¸°)ë¥¼ í•´ë³´ë©´ì„œ. ì§ì ‘ ì†ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ë„ ìˆì§€ë§Œ Fluxì—ì„œëŠ” Wì™€ bë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„°(parameters)ë¡œ ë‘˜ ìˆ˜ ìˆë‹¤.julia> using Flux.Tracker\n\njulia> W = param(W)\nTracked 2Ã—5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = param(b)\nTracked 2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> l = loss(x, y)\n3.1660692660286722 (tracked)\n\njulia> back!(l)\nloss(x, y)ëŠ” ë°©ê¸ˆ ì „ê³¼ ê°™ì€ ìˆ˜(3.1660692660286722)ë¥¼ ë¦¬í„´, ê·¸ëŸ°ë° ì´ì œë¶€í„°ëŠ” ê¸°ìš¸ì–´ì§€ëŠ” ëª¨ì–‘ì„ ê´€ì°° ê¸°ë¡í•˜ì—¬ ê°’ì„ ì¶”ì (tracked)  í•œë‹¤. back!ì„ í˜¸ì¶œí•˜ë©´ Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤. ê¸°ìš¸ê¸°ê°€ ë­”ì§€ ì•Œì•„ëƒˆìœ¼ë‹ˆ Wë¥¼ ê³ ì³ê°€ë©´ì„œ ëª¨ë¸ì„ í›ˆë ¨í•˜ì.julia> W.grad\n2Ã—5 Array{Float64,2}:\n 0.949491  1.81066  1.67074  0.480662  0.367352\n 1.49163   2.84449  2.62468  0.755107  0.577101\n\njulia> # íŒŒë¼ë¯¸í„° ì—…ëƒ\n       W.data .-= 0.1(W.grad)\n2Ã—5 Array{Float64,2}:\n  0.762798   0.110647   0.0127989  0.890913  0.473484\n -0.0639541  0.693267  -0.0163046  0.385161  0.714602\n\njulia> loss(x, y) # ~ 2.5\n1.1327711929294395 (tracked)ì˜ˆì¸¡ ì‹¤íŒ¨(loss)ê°€ ì¡°ê¸ˆ ì¤„ì–´ë“¤ì—ˆë‹¤. x ì˜ˆì¸¡ì´ ëª©í‘œ ëŒ€ìƒ(target) yì— ì¢€ ë” ê°€ê¹Œì›Œì¡Œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë°ì´í„°ê°€ ìˆìœ¼ë©´ ëª¨ë¸ í›ˆë ¨í•˜ê¸°ë„ ì‹œë„í•  ìˆ˜ ìˆë‹¤.ë³µì¡í•œ ë”¥ëŸ¬ë‹ì´ Fluxì—ì„œëŠ” ì´ì™€ ê°™ì€ ì˜ˆì œì²˜ëŸ¼ ë‹¨ìˆœí•´ì§„ë‹¤. ë¬¼ë¡  ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°¯ìˆ˜ê°€ ë°±ë§Œê°œê°€ ë„˜ì–´ê°€ê³  ë³µì¡í•œ ì œì–´ íë¦„ì„ ê°–ê²Œ ë˜ë©´ ë‹¤ë¥¸ ëª¨ì–‘ì„ ê°–ê² ì§€. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë³µì¡ì„±ì„ ë‹¤ë£¨ëŠ” ê²ƒì—ëŠ” ë­ê°€ ìˆëŠ”ì§€ í•œë²ˆ ì‚´í´ë³´ì."
},

{
    "location": "Flux/models/basics/#ë ˆì´ì–´-ë§Œë“¤ê¸°-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ë ˆì´ì–´ ë§Œë“¤ê¸°",
    "category": "section",
    "text": "ì´ì œë¶€í„°ëŠ” ë¦¬ë‹ˆì–´ ë¦¬ê·¸ë ˆì…˜ ë³´ë‹¤ ë³µì¡í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì. ì˜ˆë¥¼ ë“¤ì–´, ë‘ ê°œì˜ ë¦¬ë‹ˆì–´ ë ˆì´ì–´ ì‚¬ì´ì— ì‹œê·¸ëª¨ì´ë“œ (Ïƒ) ì²˜ëŸ¼ ë¹„ì„ í˜•(nonlinearity, ì»¤ë¸Œì²˜ëŸ¼ ì§ì„ ì´ ì•„ë‹Œ ê±°)ë¥¼ ê°–ëŠ” ë„˜ì´ ìˆì„ë•Œ, ìœ„ì˜ ìŠ¤íƒ€ì¼ì€ ì•„ë˜ì™€ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤:julia> using Flux\n\njulia> W1 = param(rand(3, 5))\nTracked 3Ã—5 Array{Float64,2}:\n 0.540422  0.680087  0.743124  0.0216563  0.377793\n 0.416939  0.51823   0.464998  0.419852   0.446143\n 0.260294  0.392582  0.46784   0.549495   0.373124\n\njulia> b1 = param(rand(3))\nTracked 3-element Array{Float64,1}:\n 0.213799\n 0.373862\n 0.243417\n\njulia> layer1(x) = W1 * x .+ b1\nlayer1 (generic function with 1 method)\n\njulia> W2 = param(rand(2, 3))\nTracked 2Ã—3 Array{Float64,2}:\n 0.789744  0.389376  0.172613\n 0.472963  0.21518   0.220236\n\njulia> b2 = param(rand(2))\nTracked 2-element Array{Float64,1}:\n 0.121207\n 0.502486\n\njulia> layer2(x) = W2 * x .+ b2\nlayer2 (generic function with 1 method)\n\njulia> model(x) = layer2(Ïƒ.(layer1(x)))\nmodel (generic function with 1 method)\n\njulia> model(rand(5)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 1.06727\n 1.13835ì‘ë™ì€ í•˜ëŠ”ë° ì¤‘ë³µ ì‘ì—…ì´ ë§ì•„ ë³´ê¸°ì— ì¢‹ì§€ ì•Šë‹¤ - íŠ¹íˆ ë ˆì´ì–´ë¥¼ ë” ì¶”ê°€í•œë‹¤ë©´. ë¦¬ë‹ˆì–´ ë ˆì´ì–´ë¥¼ ëŒë ¤ì£¼ëŠ” í•¨ìˆ˜ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì´ê²ƒë“¤ì„ ì •ë¦¬í•˜ì.julia> function linear(in, out)\n         W = param(randn(out, in))\n         b = param(randn(out))\n         x -> W * x .+ b\n       end\nlinear (generic function with 1 method)\n\njulia> linear1 = linear(5, 3) # linear1.W í•  ìˆ˜ ìˆë‹¥ (ìµëª…í•¨ìˆ˜ ë¦¬í„´)\n(::#3) (generic function with 1 method)\n\njulia> linear1.W\nTracked 3Ã—5 Array{Float64,2}:\n -1.72011   -1.07297   0.396755  -0.117604   0.25952\n -0.16694    0.99327  -0.589717  -1.87123    0.141679\n -0.972281  -1.84836   2.55071   -0.136674  -0.147826\n\njulia> linear2 = linear(3, 2)\n(::#3) (generic function with 1 method)\n\njulia> model(x) = linear2(Ïƒ.(linear1(x)))\nmodel (generic function with 1 method)\n\njulia> model(x) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 2.75582\n 0.416809ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” structë¡œ íƒ€ì…ì„ ë§Œë“¤ì–´ì„œ ì–´íŒŒì¸(affine) ë ˆì´ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ìˆë‹¤.julia> struct Affine\n         W\n         b\n       end\n\njulia> Affine(in::Integer, out::Integer) =\n         Affine(param(randn(out, in)), param(randn(out)))\nAffine\n\njulia> # ì˜¤ë²„ë¡œë“œ í•˜ë©´ ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•  ìˆ˜ ìˆë‹¤\n       (m::Affine)(x) = m.W * x .+ m.b\n\njulia> a = Affine(10, 5)\nAffine(param([0.0252182 -1.99122 â€¦ -0.191235 0.294728; 1.13559 1.50226 â€¦ -2.43917 0.56976; â€¦ ; -0.735177 0.202646 â€¦ -0.301945 -0.183598; 1.05967 0.986786 â€¦ -1.57835 -0.0893871]), param([-0.39419, -1.26818, 0.757665, 0.941398, -0.783242]))\n\njulia> a(rand(10)) # => 5-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 5-element Array{Float64,1}:\n -0.945544\n -0.575674\n  2.93741\n  0.111253\n -0.843172ì¶•í•˜í•©ë‹ˆë‹¤! Fluxì—ì„œ ë‚˜ì˜¤ëŠ” Dense ë ˆì´ì–´ ë§Œë“¤ê¸° ì„±ê³µ! FluxëŠ” ë§ì€ ì¬ë°ŒëŠ” ë ˆì´ì–´ë“¤ì´ ìˆëŠ”ë°, ê·¸ê²ƒë“¤ì„ ì§ì ‘ ë§Œë“œëŠ” ê²ƒ ë˜í•œ ì •ë§ ì‰½ë‹¤.(Denseì™€ ë‹¤ë¥¸ í•œê°€ì§€ - í¸ì˜ë¥¼ ìœ„í•´ í™œì„±(activation) í•¨ìˆ˜ë¥¼ ë’¤ì— ì¶”ê°€í•  ìˆ˜ë„ ìˆë‹¤. Dense(10, 5, Ïƒ) ìš”ëŸ°ì‹ìœ¼ë¡œ.)"
},

{
    "location": "Flux/models/basics/#ì´ì˜ê²Œ-ìŒ“ì•„ë³´ì-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ì´ì˜ê²Œ ìŒ“ì•„ë³´ì",
    "category": "section",
    "text": "ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì€ í”í•˜ë‹¤: (layer1 ì´ë¦„ì´ ê²¹ì¹˜ë‹ˆ REPLì„ ìƒˆë¡œ ë„ìš°ì)julia> using Flux\n\njulia> layer1 = Dense(10, 5, Ïƒ)\nDense(10, 5, NNlib.Ïƒ)\n\njulia> # ...\n       model(x) = layer3(layer2(layer1(x)))\nmodel (generic function with 1 method)ê¸°ë‹¤ë—ê²Œ ì—°ê²°(chains) í• ë¼ë¯„, ë‹¤ìŒê³¼ ê°™ì´ ë ˆì´ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ”ê²Œ ì¢€ ë” ì§ê´€ì ì´ë‹¤:julia> layers = [Dense(10, 5, Ïƒ), Dense(5, 2), softmax]\n3-element Array{Any,1}:\n Dense(10, 5, NNlib.Ïƒ)\n Dense(5, 2)\n NNlib.softmax\n\njulia> model(x) = foldl((x, m) -> m(x), x, layers)\nmodel (generic function with 1 method)\n\njulia> model(rand(10)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 0.593021\n 0.406979í¸ë¦¬í•˜ê²Œ ì“°ë¼ê³  ì´ê²ƒ ì—­ì‹œ Fluxì—ì„œ ì œê³µí•œë‹¤:julia> model2 = Chain(\n         Dense(10, 5, Ïƒ),\n         Dense(5, 2),\n         softmax)\nChain(Dense(10, 5, NNlib.Ïƒ), Dense(5, 2), NNlib.softmax)\n\njulia> model2(rand(10)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 0.172085\n 0.827915ê³ ì˜¤ê¸‰ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°™ì•„ ë³´ì¸ë‹¤; ì–´ëŠë§Œí¼ ê°„ë‹¨í•˜ê²Œ ì¶”ìƒí™” í•˜ëŠ”ì§€ ë³´ì•˜ì„ ê²ƒì´ë‹¤. ì¤„ë¦¬ì•„ ì½”ë“œì˜ ê°•ë ¥í•¨ì„ ë†“ì¹˜ì§€ ì•Šì•˜ë‹¤.ì´ëŸ° ì ‘ê·¼ë²•ì˜ ì¢‹ì€ ì ì€ \"ëª¨ë¸\"ì´ í•¨ìˆ˜ë¼ëŠ” ê²ƒì´ë‹¤ (í›ˆë ¨ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜), í•¨ìˆ˜ í•©ì„±(âˆ˜) ë˜í•œ ê°€ëŠ¥í•˜ë‹¤.julia> m = Dense(5, 2) âˆ˜ Dense(10, 5, Ïƒ)\n(::#55) (generic function with 1 method)\n\njulia> m(rand(10))\nTracked 2-element Array{Float64,1}:\n -1.28749\n -0.202492ë§ˆì°¬ê°€ì§€ë¡œ, Chainì€ ì¤„ë¦¬ì•„ í•¨ìˆ˜ì™€ ì´ì˜ê²Œ ë™ì‘í•œë‹¤.julia> m = Chain(x -> x^2, x -> x+1)\nChain(#3, #4)\n\njulia> m(5) # => 26\n26"
},

{
    "location": "Flux/models/basics/#ë ˆì´ì–´-ë„ìš°ë¯¸ë“¤-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ë ˆì´ì–´ ë„ìš°ë¯¸ë“¤",
    "category": "section",
    "text": "FluxëŠ” ì‚¬ìš©ìì˜ ì»¤ìŠ¤í…€ ë ˆì´ì–´ë¥¼ ë„ì™€ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì œê³µí•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œí•˜ë©´julia> Flux.treelike(Affine)\nadapt (generic function with 1 method)Affine ë ˆì´ì–´ì— ë¶€ê°€ì ì¸ ìœ ìš©í•œ ê¸°ëŠ¥ì´ ì¶”ê°€ëœë‹¤, íŒŒë¼ë¯¸í„° ëª¨ìœ¼ê¸°(collecting)ë‚˜ GPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ê°™ì€ ì‘ì—…ì„ í•  ìˆ˜ ìˆë‹¤."
},

{
    "location": "Flux/models/recurrence/#",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ìˆœí™˜(Recurrence)",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/recurrence/#ìˆœí™˜-ëª¨ë¸(Recurrent-Models)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ìˆœí™˜ ëª¨ë¸(Recurrent Models)",
    "category": "section",
    "text": ""
},

{
    "location": "Flux/models/recurrence/#ê¸°ì–µ-ì„¸í¬(Recurrent-Cells,-ìˆœí™˜-ì…€,-ë‡Œë¥¼-ëª¨ë°©í•œ-ê±°)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ê¸°ì–µ ì„¸í¬(Recurrent Cells, ìˆœí™˜ ì…€, ë‡Œë¥¼ ëª¨ë°©í•œ ê±°)",
    "category": "section",
    "text": "ë‹¨ìˆœí•œ í”¼ë“œí¬ì›Œë“œ(feedforward, ì‚¬ì´í´(cycle)ì´ë‚˜ ë£¨í”„(loop)ê°€ ì—†ëŠ” ë„¤íŠ¸ì›Œí¬) ê²½ìš°, ëª¨ë¸ mì€ ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ xáµ¢ì— ëŒ€í•œ yáµ¢ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë‹¤. (ì˜ˆë¥¼ ë“¤ì–´, xë¥¼ MNIST ìˆ«ìë¼ ì¹˜ë©´ yëŠ” ê·¸ê²ƒì„ ë¶„ë¥˜í•œ ìˆ«ì.) ì˜ˆì¸¡ì€ ì„œë¡œ ì™„ì „íˆ ë…ë¦½ì ì´ë©° xê°€ ê°™ìœ¼ë©´ yë„ ì–¸ì œë‚˜ ë™ì¼í•˜ë‹¤.yâ‚ = f(xâ‚)\nyâ‚‚ = f(xâ‚‚)\nyâ‚ƒ = f(xâ‚ƒ)\n# ...ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ëŠ” íˆë“  ìƒíƒœ(hidden state, ìˆ¨ê²¨ë…¼ ìƒíƒœ)ê°€ ì¡´ì¬í•˜ë©° ëª¨ë¸ì„ ëŒë¦´ ë•Œ ë§¤ë²ˆ ê·¸ ìƒíƒœë¥¼ ë‹¤ìŒìœ¼ë¡œ ë„˜ê¸´ë‹¤. ê·¸ë˜ì„œ ì´ ëª¨ë¸ì€ ê·¸ë•Œë§ˆë‹¤ ì´ì „ hë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³ , ìƒˆë¡œìš´ hë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚´ ë†“ëŠ”ë‹¤.h = # ... ì´ˆê¸° ìƒíƒœ ...\nh, yâ‚ = f(h, xâ‚)\nh, yâ‚‚ = f(h, xâ‚‚)\nh, yâ‚ƒ = f(h, xâ‚ƒ)\n# ...hì— ì €ì¥í•œ ì •ë³´ëŠ” ë‹¤ìŒë²ˆ ì˜ˆì¸¡ì„ ìœ„í•´ ìœ ì§€í•˜ê³ , ê·¸ë˜ì„œ ë§ˆì¹˜ í•¨ìˆ˜ì˜ ë©”ëª¨ë¦¬ ê°™ì€ ì—­í• ì„ í•˜ê²Œ í•œë‹¤. ì´ê²ƒì€ xì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì´ì „ê¹Œì§€ ëª¨ë¸ì— ì£¼ì…í•œ ëª¨ë“  ì…ë ¥ìœ¼ë¡œë¶€í„° ì˜í–¥ì„ ë°›ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.(ìš”ê±°ëŠ” ì¤‘ìš”í•œ ê±°ë‹ˆê¹Œ ì˜ˆë¥¼ ë“¤ì–´, xë¥¼ ë¬¸ì¥ì—ì„œì˜ í•œ ë‹¨ì–´ë¼ ë³´ì; ë§Œì•½ì— \"bank\"ë¼ëŠ” ì˜ì–´ ë‹¨ì–´ê°€ ì£¼ì–´ì§€ë©´ ëª¨ë¸ì€ ì´ì „ ì…ë ¥ì´ \"ê°• river\" ì´ë©´ ê°•ë‘‘ìœ¼ë¡œ, \"íˆ¬ì investment\"ë©´ ì€í–‰ìœ¼ë¡œ í•´ì„í•´ì•¼ í•œë‹¤.)Fluxì˜ RNN ì§€ì›ì€ ìˆ˜í•™ì  ê´€ì ì„ ì§€ë‹ˆê³  ìˆë‹¤. ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” RNNì€ í‘œì¤€ \"Dense\" ë ˆì´ì–´ë¥¼ ë”°ë¥´ê³ , ê·¸ ì¶œë ¥ì€ íˆë“  ìƒíƒœì´ë‹¤.julia> using Flux\n\njulia> Wxh = randn(5, 10)\n5Ã—10 Array{Float64,2}:\n -0.197167   0.0931036  -1.13283   â€¦   0.426711   1.5678      0.488363\n -1.19948   -1.05618     1.057        -1.85708    2.05188    -0.732148\n -0.848823   0.147774    1.66139      -0.777346  -0.0650354   0.36015\n -0.380701   0.737349    0.426964      0.694122  -1.46597    -1.00572\n -0.789044  -0.374745   -0.996698      0.505453  -0.117276    1.35148\n\njulia> Whh = randn(5, 5)\n5Ã—5 Array{Float64,2}:\n -1.12946    -0.523065   0.0547692  -0.305124  -0.105809\n -0.195351    0.588007   0.616959    0.779213  -0.145329\n -0.265139   -0.535485  -0.300887    2.13263   -1.53089\n -0.0537235  -1.47912   -0.883858    0.993426  -0.354738\n  0.486817    0.170843   0.0440353   0.177502   0.730423\n\njulia> b   = randn(5)\n5-element Array{Float64,1}:\n  0.982592\n -0.724775\n  0.118081\n  0.140369\n -1.07578\n\njulia> function rnn(h, x)\n         h = tanh.(Wxh * x .+ Whh * h .+ b)\n         return h, h\n       end\nrnn (generic function with 1 method)\n\njulia> x = rand(10) # ë”ë¯¸ ë°ì´í„°\n10-element Array{Float64,1}:\n 0.312436\n 0.384043\n 0.972045\n 0.194086\n 0.496317\n 0.654925\n 0.0311892\n 0.494105\n 0.338846\n 0.204689\n\njulia> h = rand(5)  # ì´ˆê¸° íˆë“  ìƒíƒœ\n5-element Array{Float64,1}:\n 0.861124\n 0.994686\n 0.560054\n 0.371721\n 0.159454\n\njulia> h, y = rnn(h, x)\n([-0.963817, -0.198195, 0.903936, -0.686608, -0.839093], [-0.963817, -0.198195, 0.903936, -0.686608, -0.839093])ë§ˆì§€ë§‰ rnnì„ ì¢€ ë” ëŒë ¤ë³´ë©´, ì¶œë ¥ yëŠ” ì…ë ¥ xê°€ ê°™ì€ë°ë„ ì¡°ê¸ˆì”© ë°”ë€ŒëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.julia> h, y = rnn(h, x)\n([0.812906, -0.767065, 0.945139, 0.0198447, -0.996763], [0.812906, -0.767065, 0.945139, 0.0198447, -0.996763])\n\njulia> h, y = rnn(h, x)\n([-0.647084, -0.799032, 0.997557, 0.902798, -0.984697], [-0.647084, -0.799032, 0.997557, 0.902798, -0.984697])ì•ì„œ ì–¸ê¸‰í•œ rnn í•¨ìˆ˜ëŠ” ëª…ì‹œì ìœ¼ë¡œ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ê¸°ì–µ ì„¸í¬(cells) ì´ë‹¤. ë‹¤ì–‘í•œ ê¸°ì–µ ì„¸í¬ê°€ ì¡´ì¬í•˜ë©° ë ˆì´ì–´ ì°¸ì¡°ì— ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤. ìœ„ì˜ ì˜ˆì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿€ ìˆ˜ ìˆë‹¤:julia> using Flux\n\njulia> rnn2 = Flux.RNNCell(10, 5)\nRNNCell(10, 5, tanh)\n\njulia> x = rand(10) # ë”ë¯¸ ë°ì´í„°\n10-element Array{Float64,1}:\n 0.142406\n 0.944597\n 0.973233\n 0.434782\n 0.715639\n 0.763562\n 0.280661\n 0.293604\n 0.496457\n 0.173372\n\njulia> h = rand(5)  # ì´ˆê¸° íˆë“  ìƒíƒœ\n5-element Array{Float64,1}:\n 0.602545\n 0.998396\n 0.558707\n 0.637564\n 0.0313308\n\njulia> h, y = rnn2(h, x)\n(param([-0.160217, -0.741263, 0.048164, 0.963063, 0.0301785]), param([-0.160217, -0.741263, 0.048164, 0.963063, 0.0301785]))"
},

{
    "location": "Flux/models/recurrence/#ìƒíƒœë¥¼-ê°–ëŠ”-ëª¨ë¸-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ìƒíƒœë¥¼ ê°–ëŠ” ëª¨ë¸",
    "category": "section",
    "text": "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, íˆë“  ìƒíƒœë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” ê±°ëŠ” ê·€ì°®ìœ¼ë‹ˆê¹Œ ëª¨ë¸ì´ ìƒíƒœë¥¼ ê°–ê²Œë” ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. FluxëŠ” Recur ë˜í¼ë¥¼ ì œê³µí•œë‹¤.julia> x = rand(10)\n10-element Array{Float64,1}:\n 0.165593\n 0.502313\n 0.120926\n 0.505827\n 0.917068\n 0.557163\n 0.688472\n 0.791826\n 0.0838632\n 0.709302\n\njulia> h = rand(5)\n5-element Array{Float64,1}:\n 0.40008\n 0.48858\n 0.551568\n 0.0688404\n 0.0583865\n\njulia> m = Flux.Recur(rnn, h)\nRecur(rnn)\n\njulia> y = m(x)\n5-element Array{Float64,1}:\n  0.963414\n -0.999974\n  0.739107\n  0.976241\n  0.986023Recur ë˜í¼ëŠ” m.state í•„ë“œì— ìƒíƒœë¥¼ ì €ì¥í•œë‹¤.RNN(10, 5) ìƒì„±ìë¥¼ ì‚¬ìš©í•˜ë©´ - RNNCellê³¼ ëŒ€ì‘í•˜ëŠ” - ë‹¤ìŒê³¼ ê°™ì´ ì´ê±°ëŠ” ë‹¨ìˆœíˆ ë˜í¼ ì…€ì´ë‹¤.julia> RNN(10, 5)\nRecur(RNNCell(10, 5, tanh))"
},

{
    "location": "Flux/models/recurrence/#ì‹œí€€ìŠ¤(Sequences,-ì—°ì†ë˜ëŠ”-ê°’)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ì‹œí€€ìŠ¤(Sequences, ì—°ì†ë˜ëŠ” ê°’)",
    "category": "section",
    "text": "ì¢…ì¢… ê°œë³„ì ì¸ x ë³´ë‹¤ëŠ” ì—°ì†ë˜ëŠ” ì…ë ¥ì„ ë‹¤ë£¨ê¸¸ ì›í•œë‹¤.julia> seq = [rand(10) for i = 1:10]\n10-element Array{Array{Float64,1},1}:\n [0.443911, 0.955247, 0.980153, 0.313181, 0.0426581, 0.354755, 0.113961, 0.222873, 0.865114, 0.14094]\n [0.50466, 0.0204917, 0.890547, 0.574102, 0.301098, 0.944295, 0.95414, 0.36809, 0.341546, 0.474998]\n [0.474114, 0.152628, 0.364967, 0.601978, 0.212361, 0.66016, 0.12101, 0.944988, 0.417781, 0.715282]\n [0.0776375, 0.843099, 0.000618674, 0.352273, 0.977611, 0.801756, 0.550702, 0.311638, 0.285711, 0.0856441]\n [0.603498, 0.863035, 0.89494, 0.506224, 0.840984, 0.13453, 0.43549, 0.216554, 0.361081, 0.0965758]\n [0.236062, 0.407028, 0.357854, 0.875694, 0.0468227, 0.786622, 0.616748, 0.791976, 0.800668, 0.147169]\n [0.739452, 0.38329, 0.961215, 0.113691, 0.381309, 0.57526, 0.0170709, 0.403656, 0.445509, 0.051497]\n [0.956629, 0.624735, 0.14811, 0.202354, 0.484018, 0.250409, 0.0352729, 0.809209, 0.831828, 0.826355]\n [0.388553, 0.42596, 0.736068, 0.454156, 0.626974, 0.641246, 0.444018, 0.768584, 0.118879, 0.416568]\n [0.307721, 0.176393, 0.371934, 0.714272, 0.886859, 0.333667, 0.721609, 0.975586, 0.59609, 0.771424]Recurë¡œ ëª¨ë¸ì„ ì‹œí€€ìŠ¤ì˜ ê° í•­ëª©ë§ˆë‹¤ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤:julia> m.(seq) # 5-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ëŒë ¤ì¤€ë‹¤\n10-element Array{Array{Float64,1},1}:\n [0.958516, -0.996974, 0.640934, -0.440203, 0.991754]\n [0.998417, -0.998238, 0.988128, 0.924522, 0.999099]\n [0.943455, -0.999939, 0.94332, 0.638572, 0.999795]\n [0.997841, -0.999912, 0.414106, 0.705974, 0.999871]\n [0.9896, -0.96634, 0.903348, 0.805409, 0.949429]\n [0.990047, -0.999849, 0.991448, 0.950895, 0.999938]\n [0.980617, -0.988072, 0.978565, -0.785643, 0.985682]\n [0.98617, -0.99938, -0.791134, 0.603178, 0.0937938]\n [0.946547, -0.893022, 0.914559, 0.999905, 0.984556]\n [0.989439, -0.999979, 0.964896, 0.978421, 0.999834]ë” ì»¤ë‹¤ë€ ëª¨ë¸ì— ìˆœí™˜ ë ˆì´ì–´(recurrent layers)ë¥¼ ì—°ì‡„ì (chain)ìœ¼ë¡œ ì—°ê²° í•  ìˆ˜ ìˆë‹¤.julia> m = Chain(LSTM(10, 15), Dense(15, 5))\nChain(Recur(LSTMCell(10, 60)), Dense(15, 5))\n\njulia> m.(seq)\n10-element Array{TrackedArray{â€¦,Array{Float64,1}},1}:\n param([0.0779735, 0.0534096, -0.0245852, -0.0699291, -0.00650743])\n param([0.203825, -0.0307184, -0.0940759, -0.100437, 0.0523315])\n param([0.21071, -0.19635, -0.106985, -0.185204, 0.132647])\n param([0.314643, -0.205525, -0.00144219, -0.165195, 0.197256])\n param([0.351024, -0.116196, 0.00489051, -0.255343, 0.209503])\n param([0.370406, -0.125797, -0.0506301, -0.253045, 0.179001])\n param([0.349787, -0.091392, -0.0699977, -0.249944, 0.197391])\n param([0.370064, -0.21158, -0.00144108, -0.337597, 0.24153])\n param([0.396285, -0.240793, -0.0263459, -0.358695, 0.260678])\n param([0.464372, -0.316526, -0.0295575, -0.352548, 0.251627])"
},

{
    "location": "Flux/models/recurrence/#ê¸°ìš¸ê¸°-ê³„ì‚°-ê¸°ë¡-ì˜ë¼ë‚´ê¸°(Truncating-Gradients)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ê¸°ìš¸ê¸° ê³„ì‚° ê¸°ë¡ ì˜ë¼ë‚´ê¸°(Truncating Gradients)",
    "category": "section",
    "text": "ê¸°ë³¸ì ìœ¼ë¡œ, ìˆœí™˜ ë ˆì´ì–´ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì „ì²´ ê¸°ë¡(history)ì„ ë‚´í¬í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 100ê°œì˜ ì…ë ¥ì„ ê°€ì§„ ëª¨ë¸ì„ ì‹¤í–‰í•  ë•Œ, back!ì„ í•˜ë©´ 100ê°œì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤. ê·¸ëŸ¬ê³  ë‹¤ë¥¸ 10ê°œì˜ ì…ë ¥ì„ ë” ê³„ì‚°í•œë‹¤ë©´ 110ê°œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤ - ì´ê±°ëŠ” ëˆ„ì ë˜ë¯€ë¡œ ë¹ ë¥´ê²Œ ì—°ì‚° ë¹„ìš©ì´ ì¦ê°€í•œë‹¤.ì´ê±°ë¥¼ ë§‰ëŠ” ë°©ë²•ì€ ê¸°ìš¸ê¸° ê³„ì‚° ê¸°ë¡ì„ ì˜ë¼ë‚´ì–´(truncate) ì§€ì›Œì£¼ëŠ” ê²ƒì´ë‹¤.julia> Flux.truncate!(m)\ntruncate!ì„ í˜¸ì¶œí•˜ë©´ ê¹”ë”ì´ ì²­ì†Œí•´ ì¤€ë‹¤. ê·¸ë˜ì„œ ë” ë§ì€ ì…ë ¥ì˜ ëª¨ë¸ì„ ì‹¤í–‰í•´ë„ ë¹„ì‹¼ ê¸°ìš¸ê¸° ì—°ì‚° ì—†ì´ í•´ë‚¼ ìˆ˜ ìˆë‹¤.truncate!ëŠ” ì—¬ëŸ¬ ê°œì˜ ì»¤ë‹¤ë€ ì‹œí€€ìŠ¤ ë©ì–´ë¦¬ë¥¼ ë‹¤ë£° ë•Œ ìœ ìš©í•˜ì§€ë§Œ, ì„œë¡œ ë…ë¦½ì ì¸ ì‹œí€€ìŠ¤ë“¤ì„ ë‹¤ë£¨ê³  ì‹¶ì„ ë•Œë„ ìˆë‹¤. ê·¸ ê²½ìš° íˆë“  ìƒíƒœëŠ” ì›ë˜ ê°’ìœ¼ë¡œ ì™„ì „íˆ ì´ˆê¸°í™” ë˜ì–´ ëˆ„ì ëœ ì •ë³´ë¥¼ ë²„ë¦°ë‹¤. ê·¸ë ‡ê²Œ í•˜ê³  ì‹¶ìœ¼ë©´ reset!ì„ í•´ ì£¼ì.julia> Flux.reset!(m)\n"
},

{
    "location": "Flux/models/regularisation/#",
    "page": "ì •ê·œí™”(Regularisation)",
    "title": "ì •ê·œí™”(Regularisation)",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/regularisation/#ì •ê·œí™”(Regularisation)-1",
    "page": "ì •ê·œí™”(Regularisation)",
    "title": "ì •ê·œí™”(Regularisation)",
    "category": "section",
    "text": "ì´ë²ˆì—ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì •ê·œí™” í•´ ë³´ì. vecnormê³¼ ê°™ì€ ì •ê·œí™”ë¥¼ í•´ì£¼ëŠ” ì ì ˆí•œ í•¨ìˆ˜ë¥¼ ê° ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì ìš©í•˜ì—¬ ê·¸ ê²°ê³¼ë¥¼ ëª¨ë“  lossì— ë”í•˜ë„ë¡ í•˜ì.ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ ê°„ë‹¨í•œ regressionì„ ë³´ì.julia> using Flux\n\njulia> m = Dense(10, 5)\nDense(10, 5)\n\njulia> loss(x, y) = Flux.crossentropy(softmax(m(x)), y)\nloss (generic function with 1 method)m.Wì™€ m.b íŒŒë¼ë¯¸í„°ì— L2 normì„ ì·¨í•˜ì—¬ ì •ê·œí™” í•´ë³´ì.julia> penalty() = vecnorm(m.W) + vecnorm(m.b)\npenalty (generic function with 1 method)\n\njulia> loss(x, y) = Flux.crossentropy(softmax(m(x)), y) + penalty()\nloss (generic function with 1 method)ë ˆì´ì–´ë¥¼ ì´ìš©í•˜ëŠ” ê²½ìš°, FluxëŠ” params í•¨ìˆ˜ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. sum(vecnorm, params)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ë¥¼ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤.julia> params(m)\n2-element Array{Any,1}:\n param([-0.61839 -0.556047 â€¦ -0.460808 -0.107646; 0.346293 -0.375076 â€¦ -0.608704 -0.181025; â€¦ ; -0.2226 -0.0992159 â€¦ 0.0707984 -0.429173; -0.331058 -0.291995 â€¦ 0.383368 0.156716])\n param([0.0, 0.0, 0.0, 0.0, 0.0])\n\njulia> sum(vecnorm, params(m))\n2.4130860599427706 (tracked)ì¢€ ë” í° ê·œëª¨ì˜ ì˜ˆë¡œ, ë©€í‹°-ë ˆì´ì–´ í¼ì…‰íŠ¸ë¡ (perceptron)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.julia> m = Chain(\n         Dense(28^2, 128, relu),\n         Dense(128, 32, relu),\n         Dense(32, 10), softmax)\nChain(Dense(784, 128, NNlib.relu), Dense(128, 32, NNlib.relu), Dense(32, 10), NNlib.softmax)\n\njulia> loss(x, y) = Flux.crossentropy(m(x), y) + sum(vecnorm, params(m))\nloss (generic function with 1 method)\n\njulia> loss(rand(28^2), rand(10))\n39.128892409412174 (tracked)"
},

{
    "location": "Flux/models/layers/#",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/layers/#ê¸°ë³¸-ë ˆì´ì–´-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ê¸°ë³¸ ë ˆì´ì–´",
    "category": "section",
    "text": "ê±°ì˜ ëª¨ë“  ì‹ ê²½ë§(neural networks)ì˜ í† ëŒ€ë¥¼ ë‹¤ìŒì˜ í•µì‹¬ ë ˆì´ì–´ë¡œ êµ¬ì„±í•œë‹¤.Chain\nDense\nConv2D"
},

{
    "location": "Flux/models/layers/#ìˆœí™˜-ë ˆì´ì–´(Recurrent-Layers)-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ìˆœí™˜ ë ˆì´ì–´(Recurrent Layers)",
    "category": "section",
    "text": "ìœ„ì˜ í•µì‹¬ ë ˆì´ì–´ì™€ í•¨ê»˜, ì‹œí€€ìŠ¤ ë°ì´í„°(ë‹¤ë¥¸ ì¢…ë¥˜ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°)ë¥¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.RNN\nLSTM\nFlux.Recur"
},

{
    "location": "Flux/models/layers/#í™œì„±-í•¨ìˆ˜(Activation-Functions)-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "í™œì„± í•¨ìˆ˜(Activation Functions)",
    "category": "section",
    "text": "ëª¨ë¸ì˜ ë ˆì´ì–´ ì¤‘ê°„ì— ë¹„ì„ í˜•ì„±(Non-linearities)ì„ ê°–ì„ ë•Œ ì‚¬ìš©í•œë‹¤. í•¨ìˆ˜ì˜ ëŒ€ë¶€ë¶„ì€ NNlibì— ì •ì˜ë˜ì–´ ìˆê³  Fluxì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.íŠ¹ë³„í•œ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ í™œì„± í•¨ìˆ˜ëŠ” ë³´í†µ ìŠ¤ì¹¼ë¼(scalars) ê°’ì„ ì²˜ë¦¬í•œë‹¤. ë°°ì—´ì— ì ìš©í•˜ë ¤ë©´ Ïƒ.(xs), relu.(xs) ì²˜ëŸ¼ .ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŒ… í•´ ì£¼ì.Ïƒ\nrelu\nleakyrelu\nelu\nswish"
},

{
    "location": "Flux/models/layers/#ì •ìƒí™”(Normalisation)-and-ì •ê·œí™”(Regularisation)-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ì •ìƒí™”(Normalisation) & ì •ê·œí™”(Regularisation)",
    "category": "section",
    "text": "ì´ ë ˆì´ì–´ë“¤ì€ ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì¡°ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ í›ˆë ¨ ì‹œê°„(training times)ì˜ ê°œì„  ê·¸ë¦¬ê³  ì˜¤ë²„í”¼íŒ…(overfitting, ê³¼ì í•©)ì„ ì¤„ì—¬ ì¤€ë‹¤.Flux.testmode!\nBatchNorm\nDropout\nLayerNorm"
},

{
    "location": "Flux/training/optimisers/#",
    "page": "ìµœì í™”",
    "title": "ìµœì í™”",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/training/optimisers/#ìµœì í™”-í•¨ìˆ˜(Optimisers)-1",
    "page": "ìµœì í™”",
    "title": "ìµœì í™” í•¨ìˆ˜(Optimisers)",
    "category": "section",
    "text": "ê°„ë‹¨í•œ ë¦¬ë‹ˆì–´ ë¦¬ê·¸ë ˆì…˜ì—ì„œ ìš°ë¦¬ëŠ” ë”ë¯¸ ë°ì´í„°ë¥¼ ë§Œë“  í›„, ì†ì‹¤(loss)ì„ ê³„ì‚°í•˜ê³  ì—­ì „íŒŒ(backpropagate) í•˜ì—¬ íŒŒë¼ë¯¸í„° Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì˜€ë‹¤.julia> using Flux\n\njulia> W = param(rand(2, 5))\nTracked 2Ã—5 Array{Float64,2}:\n 0.215021  0.22422   0.352664  0.11115   0.040711\n 0.180933  0.769257  0.361652  0.783197  0.545495\n\njulia> b = param(rand(2))\nTracked 2-element Array{Float64,1}:\n 0.205216\n 0.150938\n\njulia> predict(x) = W*x .+ b\npredict (generic function with 1 method)\n\njulia> loss(x, y) = sum((predict(x) .- y).^2)\nloss (generic function with 1 method)\n\njulia> x, y = rand(5), rand(2) # ë”ë¯¸ ë°ì´í„°\n([0.153473, 0.927019, 0.40597, 0.783872, 0.392236], [0.261727, 0.00917161])\n\njulia> l = loss(x, y) # ~ 3\n3.6352060699201565 (tracked)\n\njulia> Flux.back!(l)\nê¸°ìš¸ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ê³ ì í•œë‹¤. ì†ì‹¤ì„ ì¤„ì´ë ¤ê³  ë§ì´ë‹¤. ì—¬ê¸°ì„œ í•œê°€ì§€ ë°©ë²•ì€:function update()\n  Î· = 0.1 # í•™ìŠµí•˜ëŠ” ì†ë„(Learning Rate)\n  for p in (W, b)\n    p.data .-= Î· .* p.grad # ì—…ë°ì´íŠ¸ ì ìš©\n    p.grad .= 0            # ê¸°ìš¸ê¸° 0ìœ¼ë¡œ clear\n  end\nendupdateë¥¼ í˜¸ì¶œí•˜ë©´ íŒŒë¼ë¯¸í„° Wì™€ bëŠ” ë°”ë€Œê³  ì†ì‹¤(loss)ì€ ë‚´ë ¤ê°„ë‹¤.ë‘ê°€ì§€ëŠ” ì§šê³  ë„˜ì–´ê°€ì: ëª¨ë¸ì—ì„œ í›ˆë ¨í•  íŒŒë¼ë¯¸í„°ì˜ ëª©ë¡ (ì—¬ê¸°ì„œëŠ” [W, b]), ê·¸ë¦¬ê³  ì—…ë°ì´íŠ¸ ì§„í–‰ ì†ë„. ì—¬ê¸°ì„œì˜ ì—…ë°ì´íŠ¸ëŠ” ê°„ë‹¨í•œ gradient descent(ê²½ì‚¬ í•˜ê°•, x .-= Î· .* Î”) ì˜€ì§€ë§Œ, ëª¨ë©˜í…€(momentum)ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ë‹¤ ì–´ë ¤ìš´ ê²ƒë„ í•´ë³´ê³  ì‹¶ì„ ê²ƒì´ë‹¤.ì—¬ê¸°ì„œ ë³€ìˆ˜ë¥¼ ì–»ëŠ” ê²ƒì€ ì•„ë¬´ê²ƒë„ ì•„ë‹ˆì§€ë§Œ, ë ˆì´ì–´ë¥¼ ë³µì¡í•˜ê²Œ ìŒ“ëŠ”ë‹¤ë©´ ê³¨ì¹˜ ì¢€ ì•„í”Œ ê²ƒì´ë‹¤.julia> m = Chain(\n         Dense(10, 5, Ïƒ),\n         Dense(5, 2), softmax)\nChain(Dense(10, 5, NNlib.Ïƒ), Dense(5, 2), NNlib.softmax)[m[1].W, m[1].b, ...] ì´ë ‡ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒ ëŒ€ì‹ , Fluxì—ì„œ ì œê³µí•˜ëŠ” params(m) í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ ëª©ë¡ì„ êµ¬í•  ê²ƒì´ë‹¤.julia> opt = SGD([W, b], 0.1) # Gradient descent(ê²½ì‚¬ í•˜ê°•)ì„ learning rate(í•™ìŠµ ì†ë„) 0.1 ìœ¼ë¡œ í•œë‹¤\n(::#71) (generic function with 1 method)\n\njulia> opt() # `W`ì™€ `b`ë¥¼ ë³€ê²½í•˜ë©° ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•œë‹¤\nìµœì í™” í•¨ìˆ˜ëŠ” íŒŒë¼ë¯¸í„° ëª©ë¡ì„ ë°›ì•„ ìœ„ì˜ updateì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ ëŒë ¤ì¤€ë‹¤. optë‚˜ updateë¥¼ í›ˆë ¨ ë£¨í”„(training loop)ì— ë„˜ê²¨ì¤„ ìˆ˜ ìˆëŠ”ë°, ë§¤ë²ˆ ë°ì´í„°ì˜ ë¯¸ë‹ˆ-ë°°ì¹˜(mini-batch)ë¥¼ í•œ í›„ì— ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ê²ƒì´ë‹¤."
},

{
    "location": "Flux/training/optimisers/#ìµœì í™”-í•¨ìˆ˜-ì°¸ê³ -1",
    "page": "ìµœì í™”",
    "title": "ìµœì í™” í•¨ìˆ˜ ì°¸ê³ ",
    "category": "section",
    "text": "ëª¨ë“  ìµœì í™” í•¨ìˆ˜ëŠ” ë„˜ê²¨ë°›ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” í•¨ìˆ˜ë¥¼ ëŒë ¤ì¤€ë‹¤.SGD\nMomentum\nNesterov\nADAM"
},

{
    "location": "Flux/training/training/#",
    "page": "í›ˆë ¨ì‹œí‚¤ê¸°",
    "title": "í›ˆë ¨ì‹œí‚¤ê¸°",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/training/training/#í›ˆë ¨ì‹œí‚¤í‚¤(Training)-1",
    "page": "í›ˆë ¨ì‹œí‚¤ê¸°",
    "title": "í›ˆë ¨ì‹œí‚¤í‚¤(Training)",
    "category": "section",
    "text": "ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ë ¤ë©´ ì„¸ê°€ì§€ê°€ í•„ìš”í•˜ë‹¤:ëª©í‘œ í•¨ìˆ˜(objective function), ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì–¼ë§Œí¼ ì˜ í‰ê°€í•  ê²ƒì¸ê°€.\në°ì´í„° í¬ì¸íŠ¸ì˜ ë¬¶ìŒ(A collection of data points)ì„ ëª©í‘œ í•¨ìˆ˜ì— ë„˜ê²¨ì¤„ ê²ƒì´ë‹¤.\nìµœì í™” í•¨ìˆ˜ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì ì ˆí•˜ê²Œ ì—…ë°ì´íŠ¸ í•  ê²ƒì´ë‹¤.ê·¸ë¦¬í•˜ì—¬ Flux.train!ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œí•œë‹¤:Flux.train!(objective, data, opt)ëª¨ë¸ ë™ë¬¼ì›(model zoo)ì— ì—¬ëŸ¬ê°€ì§€ ì˜ˆì œê°€ ìˆë‹¤."
},

{
    "location": "Flux/training/training/#ì†ì‹¤-í•¨ìˆ˜(Loss-Functions)-1",
    "page": "í›ˆë ¨ì‹œí‚¤ê¸°",
    "title": "ì†ì‹¤ í•¨ìˆ˜(Loss Functions)",
    "category": "section",
    "text": "ëª©í‘œ í•¨ìˆ˜ëŠ” ë°˜ë“œì‹œ ëª¨ë¸ê³¼ ëŒ€ìƒ(target)ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìë¥¼ ëŒë ¤ì£¼ì–´ì•¼ í•œë‹¤ - ëª¨ë¸ì˜ loss. ê¸°ì´ˆì—ì„œ ì •ì˜í•œ loss í•¨ìˆ˜ê°€ ëª©í‘œ(an objective)ë¡œì„œ ì‘ë™í•  ê²ƒì´ë‹¤. ëª¨ë¸ì˜ ê´€ì ì—ì„œ ëª©í‘œë¥¼ ì •ì˜í•  ìˆ˜ë„ ìˆë‹¤:julia> using Flux\n\njulia> m = Chain(\n         Dense(784, 32, Ïƒ),\n         Dense(32, 10), softmax)\nChain(Dense(784, 32, NNlib.Ïƒ), Dense(32, 10), NNlib.softmax)\n\njulia> loss(x, y) = Flux.mse(m(x), y)\nloss (generic function with 1 method)\n\n# ë‚˜ì¤‘ì—\njulia> Flux.train!(loss, data, opt)ëª©í‘œëŠ” í•­ìƒ m(x)ì˜ ì˜ˆì¸¡ê³¼ ëŒ€ìƒ yì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë¹„ìš© í•¨ìˆ˜(cost function)ì˜ ê´€ì ì—ì„œ ì •ì˜ëœë‹¤. FluxëŠ” mean squared errorë¥¼ êµ¬í•˜ëŠ” mseë‚˜, cross entropy lossë¥¼ êµ¬í•˜ëŠ” crossentropy ê°™ì€ ë¹„ìš© í•¨ìˆ˜ë¥¼ ë‚´ì¥í•˜ê³  ìˆë‹¤. ì›í•œë‹¤ë©´ ì§ì ‘ ê³„ì‚°í•´ ë³¼ ìˆ˜ë„ ìˆë‹¤."
},

{
    "location": "Flux/training/training/#ë°ì´í„°ì„¸íŠ¸(Datasets)-1",
    "page": "í›ˆë ¨ì‹œí‚¤ê¸°",
    "title": "ë°ì´í„°ì„¸íŠ¸(Datasets)",
    "category": "section",
    "text": "data ì¸ìëŠ” í›ˆë ¨í•  ë°ì´í„°(ë³´í†µ ì…ë ¥ xì™€ target ì¶œë ¥ y)ì˜ ë¬¶ìŒì„ ì œê³µí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë”± í•˜ë‚˜ ìˆëŠ” ë”ë¯¸ ë°ì´í„° ì„¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:x = rand(784)\ny = rand(10)\ndata = [(x, y)]Flux.train!ì€ loss(x, y)ì„ í˜¸ì¶œí•˜ê³ , ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ë©°, ê°€ì¤‘ì¹˜(weights)ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ë‹¤ìŒ ë°ì´í„° í¬ì¸íŠ¸ë¡œ ì´ë™í•œë‹¤. ê°™ì€ ë°ì´í„°ë¥¼ ì„¸ ë²ˆ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆë‹¤:data = [(x, y), (x, y), (x, y)]\n# ë˜ëŠ” ì•„ë˜ì™€ ê°™ì´\ndata = Iterators.repeated((x, y), 3)xì™€ yëŠ” ë³„ë„ë¡œ ì½ì–´ë“¤ì–´ëŠ” ê²ƒì´ ë³´í†µì´ë‹¤. ì´ëŸ´ ê²½ìš°ì— zipì„ ì“¸ ìˆ˜ ìˆë‹¤:xs = [rand(784), rand(784), rand(784)]\nys = [rand( 10), rand( 10), rand( 10)]\ndata = zip(xs, ys)ê¸°ë³¸ì ìœ¼ë¡œ train!ì€ ë°ì´í„°ë¥¼ ì˜¤ì§ í•œë²ˆë§Œ ìˆœíšŒí•œë‹¤ (í•œ ì„¸ëŒ€, a single \"epoch\"). ì—¬ëŸ¬ ì„¸ëŒ€ë¥¼ ëŒë¦¬ëŠ” @epochs ë§¤í¬ë¡œë¥¼ ì œê³µí•˜ê³  ìˆìœ¼ë‹ˆ REPLì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ í•´ ë³´ì.julia> using Flux: @epochs\n\njulia> @epochs 2 println(\"hello\")\nINFO: Epoch 1\nhello\nINFO: Epoch 2\nhello\n\njulia> @epochs 2 Flux.train!(...)\n# ë‘ ì„¸ëŒ€ì— ê±¸ì³ í›ˆë ¨í•œë‹¤"
},

{
    "location": "Flux/training/training/#ì»¬ë°±(Callbacks)-1",
    "page": "í›ˆë ¨ì‹œí‚¤ê¸°",
    "title": "ì»¬ë°±(Callbacks)",
    "category": "section",
    "text": "train!ì€ cb ì¸ìë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë°›ëŠ”ë°, ì»¬ë°± í•¨ìˆ˜ë¥¼ ì¤˜ì„œ í›ˆë ¨ ê³¼ì •ì„ ì§€ì¼œë³¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´:train!(objective, data, opt, cb = () -> println(\"training\"))ì»¬ë°±ì€ í›ˆë ¨ ë°ì´í„°ì˜ ë°°ì¹˜(batch) ë§ˆë‹¤ í˜¸ì¶œëœë‹¤. ì¢€ë” ì ê²Œ í˜¸ì¶œí•˜ë ¤ë©´ Flux.throttle(f, timeout)ë¥¼ ì£¼ì–´ fê°€ ë§¤ timeout ì´ˆ ì´ìƒ í˜¸ì¶œë˜ëŠ” ê²ƒì„ ë§‰ëŠ”ë‹¤.ì»¬ë°±ì„ ì‚¬ìš©í•˜ëŠ” ì „í˜•ì ì¸ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:test_x, test_y = # ... í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë‹¨ì¼ ë°°ì¹˜(single batch) ë§Œë“¤ê¸° ...\nevalcb() = @show(loss(test_x, test_y))\n\nFlux.train!(objective, data, opt,\n            cb = throttle(evalcb, 5))"
},

{
    "location": "Flux/data/onehot/#",
    "page": "ì›-í•« ì¸ì½”ë”©",
    "title": "ì›-í•« ì¸ì½”ë”©",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/data/onehot/#ì›-í•«-ì¸ì½”ë”©(One-Hot-Encoding)-1",
    "page": "ì›-í•« ì¸ì½”ë”©",
    "title": "ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)",
    "category": "section",
    "text": "ì°¸true, ê±°ì§“false í˜¹ì€ ê³ ì–‘ì´cat, ê°•ì•„ì§€dog ì™€ ê°™ì€ ë²”ì£¼í˜• ë³€ìˆ˜(categorical variables)ë¡œ ì¸ì½”ë”© í•´ ë³´ì. \"one-of-k\" ë˜ëŠ” \"one-hot\" í˜•ì‹ì´ ë˜ê³  FluxëŠ” onehot í•¨ìˆ˜ë¡œ ì‰½ê²Œ í•  ìˆ˜ ìˆë‹¤.julia> using Flux: onehot\n\njulia> onehot(:b, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n  true\n false\n\njulia> onehot(:c, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n false\n  trueì—­í•¨ìˆ˜ëŠ” argmax (ë¶ˆë¦¬ì–¸ ì´ë‚˜ ì¼ë°˜ í™•ë¥  ë¶„í¬(general probability distribution)ë¥¼ ì¸ìë¡œ ë°›ëŠ”ë‹¤) ì´ë‹¤.julia> argmax(ans, [:a, :b, :c])\n:c\n\njulia> argmax([true, false, false], [:a, :b, :c])\n:a\n\njulia> argmax([0.3, 0.2, 0.5], [:a, :b, :c])\n:c"
},

{
    "location": "Flux/data/onehot/#ë°°ì¹˜(Batches)-1",
    "page": "ì›-í•« ì¸ì½”ë”©",
    "title": "ë°°ì¹˜(Batches)",
    "category": "section",
    "text": "onehotbatchëŠ” ì›-í•« ë²¡í„°ì˜ ë°°ì¹˜(batch, ë§¤íŠ¸ë¦­ìŠ¤)ë¥¼ ë§Œë“¤ì–´ ì¤€ë‹¤. argmaxëŠ” ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ ì·¨ê¸‰í•œë‹¤.julia> using Flux: onehotbatch\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3Ã—3 Flux.OneHotMatrix:\n false   true  false\n  true  false   true\n false  false  false\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Array{Symbol,1}:\n  :b\n  :a\n  :bìœ„ì˜ ì—°ì‚°ì€ Array ëŒ€ì‹  OneHotVectorì™€ OneHotMatrixë¥¼ ëŒë ¤ì¤€ë‹¤. OneHotVectorëŠ” ì¼ë°˜ì ì¸ ë²¡í„°ì²˜ëŸ¼ ë™ì‘í•˜ëŠ”ë° ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë°”ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ê³„ì‚° ë¹„ìš©ì´ ë“¤ì§€ ì•Šë„ë¡ ì²˜ë¦¬í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë§¤íŠ¸ë¦­ìŠ¤ì™€ ì›-í•« ë²¡í„°ì„ ê³±í•˜ëŠ” ê²½ìš°, ë‚´ë¶€ì ìœ¼ë¡œëŠ” ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ê´€ë ¨ëœ í–‰ë§Œì„ ì˜ë¼ë‚´ëŠ” ì‹ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤."
},

{
    "location": "Flux/gpu/#",
    "page": "GPU ì§€ì›",
    "title": "GPU ì§€ì›",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/gpu/#GPU-ì§€ì›-1",
    "page": "GPU ì§€ì›",
    "title": "GPU ì§€ì›",
    "category": "section",
    "text": "GPU ê°™ì´ í•˜ë“œì›¨ì–´ ë°±ì—”ë“œë¡œ í•˜ëŠ” ë°°ì—´ ì—°ì‚°ì˜ ì§€ì›ì€ CuArraysì™€ ê°™ì€ ì™¸ë¶€ íŒ¨í‚¤ì§€ë¥¼ ì œê³µí•œë‹¤. FluxëŠ” ë°°ì—´ì˜ íƒ€ì…ì„ ì •í•˜ì§€ ì•Šì•˜ê¸°ì—(agnostic) ëª¨ë¸ ê°€ì¤‘ì¹˜(weights)ì™€ ë°ì´í„°ë¥¼ GPUì— ì˜®ê²¨ì£¼ë©´ Fluxê°€ ì´ë¥¼ ë‹¤ë£° ìˆ˜ ìˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, CuArrays (cu ì»¨ë²„í„°ë¡œ ë³€í™˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì˜ˆì œë¥¼ NVIDIA GPUì—ì„œ ëŒë¦´ ìˆ˜ ìˆë‹¤.using CuArrays\n\nW = cu(rand(2, 5)) # 2Ã—5 CuArray\nb = cu(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = cu(rand(5)), cu(rand(2)) # ë”ë¯¸ ë°ì´í„°\nloss(x, y) # ~ 3íŒŒë¼ë¯¸í„° (W, b)ì™€ ë°ì´í„° ì„¸íŠ¸ (x, y)ë¥¼ cuda ë°°ì—´ë¡œ ë³€í™˜í•˜ì˜€ë‹¤. ë„í•¨ìˆ˜(derivatives)ì™€ í›ˆë ¨ ê°’ì€ ì „ê³¼ ë™ì¼í•˜ë‹¤.Dense ë ˆì´ì–´ë‚˜ Chain ê°™ì€ ì¡°ë¦½ ëª¨ë¸(structured model)ë¥¼ ì •ì˜í•˜ì˜€ìœ¼ë©´, ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ë³€í™˜ì‹œì¼œì•¼ í•œë‹¤. Fluxì—ì„œ ì œê³µí•˜ëŠ” mapleaves í•¨ìˆ˜ë¡œ ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œêº¼ë²ˆì— ë³€ê²½í•  ìˆ˜ ìˆë‹¤.d = Dense(10, 5, Ïƒ)\nd = mapleaves(cu, d)\nd.W # Tracked CuArray\nd(cu(rand(10))) # CuArray output\n\nm = Chain(Dense(10, 5, Ïƒ), Dense(5, 2), softmax)\nm = mapleaves(cu, m)\nd(cu(rand(10)))í¸ì˜ìƒ FluxëŠ” gpu í•¨ìˆ˜ë¥¼ ì œê³µí•˜ì—¬ GPUê°€ ì´ìš© ê°€ëŠ¥í•œ ê²½ìš° ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ GPUë¡œ ë³€í™˜í•˜ê²Œ í•œë‹¤. ê·¸ëƒ¥ì€ ì•”ê²ƒë„ ì•ˆí•˜ì§€ë§Œ CuArrays ë¥¼ ë¡œë”©(using CuArrays)í•œ ê²½ìš°ëŠ” ë°ì´í„°ë¥¼ GPUì— ì˜®ê²¨ì¤€ë‹¤.julia> using Flux, CuArrays\n\njulia> m = Dense(10,5) |> gpu\nDense(10, 5)\n\njulia> x = rand(10) |> gpu\n10-element CuArray{Float32,1}:\n 0.800225\n â‹®\n 0.511655\n\njulia> m(x)\nTracked 5-element CuArray{Float32,1}:\n -0.30535\n â‹®\n -0.618002ë¹„ìŠ·í•œ ìš©ë„ë¡œ cpuëŠ” ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ GPUì—ì„œ ê·¸ë§ŒëŒë¦¬ê²Œ í•œë‹¤.julia> x = rand(10) |> gpu\n10-element CuArray{Float32,1}:\n 0.235164\n â‹®\n 0.192538\n\njulia> x |> cpu\n10-element Array{Float32,1}:\n 0.235164\n â‹®\n 0.192538"
},

{
    "location": "Flux/saving/#",
    "page": "ì €ì¥ & ë¶ˆëŸ¬ì˜¤ê¸°",
    "title": "ì €ì¥ & ë¶ˆëŸ¬ì˜¤ê¸°",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/saving/#ëª¨ë¸ì„-ì €ì¥í•˜ê³ -ë¶ˆëŸ¬ì˜¤ê¸°-1",
    "page": "ì €ì¥ & ë¶ˆëŸ¬ì˜¤ê¸°",
    "title": "ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°",
    "category": "section",
    "text": "ëª¨ë¸ì„ ì €ì¥í•˜ê³ ëŠ” ì°¨í›„ì— ì´ë¥¼ ë¶ˆëŸ¬ë“¤ì—¬ ì‹¤í–‰í•˜ê³  ì‹¶ì€ê°€. ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ BSON.jl ì´ë‹¤.ëª¨ë¸ì„ ì €ì¥í•˜ì:julia> using Flux\n\njulia> model = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" modelë¶ˆëŸ¬ì˜¤ê¸°:julia> using Flux\n\njulia> using BSON: @load\n\njulia> @load \"mymodel.bson\" model\n\njulia> model\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)ëª¨ë¸ì€ ë³´í†µì˜ ì¤„ë¦¬ì•„ íƒ€ì…ì´ë‹¤. ë”°ë¼ì„œ ì¤„ë¦¬ì•„ ì €ì¥ í¬ë§·ì´ë©´ ì–´ëŠ ê²ƒì´ë¼ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. BSON.jlì€ íŠ¹íˆ ì˜ ì§€ì›í•˜ë©° ì•ìœ¼ë¡œë„ ë˜ë„ë¡ í˜¸í™˜ì„ ìœ ì§€í•œë‹¤ (ì§€ê¸ˆ ì €ì¥í•œ ëª¨ë¸ì´ Fluxì˜ ì°¨í›„ ë²„ì „ì—ì„œë„ ë¶ˆëŸ¬ë“¤ì¼ ìˆ˜ ìˆê²Œ).note: Note\nGPUì— ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ì˜€ìœ¼ë©´, GPU ì§€ì›ì´ ì•ˆë˜ëŠ” ê²½ìš°ì—ëŠ” ì´ë¥¼ ë¶ˆëŸ¬ ë“¤ì¼ ìˆ˜ ì—†ë‹¤. ì €ì¥í•˜ê¸° ì „ì— ëª¨ë¸ì„ CPUë¡œ ëŒë ¤ë†“ê¸° ì—ì„œì˜ cpu(model)ë¥¼ í•´ì£¼ëŠ”ê²Œ ê°€ì¥ ì¢‹ì€ ë°©ë²•ì´ë‹¤."
},

{
    "location": "Flux/saving/#ëª¨ë¸-ê°€ì¤‘ì¹˜-ì €ì¥í•˜ê¸°-1",
    "page": "ì €ì¥ & ë¶ˆëŸ¬ì˜¤ê¸°",
    "title": "ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥í•˜ê¸°",
    "category": "section",
    "text": "ì–´ë–¤ ê²½ìš°ëŠ” ì €ì¥ì€ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œ í•˜ê³  ì½”ë“œì—ì„œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì¬êµ¬ì„±í•˜ëŠ”ê²Œ ìœ ìš©í•œ ë°©ë²•ì¼ ìˆ˜ ìˆë‹¤. params(model)ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. data.(params)ì„ í•˜ë©´ ì¶”ì  ë‚´ì—­ ë°ì´í„°ë¥¼ ì§€ìš¸ ìˆ˜ ìˆë‹¤.julia> using Flux\n\njulia> model = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> weights = Tracker.data.(params(model));\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" weightsFlux.loadparams!ë¡œ ì‰½ê²Œ ëª¨ë¸ì— íŒŒë¼ë¯¸í„°ë¥¼ ë¶ˆëŸ¬ë“¤ì¼ ìˆ˜ ìˆë‹¤.julia> using Flux\n\njulia> model = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> using BSON: @load\n\njulia> @load \"mymodel.bson\" weights\n\njulia> Flux.loadparams!(model, weights)ìƒˆë¡œ ëœ¬ modelì€ ì „ì— íŒŒë¼ë¯¸í„° ì €ì¥í•œ ê²ƒê³¼ ì¼ì¹˜í•œë‹¤."
},

{
    "location": "Flux/saving/#ì²´í¬í¬ì¸íŒ…-1",
    "page": "ì €ì¥ & ë¶ˆëŸ¬ì˜¤ê¸°",
    "title": "ì²´í¬í¬ì¸íŒ…",
    "category": "section",
    "text": "ì¥ì‹œê°„ í›ˆë ¨ì— ìˆì–´ ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ê²ƒì€ ì°¸ ì¢‹ì€ ìƒê°ì´ë‹¤. ê·¸ëŸ¬ë©´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì–´ë„ (íŒŒì›Œê°€ ë‚˜ê°€ëŠ” ë“±ë“±ì˜ ì´ìœ ë¡œ) ë‹¤ì‹œ ì¬ê°œí•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” train!ì˜ ì»¬ë°± í•¨ìˆ˜ì—ì„œ ëª¨ë¸ì„ ì €ì¥í•˜ë©´ ëœë‹¤.using Flux: throttle\nusing BSON: @save\n\nm = Chain(Dense(10,5,relu),Dense(5,2),softmax)\n\nevalcb = throttle(30) do\n  # loss ë³´ê¸°\n  @save \"model-checkpoint.bson\" model\nendì´ëŸ¬ë©´ \"model-checkpoint.bson\" íŒŒì¼ì„ 30ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸ í•œë‹¤.í›ˆë ¨ì‹œí‚¤ëŠ” ë™ì•ˆì— ëª¨ë¸ì„ ì—°ë‹¬ì•„ ì €ì¥í•˜ëŠ” ê¹Œë¦¬í•œ ë°©ë²•ë„ ìˆëŠ”ë° ì˜ˆë¥¼ ë“¤ë©´@save \"model-$(now()).bson\" modelì´ë ‡ê²Œ í•˜ë©´ \"model-2018-03-06T02:57:10.41.bson\"ê³¼ ê°™ì´ ì—°ë‹¬ì•„ì„œ ëª¨ë¸ì´ ì €ì¥ëœë‹¤. í˜„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ lossë„ ì €ì¥í•  ìˆ˜ ìˆì–´ì„œ, ì˜¤ë²„í”¼íŒ… ì‹œì‘í•œë‹¤ ì‹¶ìœ¼ë©´ ì´ì „ ì‚¬ë³¸ì˜ ëª¨ë¸ë¡œ ë³µêµ¬ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë‹¤.@save \"model-$(now()).bson\" model loss = testloss()ëª¨ë¸ì˜ ìµœì í™” ìƒíƒœê¹Œì§€ë„ ì €ì¥í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ì •í™•í•˜ê²Œ ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì´ì–´ í›ˆë ¨ì„ ì¬ê°œí•  ìˆ˜ ìˆë‹¤.opt = ADAM(params(model))\n@save \"model-$(now()).bson\" model opt"
},

{
    "location": "Flux/community/#",
    "page": "ì»¤ë®¤ë‹ˆí‹°",
    "title": "ì»¤ë®¤ë‹ˆí‹°",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/community/#ì»¤ë®¤ë‹ˆí‹°-1",
    "page": "ì»¤ë®¤ë‹ˆí‹°",
    "title": "ì»¤ë®¤ë‹ˆí‹°",
    "category": "section",
    "text": "ëª¨ë“  Flux ì‚¬ìš©ìëŠ” ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬ì— ì ê·¹ í™˜ì˜í•œë‹¤. Julia í¬ëŸ¼, ìŠ¬ë™ (ì±„ë„ #machine-learning), ë˜ëŠ” Fluxì˜ Gitter ë¥¼ ì´ìš©í•˜ì. ì§ˆë¬¸ì´ë‚˜ ì´ìŠˆì— ëŒ€í•´ ë„ìš¸ ê²ƒì´ë‹¤.Fluxë¥¼ í•´í‚¹í•˜ëŠ”ë° ê´€ì‹¬ì´ ìˆìœ¼ë©´, ì†ŒìŠ¤ ì½”ë“œëŠ” ì—´ë ¤ ìˆê³  ì´í•´í•˜ê¸° ì‰½ë‹¤ â€“ ì¼ë°˜ì ì¸ ì¤„ë¦¬ì•„ ì½”ë“œë¡œ ë˜ì–´ ìˆë‹¤. intro ì´ìŠˆ ë¶€í„° ê´€ì‹¬ìˆê²Œ ì‚´í´ë³´ê³  ì‹œì‘í•´ ë³´ì.ğŸ¦‰ ë²ˆì—­ ì™„ë£Œ 2018-03-12"
},

{
    "location": "DataFlow/vertices/#",
    "page": "DataFlow ë²„í‹°ìŠ¤(vertices)",
    "title": "DataFlow ë²„í‹°ìŠ¤(vertices)",
    "category": "page",
    "text": "ğŸ¦‰ https://github.com/MikeInnes/DataFlow.jl/blob/master/docs/vertices.md ë²ˆì—­ë˜¥ ì¢€ ë§ì´ ëˆ„ê³  ë‚˜ì„œDataFlow provides two things, a graph data structure and a common syntax for describing graphs. You're not tied down to using either of these things; you could use the syntax and immediately convert graphs to an adjacency matrix for processing, for example, or you could generate the graphs through other means while taking advantage of DataFlow's library of common graph operations.DataFlow explicitly keeps the data structure very simple and doesn't try to attach any kind of meaning to it. The graphs could represent straightforward Julia programs, or Bayesian networks, or an electrical circuit. Libraries using DataFlow will probably want to extend the syntax and manipulate the graph in order to generate appropriate code for the application."
},

{
    "location": "DataFlow/vertices/#Data-Structures-1",
    "page": "DataFlow ë²„í‹°ìŠ¤(vertices)",
    "title": "Data Structures",
    "category": "section",
    "text": "DataFlow actually comes with two related data structures, the DVertex and the IVertex. Both represent nodes in a graph with inputs/outputs to/from other nodes in the graph. IVertex is input-linked, somewhat like a linked list â€“ it keeps a reference to nodes which serve as input. DVertex is doubly-linked, analogous to a doubly-linked list â€“ it refers to its input as well as all the nodes which take it as input. DVertex are technically more expressive but are also much harder to work with, so it's usually best to convert to input-linked as soon as possible (via DataFlow.il() for example).# src/graph/graph.jl\nabstract type Vertex{T} end\n\n# src/graph/ilgraph.jl\nstruct IVertex{T} <: Vertex{T}\n  value::T\n  inputs::Vector{IVertex{T}}\n  # outputs::Set{IVertex{T}} # DVertex has this in addition\nendIVertex can be seen as very similar to an Expr object in Julia. For example, the expression x+length(xs) will be stored in a very similar way:julia> using DataFlow\n\njulia> x = 2\n2\n\njulia> Expr(:call, :+, x, Expr(:call, :length, :xs))\n:(2 + length(xs))\n\njulia> IVertex(:+, IVertex(:x), IVertex(:length, IVertex(:xs)))\nIVertex{Symbol}(x() + length(xs()))The key difference is that object identity is important in DataFlow graphs. Say we build an expression tree like this:julia> foo = Expr(:call, :length, :xs)\n:(length(xs))\n\njulia> Expr(:call, :+, foo, foo)\n:(length(xs) + length(xs))This prints as length(xs)+length(xs) regardless of the fact that we reused the length(xs) expression object. In DataFlow the reuse makes a big difference:julia> g = IVertex{Any}\nIVertex\n\njulia> g(:+, g(:length, g(:xs)), g(:length, g(:xs)))\nIVertex(length(xs()) + length(xs()))\n\njulia> foo = g(:length, g(:xs))\nIVertex(length(xs()))\n\njulia> g(:+, foo, foo)\nIVertex(\neland = length(xs())\neland + eland)The reuse is now encoded in the program graph. Note that the data structure above has no conception of a \"variable\" since the flow of data is directly represented; instead, variables will be generated for us if and when they are needed in the syntax conversion."
},

{
    "location": "DataFlow/vertices/#Algorithms-1",
    "page": "DataFlow ë²„í‹°ìŠ¤(vertices)",
    "title": "Algorithms",
    "category": "section",
    "text": "The basic approach to working with DataFlow graphs is to use the same techniques as are used for trees in functional programming. That is, you can write algorithms which generate a new graph by recursively walking over the old one. This is packaged up in functions like prewalk and postwalk which allow you apply a function to each node in the graph.For example:julia> using DataFlow: postwalk, value\n\njulia> foo = g(:+, g(:length, g(:xs)), g(:length, g(:ys)))\nIVertex(length(xs()) + length(ys()))\n\njulia> postwalk(foo) do v\n         value(v) == :length && value(v[1]) == :xs ? g(:lx) : v\n       end\nIVertex(lx() + length(ys()))(The difference between pre- and postwalk is the order of traversal, which you can see using @show.) In this way you can do things like find and replace on graphs, as well as more complex structural transformations. At this point we also have everything we need to implement common subexpression elimination:julia> cse(v::IVertex, cache = Dict()) =\n         postwalk(x -> get!(cache, x, x), v)\ncse (generic function with 2 methods)We replace each node in the graph by retrieving it from a dict where values refer to themselves. This ensures that any values that are == will also be === in the resulting graph, so that common expressions are reused.julia> foo = @flow length(xs)+length(xs)\nIVertex(length(xs) + length(xs))\n\njulia> cse(foo)\nIVertex(\neland = length(xs)\neland + eland)Generally you should be able to stick to using DataFlow's high-level operations like postwalk, but in some cases you may need to write a recursive algorithm from scratch. This looks exactly like writing the same algorithm over a tree, with the caveats that (1) identical nodes may be reached by more than one route down the tree and (2) there may be cycles in the graph which cause infinite loops for naive recursion. This sounds like a nightmare but in fact we can kill these two tricky birds with a single stone; we simply memoize the function so that visiting repeated nodes ends the recursion. Make sure to cache the result of the current call before recursing.julia> using DataFlow: value, inputs, thread!\n\njulia> function replace_xs(g, cache = ObjectIdDict())\n         # Early exit if we've already processed this node\n         haskey(cache, g) && return cache[g]\n         # Create the new (empty) node and cache it\n         cache[g] = gâ€² = typeof(g)(value(g) == :xs ? :foo : value(g))\n         # For each input of the original node, process it and push\n         # the result into the new node\n         thread!(gâ€², (replace_xs(v, cache) for v in inputs(g))...)\n       end\nreplace_xs (generic function with 2 methods)\n\njulia> foo = DataFlow.cse(@flow length(xs)+length(xs))\nIVertex(\nalligator = length(xs)\nalligator + alligator)\n\njulia> replace_xs(foo)\nIVertex(\nalligator = length(xs)\nalligator + alligator)In this case forgetting the cache would result in a fairly un-disastrous length(foo)+length(foo), but in other cases it could result in a hang."
},

{
    "location": "MacroTools/README/#",
    "page": "MacroTools README",
    "title": "MacroTools README",
    "category": "page",
    "text": "ğŸ¦‰  https://github.com/MikeInnes/MacroTools.jl README.md ë²ˆì—­"
},

{
    "location": "MacroTools/README/#MacroTools.jl-1",
    "page": "MacroTools README",
    "title": "MacroTools.jl",
    "category": "section",
    "text": "This library provides helpful tools for writing macros, notably a very simple but powerful templating system and some functions that have proven useful to me (see utils.jl.)"
},

{
    "location": "MacroTools/README/#Template-Matching-1",
    "page": "MacroTools README",
    "title": "Template Matching",
    "category": "section",
    "text": "Template matching enables macro writers to deconstruct Julia expressions in a more declarative way, and without having to know in great detail how syntax is represented internally. For example, say you have a type definition:julia> ex = quote\n         struct Foo\n           x::Int\n           y\n         end\n       end\nquote  # REPL[1], line 2:\n    struct Foo # REPL[1], line 3:\n        x::Int # REPL[1], line 4:\n        y\n    end\nendIf you know what you're doing, you can pull out the name and fields via:julia> (ex.args[2].args[2], ex.args[2].args[3].args)\n(:Foo, Any[:( # REPL[2], line 3:), :(x::Int), :( # REPL[2], line 4:), :y])But this is hard to write â€“ since you have to deconstruct the type expression by hand â€“ and hard to read, since you can't tell at a glance what's being achieved. On top of that, there's a bunch of messy stuff to deal with like pesky begin blocks which wrap a single expression, line numbers, etc. etc.Enter MacroTools:julia> using MacroTools\n\njulia> @capture ex  struct T_ fields__ end\ntrue\n\njulia> T, fields\n(:Foo, Any[:(x::Int), :y])Symbols like T_ underscore are treated as catchalls which match any expression, and the expression they match is bound to the (underscore-less) variable, as above.Because @capture doubles as a test as well as extracting values, you can easily handle unexpected input (try writing this by hand):julia> @capture(ex, f_{T_}(xs__) = body_) ||\n         error(\"expected a function with a single type parameter\")\nERROR: expected a function with a single type parameter\nStacktrace:\n [1] error(::String) at ./error.jl:21Symbols like f__ (double underscored) are similar, but slurp a sequence of arguments into an array. For example:julia> @capture :[1, 2, 3, 4, 5, 6, 7]  [1, a_, 3, b__, c_]\ntrue\n\njulia> a, b, c\n(2, Any[4, 5, 6], 7)Slurps don't have to be at the end of an expression, but like the Highlander there can only be one (per expression)."
},

{
    "location": "MacroTools/README/#Matching-on-expression-type-1",
    "page": "MacroTools README",
    "title": "Matching on expression type",
    "category": "section",
    "text": "@capture can match expressions by their type, which is either the head of Expr objects or the typeof atomic stuff like Symbols and Ints. For example:julia> @capture :(foo(\"\"))  foo(x_String_string)\ntrue\n\njulia> @capture :(foo(\"$(a)\"))  foo(x_String_string)\ntrueThis will match a call to the foo function which has a single argument, which may either be a String object or a Expr(:string, ...) (e.g. @capture(:(foo(\"$(a)\")), foo(x_String_string))). Julia string literals may be parsed into either type of object, so this is a handy way to catch both.Another common use case is to catch symbol literals, e.g.julia> @capture ex  struct T_Symbol fields__ end\ntrue\n\njulia> T, fields\n(:Foo, Any[:(x::Int), :y])which will match e.g. struct Foo ... but not struct Foo{V} ...julia> @capture :(struct Foo{V} a end)  struct T_ fields__ end\ntrue\n\njulia> T\n:(Foo{V})\n\njulia> @capture :(struct Foo{V} a end)  struct T_Symbol fields__ end\nfalse"
},

{
    "location": "MacroTools/README/#Unions-1",
    "page": "MacroTools README",
    "title": "Unions",
    "category": "section",
    "text": "@capture can also try to match the expression against one pattern or another, for example:julia> @capture :(g() = 0)           (f_(args__) = body_) | function f_(args__) body_ end\ntrue\n\njulia> @capture :(function g() end)  (f_(args__) = body_) | function f_(args__) body_ end\ntruewill match both kinds of function syntax (though it's easier to use shortdef to normalise definitions). You can also do this within expressions, e.g.julia> @capture :(g() = 0)             f_(args__) | f_(args__) where T_ = body_\ntrue\n\njulia> T\n\njulia> @capture :(g(::T) where T = 0)  f_(args__) | f_(args__) where T_ = body_\ntrue\n\njulia> T\n:Tmatches a function definition, with a single type parameter bound to T if possible. If not, T = nothing."
},

{
    "location": "MacroTools/README/#Expression-Walking-1",
    "page": "MacroTools README",
    "title": "Expression Walking",
    "category": "section",
    "text": "If you've ever written any more interesting macros, you've probably found yourself writing recursive functions to work with nested Expr trees. MacroTools' prewalk and postwalk functions factor out the recursion, making macro code much more concise and robust.These expression-walking functions essentially provide a kind of find-and-replace for expression trees. For example:julia> using MacroTools: prewalk, postwalk\n\njulia> postwalk(x -> x isa Integer ? x + 1 : x, :(2+3))\n:(3 + 4)In other words, look at each item in the tree; if it's an integer, add one, if not, leave it alone.We can do more complex things if we combine this with @capture. For example, say we want to insert an extra argument into all function calls:julia> ex = quote\n           x = f(y, g(z))\n           return h(x)\n       end\nquote  # REPL[137], line 2:\n    x = f(y, g(z)) # REPL[137], line 3:\n    return h(x)\nend\n\njulia> postwalk(x -> @capture(x, f_(xs__)) ? :($f(5, $(xs...))) : x, ex)\nquote  # REPL[137], line 2:\n    x = f(5, y, g(5, z)) # REPL[137], line 3:\n    return h(5, x)\nendMost of the time, you can use postwalk without worrying about it, but we also provide prewalk. The difference is the order in which you see sub-expressions; postwalk sees the leaves of the Expr tree first and the whole expression last, while prewalk is the opposite.julia> postwalk(x -> @show(x) isa Integer ? x + 1 : x, :(2+3*4));\nx = :+\nx = 2\nx = :*\nx = 3\nx = 4\nx = :(4 * 5)\nx = :(3 + 4 * 5)\n\njulia> prewalk(x -> @show(x) isa Integer ? x + 1 : x, :(2+3*4));\nx = :(2 + 3 * 4)\nx = :+\nx = 2\nx = :(3 * 4)\nx = :*\nx = 3\nx = 4A significant difference is that prewalk will walk into whatever expression you return.julia> postwalk(x -> @show(x) isa Integer ? :(a+b) : x, 2)\nx = 2\n:(a + b)\n\njulia> prewalk(x -> @show(x) isa Integer ? :(a+b) : x, 2)\nx = 2\nx = :+\nx = :a\nx = :b\n:(a + b)This makes it somewhat more prone to infinite loops; for example, if we returned :(1+b) instead of :(a+b), prewalk would hang trying to expand all of the 1s in the expression.With these tools in hand, a useful general pattern for macros is:julia> macro foo(ex)\n           postwalk(ex) do x\n               @capture(x, a_*b_) || return x\n               return (a, b)\n           end\n       end\n@foo (macro with 1 method)\n\njulia> @foo 2\n2\n\njulia> @foo 2x\n(2, :x)\n\njulia> @foo 2x * 3\n((2, :x), 3)"
},

{
    "location": "MacroTools/README/#Function-definitions-1",
    "page": "MacroTools README",
    "title": "Function definitions",
    "category": "section",
    "text": "splitdef(def) matches a function definition of the formfunction name{params}(args; kwargs)::rtype where {whereparams}\n   body\nendand returns Dict(:name=>..., :args=>..., etc.). The definition can be rebuilt by calling MacroTools.combinedef(dict), or explicitly withjulia> dict = splitdef(:(f() = 0))\nDict{Symbol,Any} with 5 entries:\n  :name        => :f\n  :args        => Any[]\n  :kwargs      => Any[]\n  :body        => quote â€¦\n  :whereparams => ()\n\njulia> rtype = get(dict, :rtype, :Any)\n:Any\n\njulia> all_params = [get(dict, :params, [])..., get(dict, :whereparams, [])...]\n0-element Array{Any,1}\n\njulia> :(function $(dict[:name]){$(all_params...)}($(dict[:args]...);\n                                                   $(dict[:kwargs]...))::$rtype\n             $(dict[:body])\n         end)\n:(function f{}(; )::Any # REPL[83], line 3:\n        begin\n            0\n        end\n    end)splitarg(arg) matches function arguments (whether from a definition or a function call) such as x::Int=2 and returns (arg_name, arg_type, default). default is nothing when there is none. For example:julia> map(splitarg, (:(f(a=2, x::Int=nothing, y))).args[2:end])\n3-element Array{Tuple{Symbol,Symbol,Bool,Any},1}:\n (:a, :Any, false, 2)\n (:x, :Int, false, :nothing)\n (:y, :Any, false, nothing)"
},

{
    "location": "FluxJS/README/#",
    "page": "FluxJS README",
    "title": "FluxJS README",
    "category": "page",
    "text": "ğŸ¦‰ https://github.com/FluxML/FluxJS.jl README.md ë²ˆì—­"
},

{
    "location": "FluxJS/README/#Flux.JS-1",
    "page": "FluxJS README",
    "title": "Flux.JS",
    "category": "section",
    "text": "Run Flux models in the browser, via deeplearn.js."
},

{
    "location": "FluxJS/README/#JS-Output-1",
    "page": "FluxJS README",
    "title": "JS Output",
    "category": "section",
    "text": "You can see what Flux.JS sees with @code_js, which works like @code_typed or @code_native. Flux.JS simply accepts a function of arrays along with example inputs, and generates JavaScript code for you. Here's the simplest possible example:julia> using FluxJS\n\njulia> x = rand(10)\n10-element Array{Float64,1}:\n 0.882192\n 0.362089\n 0.784539\n 0.308894\n 0.869722\n 0.381076\n 0.00467682\n 0.0734247\n 0.55711\n 0.6387\n\njulia> @code_js identity(x)\nlet model = (function () {\n  let math = dl.ENV.math;\n  function model(jay) {\n    return jay;\n  };\n  model.weights = [];\n  return model;\n})();\nflux.fetchWeights(\"model.bson\").then((function (ws) {\n  return model.weights = ws;\n}));You can see that there's some setup code as Flux.JS expects to load some weights for a model. But the core of it is this function, which is exactly like the identity function in Julia.function model(kinkajou) {\n  return kinkajou;\n};Let's try something more interesting; f takes two arguments and multiplies them.julia> f(W,x) = W*x\nf (generic function with 1 method)\n\njulia> @code_js f(rand(5,10),rand(10))\nlet model = (function () {\n  let math = dl.ENV.math;\n  function model(jay, buffalo) {\n    return math.matrixTimesVector(jay, buffalo);\n  };\n  model.weights = [];\n  return model;\n})();\nflux.fetchWeights(\"model.bson\").then((function (ws) {\n  return model.weights = ws;\n}));Because Flux models are just Julia functions, we can use the same macro with them too. You'll now notice that the weights are being used.julia> using Flux\n\njulia> m = Chain(Dense(10,5,relu),Dense(5,2),softmax)\nChain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\n\njulia> @code_js m(x)\nlet model = (function () {\n  let math = dl.ENV.math;\n  function jay(buffalo) {\n    return math.add(math.matrixTimesVector(model.weights[0], buffalo), model.weights[1]);\n  };\n  function gnat(donkey) {\n    return math.relu(math.add(math.matrixTimesVector(model.weights[2], donkey), model.weights[3]));\n  };\n  function model(moose) {\n    return math.softmax(jay(gnat(moose)));\n  };\n  model.weights = [];\n  return model;\n})();\nflux.fetchWeights(\"model.bson\").then((function (ws) {\n  return model.weights = ws;\n}));There is also early support for RNNs (we compile stateful models directly, no unrolling).julia> m = Chain(RNN(10,5))\n\njulia> @code_js m(x)\nlet model = (function () {\n  let math = dl.ENV.math;\n  let init = [0.017732, 0.00991122, -0.00712077, -0.00161244, -0.00232475];\n  let states = init.slice();\n  function nightingale(seal, mongoose) {\n    return [seal, mongoose];\n  };\n  function cat(horse) {\n    let weasel = math.tanh(math.add(math.add(math.matrixTimesVector(model.weights[0], horse), math.matrixTimesVector(model.weights[1], states[0])), model.weights[2]));\n    let coati = nightingale(weasel, weasel);\n    states[0] = coati[1];\n    return coati[2];\n  };\n  function model(fish) {\n    return cat(fish);\n  };\n  model.reset = (function () {\n    states = init.slice();\n    return;\n  });\n  model.weights = [];\n  return model;\n})();\nflux.fetchWeights(\"model.bson\").then((function (ws) {\n  return model.weights = ws;\n}));In general, the more useful entry point to the package is FluxJS.compile.julia> FluxJS.compile(\"mnist\", m, rand(10))This will produce two files in the current directory: (1) mnist.js, which contains the same JavaScript code as above; (2) mnist.bson, which contains the model weights in a JS-loadable format."
},

{
    "location": "FluxJS/README/#Browser-Setup-1",
    "page": "FluxJS README",
    "title": "Browser Setup",
    "category": "section",
    "text": "Firstly, you'll need the following scripts in your <head>. The flux.js script can be found here.<head>\n  <script src=\"https://unpkg.com/deeplearn\"></script>\n  <script src=\"https://unpkg.com/bson/browser_build/bson.js\"></script>\n  <script src=\"flux.js\"></script> <!-- Or embed the script directly -->\n</head>From here, you can either link the generated code as another script, or embed it directly. In real applications you'll most likely want to wait on the fetchWeights promise, to avoid trying to use the model before it's ready.<script>\nlet model = (function () {\n  let math = dl.ENV.math;\n  function model(kinkajou) {\n    return kinkajou;\n  };\n  model.weights = [];\n  return model;\n})();\nflux.fetchWeights(\"model.bson\").then((function (ws) {\n  return model.weights = ws;\n}));\n</script>In the page, you can run the model from the dev tools.> x = dl.tensor([1,2,3,4,5,6,7,8,9,10])\n  TensorÂ {isDisposed: false, size: 10, shape: Array(1), dtype: \"float32\", strides: Array(0),Â â€¦}\n> await model(x).data()\n  Float32Array(25)Â [0.0262143611907959, -0.04852187633514404, â€¦]See the deeplearn.js docs for more information on how to work with its tensor objects."
},

{
    "location": "soc/guidelines/#",
    "page": "Application Guidelines",
    "title": "Application Guidelines",
    "category": "page",
    "text": "https://julialang.org/soc/guidelines/"
},

{
    "location": "soc/guidelines/#Before-the-Application-1",
    "page": "Application Guidelines",
    "title": "Before the Application",
    "category": "section",
    "text": "Before you apply, it's a good idea to get in touch with the Julia community. Ask questions on Discourse or join the Julia Slack to get connected with potential mentors to help find interesting projects. The Slack channel #jsoc is dedicated to helping students get the help they need. Additionally, you should make use of these connections to start making some small contributions and progress on your project early on. While PRs before the applications are not required, GSoC is extremely competitive, so the more ways you have to show your commitment, the better."
},

{
    "location": "soc/guidelines/#Application-Instructions-1",
    "page": "Application Guidelines",
    "title": "Application Instructions",
    "category": "section",
    "text": "GSoC applications must be submitted to https://summerofcode.withgoogle.com. We recommend having a mentor to help you with the application process. Please feel free to share your draft applications in the #jsoc channel of the Slack to receive feedback.Our organization does not have page or formatting requirements, but we recommend building a formal PDF document of less than 10 pages which is formatted using Word or LaTeX. If you have any questions, contact the Julia GSoC administrators at summerofcode@julialang.org."
},

{
    "location": "soc/guidelines/#Application-Guidelines-1",
    "page": "Application Guidelines",
    "title": "Application Guidelines",
    "category": "section",
    "text": "Applications are free-form, so you can discuss your project in whatever way you feel is best. The key questions we will ask ourselves when considering it are:Is the student committed to the project?\nIs the plan a reasonable amount to do in three months?\nAre there clear milestones we can use to assess progress (it should be easy to answer the question \"Is the project done?\").In more detail, you may find it useful to consider the following questions:The Project\nWhat do you want to have completed by the end of the program?\nFor example, \"a package for doing X which any Julia user can install\" or \"an extra feature for Foo.jl that does Y\"\nWho's interested in the work, and how will it benefit them?\nFor example, \"bioinformaticians will be able to set up sequencing pipelines flexibly in pure Julia\"\nIt's important to justify the project for people who may not be experts in your subject area.\nWhat are the potential hurdles you might encounter, and how can you resolve them?\nIs there anything you need to learn about as part of the work? Does your work depend on anyone else's to make progress?\nHow will you prioritize different aspects of the project like features, API usability, documentation, and robustness?\nWhat milestones can you target throughout the period?\nFor example, getting a working prototype out to beta testers by the halfway point\nAre there any stretch goals you can make if the main project goes smoothly? Tell us how you're going to wow us with the final result!\nCode portfolio. Show us a sample or two of code that you're proud of. Itdoesn't have to be Julia (but that doesn't hurt either). You don't need to be a star programmer as long as you can demonstrate interest in and commitment to your project.Deliverables List what concrete products expect to deliver by the end ofthe projectAbout you. Why you? Give us a sense of who you are as a person and as a programmer.\nWhat academic, professional or hobby programming experience do you have, and how will it help you with your project?\nHave you contributed to open source projects before? (Link us to some issues and patches, if any)\nWhy are you interested in Julia? Have you used it much before? You need to demonstrate your ability to use Julia by the beginning of the GSoC. GSoC is not for learning the Julia language, though extensive prior experience is not required.\nDo you have the mathematical/scientific background for your project? Many of the Julialang projects have a significant portion that require technical expertise and applicants need to demonstrate their ability to handle the chosen project.\nHow should we contact you? Let us know your email address and GitHub username.\nDo you have a website or blog?\nAnything else you'd like to mention!\nLogistics.\nWhat other time commitments, such as summer courses, other jobs, planned vacations, etc., will you have over the summer?"
},

{
    "location": "soc/projects/ml/#",
    "page": "Data Science & Machine Learning",
    "title": "Data Science & Machine Learning",
    "category": "page",
    "text": "https://julialang.org/soc/projects/ml.htmlNote that for any of these projects you should have code samples as part of your application, ideally as patches to one of the ML or GPU libraries or in the form of working ML models."
},

{
    "location": "soc/projects/ml/#Model-Zoo-Examples-1",
    "page": "Data Science & Machine Learning",
    "title": "Model Zoo Examples",
    "category": "section",
    "text": "Flux's model zoo contains examples of a wide range of deep learning models and techniques. This project would involve adding new models, showing how to recreate state-of-the-art results (e.g. AlphaGo) or interesting and unusual model architectures (e.g. transformer networks). We'd be particularly interested in any models involving reinforcement learning, or anything with images, sound or speech.Some experience with implementing deep learning models would be ideal for this project, but is not essential for a student willing to pick up the skills and read ML papers. It's up to you whether you implement a single ambitious model, or multiple small ones.Mentors: Mike Innes"
},

{
    "location": "soc/projects/ml/#Flux.JS-demos-1",
    "page": "Data Science & Machine Learning",
    "title": "Flux.JS demos",
    "category": "section",
    "text": "Flux.JS enables export of Flux models to the browser. However, just porting a numerical function to JavaScript is rarely exciting on its own; you need to build an interface to give input to the model (say, via a webcam) and see output (say, by displaying an annotated image) in order to see what the model is thinking.This project would involve creating new demos that show interesting models running in the browser. Examples could include:An MNIST digit classify running on hand-drawn images;\nHandwriting generation;\nLive speech recognition or production;\nAn autoencoder that allows moving sliders to generate images, and explore \"digit space\";\nA Chess or Go AI that plays against the user;\nA language analysis tool that classifies user-written text.The possibilities are pretty much endless here. This project will require a pretty solid handle on web technologies, and we'd expect much of the components created to be reusable between demos.Mentors: Mike Innes, Shashi Gowda."
},

{
    "location": "soc/projects/ml/#Model-Import-and-Export-1",
    "page": "Data Science & Machine Learning",
    "title": "Model Import and Export",
    "category": "section",
    "text": "Sharing models with other frameworks would enables us to both export models (say to JavaScript for the browser, or TensorFlow Lite for mobile, or NNVM for optimised training) and to take advantage of the large set of trained models in the wild in Julia code.This involves several stages, some or all of which could be tackled over the course of a project.Reading and writing the raw model formats. For formats like ONNX this should be relatively easy, as one can use the ProtoBuf.jl library.\nFor model import:\nConverting the raw model format to a more general graph format, such as a DataFlow.jl graph.\nDumping the model graph as Julia code.\nFor model export:\nConverting a general graph format to the raw model constructs.\nTracing Julia code to produce a dataflow graph, as in FluxJS.jl.Mentors: Mike Innes"
},

{
    "location": "soc/projects/ml/#Benchmarks-1",
    "page": "Data Science & Machine Learning",
    "title": "Benchmarks",
    "category": "section",
    "text": "A benchmark suite would help us to keep Julia's performance for ML models in shape, as well as revealing opportunities for improvement. Like the model-zoo project, this would involve contributing standard models that exercise common ML use case (images, text etc) and profiles them. The project could extend to include improving performance where possible, or creating a \"benchmarking CI\" like Julia's own nanosoldier.Mentors: Mike Innes"
},

{
    "location": "soc/projects/ml/#Compiler-Optimisations-1",
    "page": "Data Science & Machine Learning",
    "title": "Compiler Optimisations",
    "category": "section",
    "text": "Julia opens up many interesting opportunities for applying new optimisations to ML models, and exploring language design for ML. As part of this project you'd help us apply novel optimisation strategies to Julia code, with immediate benefits to Flux and other Julia users.Possible projects could include:Auto-parallelisation and vectorisation in the vain of DyNet autobatch, TensorFlow Fold and Matchbox.\nUsing Cassette and techniques similar to Flux.JS to extract dataflow computation graphs from imperative Julia code.\nApplying optimisations to computation graphs, such as eliding memory allocations, reusing memory and fusing operations, or enabling model parallelism.\nApplying Halide or Futhark-like optimisations to array expressions, as in Tokamak\nImproving Julia's GPU support, including tuning memory management and supporting CUDA streams.Mentors: Mike Innes"
},

{
    "location": "soc/projects/ml/#Sparse-GPU-and-ML-support-1",
    "page": "Data Science & Machine Learning",
    "title": "Sparse GPU and ML support",
    "category": "section",
    "text": "While Julia supports dense GPU arrays well via CuArrays, we lack up-to-date wrappers for sparse operations. This project would involve wrapping CUDA's sparse support, with CUSPARSE.jl as a starting point, adding them to CuArrays.jl, and perhaps demonstrating their use via a sparse machine learning model.Mentors: Mike Innes"
},

{
    "location": "soc/projects/ml/#Parquet.jl-enhancements-1",
    "page": "Data Science & Machine Learning",
    "title": "Parquet.jl enhancements",
    "category": "section",
    "text": "Efficient storage of tabular data is an important component of the data analysis story in the ecosystem. Julia has many options here â€“ JLD, JuliaDBâ€™s built-in serialization, CSV.write. These either suffer from lack of performance or lack of standardization. Parquet is a format for efficient storage of tabular data used in the Hadoop world. It has compression techniques which reduce disk usage as well as speed up reads. A well-rounded Parquet implementation in Julia will solve the current issues with storage formats and let Julia interoperate with software from the Hadoop world.Parquet.jl currently contains a reader for Parquet files. This project involves implementing the writer for Parquet files, as well as some enhancements to the reading functionality.Deliverables:Reader enhancements:Read a file as a NamedTuple of vectors (using NamedTuples.jl on Julia 0.6). This is on similar lines, but different from the current cursor-based reader. Probably as an implementation of AbstractBuilder that returns NamedTuple of column vectors, combined with a new iterator/cursor that returns a bunch of records instead of individual records.Writer support:Write a table (in the form of a NamedTuple of vectors) to disk. Note: we will use NamedTuple of vectors as a minimal table which can be converted back into DataFrames or IndexedTables\nImplement the compression features provided in the Parquet spec-Optionally auto detect compression scheme based on the data.Mentors: Tanmay Mohapatra"
},

{
    "location": "soc/projects/ml/#GPU-support-in-JuliaDB-1",
    "page": "Data Science & Machine Learning",
    "title": "GPU support in JuliaDB",
    "category": "section",
    "text": "JuliaDB is a distributed analytical database. It uses Juliaâ€™s multi-processing for parallelism at the moment. GPU implementations of some operations may allow relational algebra with low latency. In this project, you will be required to add basic GPU support in JuliaDB.Copy a table to GPU â€“ this may be as simple as converting every column into a CuArray or GPUArray\nmap, reduce and filter operation â€“ apply simple functions on a large table that is on the GPU\nEnsure that columnar storage format is made use of in the lower level code generated.\nThe groupby and join operations may involve first implementing an efficient sortperm that utilize the GPU, or an efficient hash table on the GPU\ngroupby kernel on GPU\njoin kernel on GPU (stretch goal)Mentors: Shashi Gowda, Mike Innes"
},

{
    "location": "soc/projects/ml/#Making-Aquiring-Open-Data-Easy-1",
    "page": "Data Science & Machine Learning",
    "title": "Making Aquiring Open-Data Easy",
    "category": "section",
    "text": "Goverments and Universities are releasing huge amounts of data under Open Data policies. Web portals such as:http://data.gov\nhttp://data.gov.au\nhttps://dataverse.harvard.edu/\nhttp://datadryad.org/\nhttps://figshare.com/Expose great quanities of data just wating to be used.DataDeps.jl is a package that helps data scientists ensures that anyone running their code has all the data it needs, no matter when or where it is run. To do this it needs a registration block, which is a chunk of julia code which says where the data can be download, who created it, what terms and conditions are on its use etc. For a simple dataset that is all in one file writing this is pretty easy â€“ copy and paste the info from the website hosting the data. When you want to dozens of datasets, some of which have dozens of files (and no easy way to download a .zip of all of them), writing this registration block is a bit more work.DataDepsGenerators.jl exists to solve that. Give it a URL (or other identifier) for a page describing a dataset, and outputs all the code for a registration block, that you can copy and paste straight into your julia project. Right now DataDepsGenerators only supports a couple of sites: GitHub (for https://github.com/BuzzFeedNews/ and https://github.com/fivethirtyeight/data/ and others) and the UCI ML Repository. This project aims to change that by adding support for the CKAN and the OA-PMH APIs.The CKAN and the OA-PMH APIs allow the automated extraction of metadata for a dataset. They are primarily used by goverment \"data.gov.*\" sites and research repositories respectively. Together they host millions of datasets, furfilling those institutions open data policies.This project is to leverage those APIs, to allow others to leaverage those data repositories to produce easily repeatable, data driven research.Expected Results: a series of patches to DataDepsGenerators.jl, giving it the capacity to generate a DataDeps registration block for any dataset hosted on site exposing a CRAN, or OAI-PMH API.Recommended Skills: Familarity with web APIs and related technolgies (e.g. REST, JSON, XML (Probably not OAUTH, but if you've done OAUTH then your more than familar enough)). Some practice with webscraping is likely to be useful. A love of data and of doing cool things with it, is a big plus.Mentors: Lyndon White (oxinabox)"
},

{
    "location": "soc/projects/ml/#Parameter-estimation-for-nonlinear-dynamical-models-1",
    "page": "Data Science & Machine Learning",
    "title": "Parameter estimation for nonlinear dynamical models",
    "category": "section",
    "text": "Machine learning has become a popular tool for understanding data, but scientists typically understand the world through the lens of physical laws and their resulting dynamical models. These models are generally differential equations given by physical first principles, where the constants in the equations such as chemical reaction rates and planetary masses determine the overall dynamics. The inverse problem to simulation, known as parameter estimation, is the process of utilizing data to determine these model parameters.The purpose of this project is to utilize the growing array of statistical, optimization, and machine learning tools in the Julia ecosystem to build library functions that make it easy for scientists to perform this parameter estimation with the most high-powered and robust methodologies. Possible projects include investigating methods for Bayesian estimation of parameters via Stan.jl and Julia-based libraries like Turing.jl, or global optimization-based approaches. Novel techniques like classifying model outcomes via support vector machines and deep neural networks is can also be considered. Research and benchmarking to attempt to find the most robust methods will take place in this project.Some work in this area can be found in DiffEqParamEstim.jl and DiffEqBayes.jl. Examples can be found in the DifferentialEquations.jl documentation.Recommended Skills: Background knowledge of standard machine learning, statistical, or optimization techniques. It's recommended but not required that one has basic knowledge of differential equations and DifferentialEquations.jl. Using the differential equation solver to get outputs from parameters can be learned on the job, but you should already be familiar (but not necessarily an expert) with the estimation techniques you are looking to employ.Expected Results: Library functions for performing parameter estimation and inferring properties of differential equation solutions from parameters. Notebooks containing benchmarks determining the effectiveness of various methods and classifying when specific approaches are appropriate will be developed simultaneously.Mentors: Chris Rackauckas"
},

{
    "location": "soc/projects/ml/#Artificial-Intelligence-Library-Package-based-on-Artificial-Intelligence-A-Modern-Approach-(AIMA)-1",
    "page": "Data Science & Machine Learning",
    "title": "Artificial Intelligence Library Package based on Artificial Intelligence - A Modern Approach (AIMA)",
    "category": "section",
    "text": "AIMA is a seminal text on representation of agents to solve AI problems. Most packages  available today as AI libraries tend to focus on ML only and not the architectural aspect of AI. The scope of the project is to create a library with a clean API  (following AIMA) to easily allow the application of core algorithms to AI problems.  The student will implement a package that brings together implementations of algorithms  like depth-first search and simulated annealing, both from other Julia packages and  from sample code in the AIMA book, and build sample programs to demonstrate AI  applications. Starter code can be found at AIMACore along with AIMASamples.Recommended Skills: Previous experience with AI or the ability to quickly pick up on  the AI algorithms in AIMAExpected Results: A well-documented library of functions derived from the AIMA book.Mentors Sambit Kumar Dash. "
},

]}
