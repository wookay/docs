var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "Home",
    "title": "Home",
    "category": "page",
    "text": ""
},

{
    "location": "#-1",
    "page": "Home",
    "title": "ðŸ¦‰",
    "category": "section",
    "text": "ë°¥ ë¨¹ê³  ë˜¥ ì‹¸ëŠ” ê³³ìž…ë‹ˆë‹¤ "
},

{
    "location": "Flux/#",
    "page": "Flux í™ˆ",
    "title": "Flux í™ˆ",
    "category": "page",
    "text": "https://github.com/FluxML/Flux.jl ìžë£Œë¥¼ ë²ˆì—­í•˜ëŠ” ê³³ìž…ë‹ˆë‹¹"
},

{
    "location": "Flux/#Flux:-ì¤„ë¦¬ì•„-ë¨¸ì‹ -ëŸ¬ë‹-ë¼ì´ë¸ŒëŸ¬ë¦¬-1",
    "page": "Flux í™ˆ",
    "title": "Flux: ì¤„ë¦¬ì•„ ë¨¸ì‹  ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬",
    "category": "section",
    "text": "FluxëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬. \"ë°°í„°ë¦¬-í¬í•¨(batteries-included, ì œí’ˆì˜ ì™„ì „í•œ ìœ ìš©ì„±ì„ ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ë¶€í’ˆì„ í•¨ê»˜ ì œê³µí•œë‹¤ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ìª½ ìš©ì–´)\" ë§Žì€ ìœ ìš©í•œ ë„êµ¬ë¥¼ ì œê³µí•œë‹¤. ì¤„ë¦¬ì•„ ì–¸ì–´ë¥¼ í’€íŒŒì›Œ(full power)ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤. ì „ì²´ ìŠ¤íƒì„ ì¤„ë¦¬ì•„ ì½”ë“œë¡œ êµ¬í˜„í•œë‹¤. GPU ì»¤ë„ë„ ê°€ëŠ¥í•˜ê³ , ê°œë³„ íŒŒíŠ¸ë¥¼ ê°œì¸ ì·¨í–¥ì— ë§žê²Œ ì¡°ìž‘í•  ìˆ˜ ìžˆë‹¤."
},

{
    "location": "Flux/#ì„¤ì¹˜í•˜ê¸°-1",
    "page": "Flux í™ˆ",
    "title": "ì„¤ì¹˜í•˜ê¸°",
    "category": "section",
    "text": "ì¤„ë¦¬ì•„ 0.6.0 ì´ìƒ, ì•„ì§ ì•ˆê¹”ì•˜ìœ¼ë©´ ì„¤ì¹˜í•˜ìž.Pkg.add(\"Flux\")\n# ì„ íƒì¸ë° ì¶”ì²œ\nPkg.update() # íŒ¨í‚¤ì§€ë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ëŽƒ\nPkg.test(\"Flux\") # ì„¤ì¹˜ ë˜‘ë°”ë¡œ ëœê±´ê°€ í™•ì¸í•˜ê¸°ê¸°ë³¸ì ì¸ ê²ƒ ë¶€í„° ì‹œìž‘í•˜ìž. ë™ë¬¼ì› ëª¨ë¸(model zoo)ì€ ì—¬ëŸ¬ê°€ì§€ ê³µí†µ ëª¨ë¸ì„ ë‹¤ë£¨ëŠ”ë° ê·¸ê±¸ë¡œ ì‹œìž‘í•´ë„ ì¢‹ë‹¤."
},

{
    "location": "Flux/models/basics/#",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/basics/#ëª¨ë¸-ë§Œë“¤ê¸°-ê¸°ì´ˆ-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ëª¨ë¸ ë§Œë“¤ê¸° ê¸°ì´ˆ",
    "category": "section",
    "text": ""
},

{
    "location": "Flux/models/basics/#ê¸°ìš¸ê¸°(Gradients,-ê²½ì‚¬)-êµ¬í•˜ê¸°-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ê¸°ìš¸ê¸°(Gradients, ê²½ì‚¬) êµ¬í•˜ê¸°",
    "category": "section",
    "text": "ê°„ë‹¨í•œ linear regression(ë¦¬ë‹ˆì–´ ë¦¬ê·¸ë ˆì…˜, ì§ì„  ëª¨ì–‘ìœ¼ë¡œ ê·¸ë ¤ì§€ëŠ” í•¨ìˆ˜)ì„ ìƒê°í•´ ë³´ìž. ì´ê²ƒì€ ìž…ë ¥ xì— ëŒ€í•œ ì¶œë ¥ ë°°ì—´ yë¥¼ ì˜ˆì¸¡í•œë‹¤. (ì¤„ë¦¬ì•„ REPLì—ì„œ ì˜ˆì œë¥¼ ë”°ë¼í•´ë³´ë©´ ì¢‹ë‹¤)julia> W = rand(2, 5)\n2Ã—5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = rand(2)\n2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> predict(x) = W*x .+ b\npredict (generic function with 1 method)\n\njulia> loss(x, y) = sum((predict(x) .- y).^2)\nloss (generic function with 1 method)\n\njulia> x, y = rand(5), rand(2) # ë”ë¯¸ ë°ì´í„°\n([0.496864, 0.947507, 0.874288, 0.251528, 0.192234], [0.901991, 0.0802404])\n\njulia> loss(x, y) # ~ 3\n3.1660692660286722ì˜ˆì¸¡ì„ ë” ìž˜í•˜ê¸° ìœ„í•´ Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ìž. loss function(ì†ì‹¤, ì˜ˆì¸¡ ì‹¤íŒ¨ í•¨ìˆ˜)ê³¼ gradient descent(ê²½ì‚¬ í•˜ê°•, ë‚´ë¦¬ë§‰ ê¸°ìš¸ê¸°)ë¥¼ í•´ë³´ë©´ì„œ. ì§ì ‘ ì†ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ë„ ìžˆì§€ë§Œ Fluxì—ì„œëŠ” Wì™€ bë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„°(parameters)ë¡œ ë‘˜ ìˆ˜ ìžˆë‹¤.julia> using Flux.Tracker\n\njulia> W = param(W)\nTracked 2Ã—5 Array{Float64,2}:\n 0.857747   0.291713  0.179873  0.938979  0.51022\n 0.0852085  0.977716  0.246164  0.460672  0.772312\n\njulia> b = param(b)\nTracked 2-element Array{Float64,1}:\n 0.663369\n 0.132996\n\njulia> l = loss(x, y)\n3.1660692660286722 (tracked)\n\njulia> back!(l)\nloss(x, y)ëŠ” ë°©ê¸ˆ ì „ê³¼ ê°™ì€ ìˆ˜(3.1660692660286722)ë¥¼ ë¦¬í„´, ê·¸ëŸ°ë° ì´ì œë¶€í„°ëŠ” ê¸°ìš¸ì–´ì§€ëŠ” ëª¨ì–‘ì„ ê´€ì°° ê¸°ë¡í•˜ì—¬ ê°’ì„ ì¶”ì (tracked)  í•œë‹¤. back!ì„ í˜¸ì¶œí•˜ë©´ Wì™€ bì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤. ê¸°ìš¸ê¸°ê°€ ë­”ì§€ ì•Œì•„ëƒˆìœ¼ë‹ˆ Wë¥¼ ê³ ì³ê°€ë©´ì„œ ëª¨ë¸ì„ í›ˆë ¨í•˜ìž.julia> W.grad\n2Ã—5 Array{Float64,2}:\n 0.949491  1.81066  1.67074  0.480662  0.367352\n 1.49163   2.84449  2.62468  0.755107  0.577101\n\njulia> # íŒŒë¼ë¯¸í„° ì—…ëŽƒ\n       W.data .-= 0.1(W.grad)\n2Ã—5 Array{Float64,2}:\n  0.762798   0.110647   0.0127989  0.890913  0.473484\n -0.0639541  0.693267  -0.0163046  0.385161  0.714602\n\njulia> loss(x, y) # ~ 2.5\n1.1327711929294395 (tracked)ì˜ˆì¸¡ ì‹¤íŒ¨(loss)ê°€ ì¡°ê¸ˆ ì¤„ì–´ë“¤ì—ˆë‹¤. x ì˜ˆì¸¡ì´ ëª©í‘œ íƒ€ê²Ÿ yì— ì¢€ ë” ê°€ê¹Œì›Œì¡Œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë°ì´í„°ê°€ ìžˆìœ¼ë©´ ëª¨ë¸ í›ˆë ¨í•˜ê¸°ë„ ì‹œë„í•  ìˆ˜ ìžˆë‹¤.ë³µìž¡í•œ ë”¥ëŸ¬ë‹ì´ Fluxì—ì„œëŠ” ì´ì™€ ê°™ì€ ì˜ˆì œì²˜ëŸ¼ ë‹¨ìˆœí•´ì§„ë‹¤. ë¬¼ë¡  ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°¯ìˆ˜ê°€ ë°±ë§Œê°œê°€ ë„˜ì–´ê°€ê³  ë³µìž¡í•œ ì œì–´ íë¦„ì„ ê°–ê²Œ ë˜ë©´ ë‹¤ë¥¸ ëª¨ì–‘ì„ ê°–ê² ì§€. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë³µìž¡ì„±ì„ ë‹¤ë£¨ëŠ” ê²ƒì—ëŠ” ë­ê°€ ìžˆëŠ”ì§€ í•œë²ˆ ì‚´íŽ´ë³´ìž."
},

{
    "location": "Flux/models/basics/#ë ˆì´ì–´-ë§Œë“¤ê¸°-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ë ˆì´ì–´ ë§Œë“¤ê¸°",
    "category": "section",
    "text": "ì´ì œë¶€í„°ëŠ” linear regression ë³´ë‹¤ ë³µìž¡í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ìž. ì˜ˆë¥¼ ë“¤ì–´, ë‘ ê°œì˜ linear ë ˆì´ì–´ ì‚¬ì´ì— ì‹œê·¸ëª¨ì´ë“œ (Ïƒ) ì²˜ëŸ¼ nonlinearity(ë¹„ì„ í˜•, ì»¤ë¸Œì²˜ëŸ¼ ì§ì„ ì´ ì•„ë‹Œ ê±°)ë¥¼ ê°–ëŠ” ë„˜ì´ ìžˆì„ë•Œ, ìœ„ì˜ ìŠ¤íƒ€ì¼ì€ ì•„ëž˜ì™€ ê°™ì´ ì“¸ ìˆ˜ ìžˆë‹¤:julia> using Flux\n\njulia> W1 = param(rand(3, 5))\nTracked 3Ã—5 Array{Float64,2}:\n 0.540422  0.680087  0.743124  0.0216563  0.377793\n 0.416939  0.51823   0.464998  0.419852   0.446143\n 0.260294  0.392582  0.46784   0.549495   0.373124\n\njulia> b1 = param(rand(3))\nTracked 3-element Array{Float64,1}:\n 0.213799\n 0.373862\n 0.243417\n\njulia> layer1(x) = W1 * x .+ b1\nlayer1 (generic function with 1 method)\n\njulia> W2 = param(rand(2, 3))\nTracked 2Ã—3 Array{Float64,2}:\n 0.789744  0.389376  0.172613\n 0.472963  0.21518   0.220236\n\njulia> b2 = param(rand(2))\nTracked 2-element Array{Float64,1}:\n 0.121207\n 0.502486\n\njulia> layer2(x) = W2 * x .+ b2\nlayer2 (generic function with 1 method)\n\njulia> model(x) = layer2(Ïƒ.(layer1(x)))\nmodel (generic function with 1 method)\n\njulia> model(rand(5)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 1.06727\n 1.13835ìž‘ë™ì€ í•˜ëŠ”ë° ì¤‘ë³µ ìž‘ì—…ì´ ë§Žì•„ ë³´ê¸°ì— ì¢‹ì§€ ì•Šë‹¤ - íŠ¹ížˆ ë ˆì´ì–´ë¥¼ ë” ì¶”ê°€í•œë‹¤ë©´. linear ë ˆì´ì–´ë¥¼ ëŒë ¤ì£¼ëŠ” í•¨ìˆ˜ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì´ê²ƒë“¤ì„ ì •ë¦¬í•˜ìž.julia> function linear(in, out)\n         W = param(randn(out, in))\n         b = param(randn(out))\n         x -> W * x .+ b\n       end\nlinear (generic function with 1 method)\n\njulia> linear1 = linear(5, 3) # linear1.W í•  ìˆ˜ ìžˆë‹¥ (ìµëª…í•¨ìˆ˜ ë¦¬í„´)\n(::#3) (generic function with 1 method)\n\njulia> linear1.W\nTracked 3Ã—5 Array{Float64,2}:\n -1.72011   -1.07297   0.396755  -0.117604   0.25952\n -0.16694    0.99327  -0.589717  -1.87123    0.141679\n -0.972281  -1.84836   2.55071   -0.136674  -0.147826\n\njulia> linear2 = linear(3, 2)\n(::#3) (generic function with 1 method)\n\njulia> model(x) = linear2(Ïƒ.(linear1(x)))\nmodel (generic function with 1 method)\n\njulia> model(x) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 2.75582\n 0.416809ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” structë¡œ íƒ€ìž…ì„ ë§Œë“¤ì–´ì„œ affine(ì–´íŒŒì¸) ë ˆì´ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ìžˆë‹¤.julia> struct Affine\n         W\n         b\n       end\n\njulia> Affine(in::Integer, out::Integer) =\n         Affine(param(randn(out, in)), param(randn(out)))\nAffine\n\njulia> # ì˜¤ë²„ë¡œë“œ í•˜ë©´ ê°ì²´ë¥¼ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí•  ìˆ˜ ìžˆë‹¤\n       (m::Affine)(x) = m.W * x .+ m.b\n\njulia> a = Affine(10, 5)\nAffine(param([0.0252182 -1.99122 â€¦ -0.191235 0.294728; 1.13559 1.50226 â€¦ -2.43917 0.56976; â€¦ ; -0.735177 0.202646 â€¦ -0.301945 -0.183598; 1.05967 0.986786 â€¦ -1.57835 -0.0893871]), param([-0.39419, -1.26818, 0.757665, 0.941398, -0.783242]))\n\njulia> a(rand(10)) # => 5-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 5-element Array{Float64,1}:\n -0.945544\n -0.575674\n  2.93741\n  0.111253\n -0.843172ì¶•í•˜í•©ë‹ˆë‹¤! Fluxì—ì„œ ë‚˜ì˜¤ëŠ” Dense ë ˆì´ì–´ ë§Œë“¤ê¸° ì„±ê³µ! FluxëŠ” ë§Žì€ ìž¬ë°ŒëŠ” ë ˆì´ì–´ë“¤ì´ ìžˆëŠ”ë°, ê·¸ê²ƒë“¤ì„ ì§ì ‘ ë§Œë“œëŠ” ê²ƒ ë˜í•œ ì •ë§ ì‰½ë‹¤.(Denseì™€ ë‹¤ë¥¸ í•œê°€ì§€ - íŽ¸ì˜ë¥¼ ìœ„í•´ activation(í™œì„±) í•¨ìˆ˜ë¥¼ ë’¤ì— ì¶”ê°€í•  ìˆ˜ë„ ìžˆë‹¤. Dense(10, 5, Ïƒ) ìš”ëŸ°ì‹ìœ¼ë¡œ.)"
},

{
    "location": "Flux/models/basics/#ì´ì˜ê²Œ-ìŒ“ì•„ë³´ìž-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ì´ì˜ê²Œ ìŒ“ì•„ë³´ìž",
    "category": "section",
    "text": "ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì€ í”í•˜ë‹¤: (layer1 ì´ë¦„ì´ ê²¹ì¹˜ë‹ˆ REPLì„ ìƒˆë¡œ ë„ìš°ìž)julia> using Flux\n\njulia> layer1 = Dense(10, 5, Ïƒ)\nDense(10, 5, NNlib.Ïƒ)\n\njulia> # ...\n       model(x) = layer3(layer2(layer1(x)))\nmodel (generic function with 1 method)ê¸°ë‹¤ëž—ê²Œ ì—°ê²°(chains) í• ë¼ë¯„, ë‹¤ìŒê³¼ ê°™ì´ ë ˆì´ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ”ê²Œ ì¢€ ë” ì§ê´€ì ì´ë‹¤:julia> layers = [Dense(10, 5, Ïƒ), Dense(5, 2), softmax]\n3-element Array{Any,1}:\n Dense(10, 5, NNlib.Ïƒ)\n Dense(5, 2)\n NNlib.softmax\n\njulia> model(x) = foldl((x, m) -> m(x), x, layers)\nmodel (generic function with 1 method)\n\njulia> model(rand(10)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 0.593021\n 0.406979íŽ¸ë¦¬í•˜ê²Œ ì“°ë¼ê³  ì´ê²ƒ ì—­ì‹œ Fluxì—ì„œ ì œê³µí•œë‹¤:julia> model2 = Chain(\n         Dense(10, 5, Ïƒ),\n         Dense(5, 2),\n         softmax)\nChain(Dense(10, 5, NNlib.Ïƒ), Dense(5, 2), NNlib.softmax)\n\njulia> model2(rand(10)) # => 2-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°\nTracked 2-element Array{Float64,1}:\n 0.172085\n 0.827915ê³ ì˜¤ê¸‰ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°™ì•„ ë³´ì¸ë‹¤; ì–´ëŠë§Œí¼ ê°„ë‹¨í•˜ê²Œ ì¶”ìƒí™” í•˜ëŠ”ì§€ ë³´ì•˜ì„ ê²ƒì´ë‹¤. ì¤„ë¦¬ì•„ ì½”ë“œì˜ ê°•ë ¥í•¨ì„ ë†“ì¹˜ì§€ ì•Šì•˜ë‹¤.ì´ëŸ° ì ‘ê·¼ë²•ì˜ ì¢‹ì€ ì ì€ \"ëª¨ë¸\"ì´ í•¨ìˆ˜ë¼ëŠ” ê²ƒì´ë‹¤ (í›ˆë ¨ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜), í•¨ìˆ˜ í•©ì„±(âˆ˜) ë˜í•œ ê°€ëŠ¥í•˜ë‹¤.julia> m = Dense(5, 2) âˆ˜ Dense(10, 5, Ïƒ)\n(::#55) (generic function with 1 method)\n\njulia> m(rand(10))\nTracked 2-element Array{Float64,1}:\n -1.28749\n -0.202492ë§ˆì°¬ê°€ì§€ë¡œ, Chainì€ ì¤„ë¦¬ì•„ í•¨ìˆ˜ì™€ ì´ì˜ê²Œ ë™ìž‘í•œë‹¤.julia> m = Chain(x -> x^2, x -> x+1)\nChain(#3, #4)\n\njulia> m(5) # => 26\n26"
},

{
    "location": "Flux/models/basics/#ë ˆì´ì–´-ë„ìš°ë¯¸ë“¤-1",
    "page": "ê¸°ë³¸ì ì¸ ê²ƒ",
    "title": "ë ˆì´ì–´ ë„ìš°ë¯¸ë“¤",
    "category": "section",
    "text": "FluxëŠ” ì‚¬ìš©ìžì˜ ì»¤ìŠ¤í…€ ë ˆì´ì–´ë¥¼ ë„ì™€ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì œê³µí•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œí•˜ë©´julia> Flux.treelike(Affine)\nadapt (generic function with 1 method)Affine ë ˆì´ì–´ì— ë¶€ê°€ì ì¸ ìœ ìš©í•œ ê¸°ëŠ¥ì´ ì¶”ê°€ëœë‹¤, íŒŒë¼ë¯¸í„° ëª¨ìœ¼ê¸°(collecting)ë‚˜ GPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ê°™ì€ ìž‘ì—…ì„ í•  ìˆ˜ ìžˆë‹¤."
},

{
    "location": "Flux/models/recurrence/#",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ìˆœí™˜(Recurrence)",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/recurrence/#ìˆœí™˜-ëª¨ë¸(Recurrent-Models)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ìˆœí™˜ ëª¨ë¸(Recurrent Models)",
    "category": "section",
    "text": ""
},

{
    "location": "Flux/models/recurrence/#ê¸°ì–µ-ì„¸í¬(Recurrent-Cells,-ìˆœí™˜-ì…€,-ë‡Œë¥¼-ëª¨ë°©í•œ-ê±°)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ê¸°ì–µ ì„¸í¬(Recurrent Cells, ìˆœí™˜ ì…€, ë‡Œë¥¼ ëª¨ë°©í•œ ê±°)",
    "category": "section",
    "text": "ë‹¨ìˆœí•œ í”¼ë“œí¬ì›Œë“œ(feedforward, ì‚¬ì´í´(cycle)ì´ë‚˜ ë£¨í”„(loop)ê°€ ì—†ëŠ” ë„¤íŠ¸ì›Œí¬) ê²½ìš°, ëª¨ë¸ mì€ ì—¬ëŸ¬ ê°œì˜ ìž…ë ¥ xáµ¢ì— ëŒ€í•œ yáµ¢ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë‹¤. (ì˜ˆë¥¼ ë“¤ì–´, xë¥¼ MNIST ìˆ«ìžë¼ ì¹˜ë©´ yëŠ” ê·¸ê²ƒì„ ë¶„ë¥˜í•œ ìˆ«ìž.) ì˜ˆì¸¡ì€ ì„œë¡œ ì™„ì „ížˆ ë…ë¦½ì ì´ë©° xê°€ ê°™ìœ¼ë©´ yë„ ì–¸ì œë‚˜ ë™ì¼í•˜ë‹¤.yâ‚ = f(xâ‚)\nyâ‚‚ = f(xâ‚‚)\nyâ‚ƒ = f(xâ‚ƒ)\n# ...ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ëŠ” ížˆë“  ìƒíƒœ(hidden state, ìˆ¨ê²¨ë…¼ ìƒíƒœ)ê°€ ì¡´ìž¬í•˜ë©° ëª¨ë¸ì„ ëŒë¦´ ë•Œ ë§¤ë²ˆ ê·¸ ìƒíƒœë¥¼ ë‹¤ìŒìœ¼ë¡œ ë„˜ê¸´ë‹¤. ê·¸ëž˜ì„œ ì´ ëª¨ë¸ì€ ê·¸ë•Œë§ˆë‹¤ ì´ì „ hë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ê³ , ìƒˆë¡œìš´ hë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚´ ë†“ëŠ”ë‹¤.h = # ... ì´ˆê¸° ìƒíƒœ ...\nh, yâ‚ = f(h, xâ‚)\nh, yâ‚‚ = f(h, xâ‚‚)\nh, yâ‚ƒ = f(h, xâ‚ƒ)\n# ...hì— ì €ìž¥í•œ ì •ë³´ëŠ” ë‹¤ìŒë²ˆ ì˜ˆì¸¡ì„ ìœ„í•´ ìœ ì§€í•˜ê³ , ê·¸ëž˜ì„œ ë§ˆì¹˜ í•¨ìˆ˜ì˜ ë©”ëª¨ë¦¬ ê°™ì€ ì—­í• ì„ í•˜ê²Œ í•œë‹¤. ì´ê²ƒì€ xì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì´ì „ê¹Œì§€ ëª¨ë¸ì— ì£¼ìž…í•œ ëª¨ë“  ìž…ë ¥ìœ¼ë¡œë¶€í„° ì˜í–¥ì„ ë°›ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.(ìš”ê±°ëŠ” ì¤‘ìš”í•œ ê±°ë‹ˆê¹Œ ì˜ˆë¥¼ ë“¤ì–´, xë¥¼ ë¬¸ìž¥ì—ì„œì˜ í•œ ë‹¨ì–´ë¼ ë³´ìž; ë§Œì•½ì— \"bank\"ë¼ëŠ” ì˜ì–´ ë‹¨ì–´ê°€ ì£¼ì–´ì§€ë©´ ëª¨ë¸ì€ ì´ì „ ìž…ë ¥ì´ \"ê°• river\" ì´ë©´ ê°•ë‘‘ìœ¼ë¡œ, \"íˆ¬ìž investment\"ë©´ ì€í–‰ìœ¼ë¡œ í•´ì„í•´ì•¼ í•œë‹¤.)Fluxì˜ RNN ì§€ì›ì€ ìˆ˜í•™ì  ê´€ì ì„ ì§€ë‹ˆê³  ìžˆë‹¤. ê°€ìž¥ ê¸°ë³¸ì´ ë˜ëŠ” RNNì€ í‘œì¤€ \"Dense\" ë ˆì´ì–´ë¥¼ ë”°ë¥´ê³ , ê·¸ ì¶œë ¥ì€ ížˆë“  ìƒíƒœì´ë‹¤.julia> using Flux\n\njulia> Wxh = randn(5, 10)\n5Ã—10 Array{Float64,2}:\n -0.197167   0.0931036  -1.13283   â€¦   0.426711   1.5678      0.488363\n -1.19948   -1.05618     1.057        -1.85708    2.05188    -0.732148\n -0.848823   0.147774    1.66139      -0.777346  -0.0650354   0.36015\n -0.380701   0.737349    0.426964      0.694122  -1.46597    -1.00572\n -0.789044  -0.374745   -0.996698      0.505453  -0.117276    1.35148\n\njulia> Whh = randn(5, 5)\n5Ã—5 Array{Float64,2}:\n -1.12946    -0.523065   0.0547692  -0.305124  -0.105809\n -0.195351    0.588007   0.616959    0.779213  -0.145329\n -0.265139   -0.535485  -0.300887    2.13263   -1.53089\n -0.0537235  -1.47912   -0.883858    0.993426  -0.354738\n  0.486817    0.170843   0.0440353   0.177502   0.730423\n\njulia> b   = randn(5)\n5-element Array{Float64,1}:\n  0.982592\n -0.724775\n  0.118081\n  0.140369\n -1.07578\n\njulia> function rnn(h, x)\n         h = tanh.(Wxh * x .+ Whh * h .+ b)\n         return h, h\n       end\nrnn (generic function with 1 method)\n\njulia> x = rand(10) # ë”ë¯¸ ë°ì´í„°\n10-element Array{Float64,1}:\n 0.312436\n 0.384043\n 0.972045\n 0.194086\n 0.496317\n 0.654925\n 0.0311892\n 0.494105\n 0.338846\n 0.204689\n\njulia> h = rand(5)  # ì´ˆê¸° ížˆë“  ìƒíƒœ\n5-element Array{Float64,1}:\n 0.861124\n 0.994686\n 0.560054\n 0.371721\n 0.159454\n\njulia> h, y = rnn(h, x)\n([-0.963817, -0.198195, 0.903936, -0.686608, -0.839093], [-0.963817, -0.198195, 0.903936, -0.686608, -0.839093])ë§ˆì§€ë§‰ rnnì„ ì¢€ ë” ëŒë ¤ë³´ë©´, ì¶œë ¥ yëŠ” ìž…ë ¥ xê°€ ê°™ì€ë°ë„ ì¡°ê¸ˆì”© ë°”ë€ŒëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìžˆë‹¤.julia> h, y = rnn(h, x)\n([0.812906, -0.767065, 0.945139, 0.0198447, -0.996763], [0.812906, -0.767065, 0.945139, 0.0198447, -0.996763])\n\njulia> h, y = rnn(h, x)\n([-0.647084, -0.799032, 0.997557, 0.902798, -0.984697], [-0.647084, -0.799032, 0.997557, 0.902798, -0.984697])ì•žì„œ ì–¸ê¸‰í•œ rnn í•¨ìˆ˜ëŠ” ëª…ì‹œì ìœ¼ë¡œ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ê¸°ì–µ ì„¸í¬(cells) ì´ë‹¤. ë‹¤ì–‘í•œ ê¸°ì–µ ì„¸í¬ê°€ ì¡´ìž¬í•˜ë©° ë ˆì´ì–´ ì°¸ì¡°ì— ê´€ë ¨ ë‚´ìš©ì´ ìžˆë‹¤. ìœ„ì˜ ì˜ˆì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿€ ìˆ˜ ìžˆë‹¤:julia> using Flux\n\njulia> rnn2 = Flux.RNNCell(10, 5)\nRNNCell(10, 5, tanh)\n\njulia> x = rand(10) # ë”ë¯¸ ë°ì´í„°\n10-element Array{Float64,1}:\n 0.142406\n 0.944597\n 0.973233\n 0.434782\n 0.715639\n 0.763562\n 0.280661\n 0.293604\n 0.496457\n 0.173372\n\njulia> h = rand(5)  # ì´ˆê¸° ížˆë“  ìƒíƒœ\n5-element Array{Float64,1}:\n 0.602545\n 0.998396\n 0.558707\n 0.637564\n 0.0313308\n\njulia> h, y = rnn2(h, x)\n(param([-0.160217, -0.741263, 0.048164, 0.963063, 0.0301785]), param([-0.160217, -0.741263, 0.048164, 0.963063, 0.0301785]))"
},

{
    "location": "Flux/models/recurrence/#ìƒíƒœë¥¼-ê°–ëŠ”-ëª¨ë¸-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ìƒíƒœë¥¼ ê°–ëŠ” ëª¨ë¸",
    "category": "section",
    "text": "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, ížˆë“  ìƒíƒœë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” ê±°ëŠ” ê·€ì°®ìœ¼ë‹ˆê¹Œ ëª¨ë¸ì´ ìƒíƒœë¥¼ ê°–ê²Œë” ì²˜ë¦¬í•  ìˆ˜ ìžˆë‹¤. FluxëŠ” Recur ëž˜í¼ë¥¼ ì œê³µí•œë‹¤.julia> x = rand(10)\n10-element Array{Float64,1}:\n 0.165593\n 0.502313\n 0.120926\n 0.505827\n 0.917068\n 0.557163\n 0.688472\n 0.791826\n 0.0838632\n 0.709302\n\njulia> h = rand(5)\n5-element Array{Float64,1}:\n 0.40008\n 0.48858\n 0.551568\n 0.0688404\n 0.0583865\n\njulia> m = Flux.Recur(rnn, h)\nRecur(rnn)\n\njulia> y = m(x)\n5-element Array{Float64,1}:\n  0.963414\n -0.999974\n  0.739107\n  0.976241\n  0.986023Recur ëž˜í¼ëŠ” m.state í•„ë“œì— ìƒíƒœë¥¼ ì €ìž¥í•œë‹¤.RNN(10, 5) ìƒì„±ìžë¥¼ ì‚¬ìš©í•˜ë©´ - RNNCellê³¼ ëŒ€ì‘í•˜ëŠ” - ë‹¤ìŒê³¼ ê°™ì´ ì´ê±°ëŠ” ë‹¨ìˆœížˆ ëž˜í¼ ì…€ì´ë‹¤.julia> RNN(10, 5)\nRecur(RNNCell(10, 5, tanh))"
},

{
    "location": "Flux/models/recurrence/#ì‹œí€€ìŠ¤(Sequences,-ì—°ì†ë˜ëŠ”-ê°’)-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ì‹œí€€ìŠ¤(Sequences, ì—°ì†ë˜ëŠ” ê°’)",
    "category": "section",
    "text": "ì¢…ì¢… ê°œë³„ì ì¸ x ë³´ë‹¤ëŠ” ì—°ì†ë˜ëŠ” ìž…ë ¥ì„ ë‹¤ë£¨ê¸¸ ì›í•œë‹¤.julia> seq = [rand(10) for i = 1:10]\n10-element Array{Array{Float64,1},1}:\n [0.443911, 0.955247, 0.980153, 0.313181, 0.0426581, 0.354755, 0.113961, 0.222873, 0.865114, 0.14094]\n [0.50466, 0.0204917, 0.890547, 0.574102, 0.301098, 0.944295, 0.95414, 0.36809, 0.341546, 0.474998]\n [0.474114, 0.152628, 0.364967, 0.601978, 0.212361, 0.66016, 0.12101, 0.944988, 0.417781, 0.715282]\n [0.0776375, 0.843099, 0.000618674, 0.352273, 0.977611, 0.801756, 0.550702, 0.311638, 0.285711, 0.0856441]\n [0.603498, 0.863035, 0.89494, 0.506224, 0.840984, 0.13453, 0.43549, 0.216554, 0.361081, 0.0965758]\n [0.236062, 0.407028, 0.357854, 0.875694, 0.0468227, 0.786622, 0.616748, 0.791976, 0.800668, 0.147169]\n [0.739452, 0.38329, 0.961215, 0.113691, 0.381309, 0.57526, 0.0170709, 0.403656, 0.445509, 0.051497]\n [0.956629, 0.624735, 0.14811, 0.202354, 0.484018, 0.250409, 0.0352729, 0.809209, 0.831828, 0.826355]\n [0.388553, 0.42596, 0.736068, 0.454156, 0.626974, 0.641246, 0.444018, 0.768584, 0.118879, 0.416568]\n [0.307721, 0.176393, 0.371934, 0.714272, 0.886859, 0.333667, 0.721609, 0.975586, 0.59609, 0.771424]Recurë¡œ ëª¨ë¸ì„ ì‹œí€€ìŠ¤ì˜ ê° í•­ëª©ë§ˆë‹¤ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìžˆë‹¤:julia> m.(seq) # 5-ì—˜ëŸ¬ë¨¼íŠ¸ ë²¡í„°ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ëŒë ¤ì¤€ë‹¤\n10-element Array{Array{Float64,1},1}:\n [0.958516, -0.996974, 0.640934, -0.440203, 0.991754]\n [0.998417, -0.998238, 0.988128, 0.924522, 0.999099]\n [0.943455, -0.999939, 0.94332, 0.638572, 0.999795]\n [0.997841, -0.999912, 0.414106, 0.705974, 0.999871]\n [0.9896, -0.96634, 0.903348, 0.805409, 0.949429]\n [0.990047, -0.999849, 0.991448, 0.950895, 0.999938]\n [0.980617, -0.988072, 0.978565, -0.785643, 0.985682]\n [0.98617, -0.99938, -0.791134, 0.603178, 0.0937938]\n [0.946547, -0.893022, 0.914559, 0.999905, 0.984556]\n [0.989439, -0.999979, 0.964896, 0.978421, 0.999834]ë” ì»¤ë‹¤ëž€ ëª¨ë¸ì— ìˆœí™˜ ë ˆì´ì–´(recurrent layers)ë¥¼ ì—°ì‡„ì (chain)ìœ¼ë¡œ ì—°ê²° í•  ìˆ˜ ìžˆë‹¤.julia> m = Chain(LSTM(10, 15), Dense(15, 5))\nChain(Recur(LSTMCell(10, 60)), Dense(15, 5))\n\njulia> m.(seq)\n10-element Array{TrackedArray{â€¦,Array{Float64,1}},1}:\n param([0.0779735, 0.0534096, -0.0245852, -0.0699291, -0.00650743])\n param([0.203825, -0.0307184, -0.0940759, -0.100437, 0.0523315])\n param([0.21071, -0.19635, -0.106985, -0.185204, 0.132647])\n param([0.314643, -0.205525, -0.00144219, -0.165195, 0.197256])\n param([0.351024, -0.116196, 0.00489051, -0.255343, 0.209503])\n param([0.370406, -0.125797, -0.0506301, -0.253045, 0.179001])\n param([0.349787, -0.091392, -0.0699977, -0.249944, 0.197391])\n param([0.370064, -0.21158, -0.00144108, -0.337597, 0.24153])\n param([0.396285, -0.240793, -0.0263459, -0.358695, 0.260678])\n param([0.464372, -0.316526, -0.0295575, -0.352548, 0.251627])"
},

{
    "location": "Flux/models/recurrence/#ê¸°ìš¸ê¸°-ìž˜ë¼ë‚´ê¸°-1",
    "page": "ìˆœí™˜(Recurrence)",
    "title": "ê¸°ìš¸ê¸° ìž˜ë¼ë‚´ê¸°",
    "category": "section",
    "text": "ê¸°ë³¸ì ìœ¼ë¡œ, ìˆœí™˜ ë ˆì´ì–´ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì „ì²´ ê¸°ë¡(history)ì„ ë‚´í¬í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 100ê°œì˜ ìž…ë ¥ì„ ê°€ì§„ ëª¨ë¸ì„ ì‹¤í–‰í•  ë•Œ, back!ì„ í•˜ë©´ 100ê°œì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤. ê·¸ëŸ¬ê³  ë‹¤ë¥¸ 10ê°œì˜ ìž…ë ¥ì„ ë” ê³„ì‚°í•œë‹¤ë©´ 110ê°œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤ - ì´ê±°ëŠ” ëˆ„ì ë˜ë¯€ë¡œ ë¹ ë¥´ê²Œ ì—°ì‚° ë¹„ìš©ì´ ì¦ê°€í•œë‹¤.ì´ê±°ë¥¼ ë§‰ëŠ” ë°©ë²•ì€ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ìž˜ë¼ë‚´ì–´(truncate) ê¸°ë¡ì„ ì§€ì›Œì£¼ëŠ” ê²ƒì´ë‹¤.julia> Flux.truncate!(m)\ntruncate!ì„ í˜¸ì¶œí•˜ë©´ ê¹”ë”ì´ ì²­ì†Œí•´ ì¤€ë‹¤. ê·¸ëž˜ì„œ ë” ë§Žì€ ìž…ë ¥ì˜ ëª¨ë¸ì„ ì‹¤í–‰í•´ë„ ë¹„ì‹¼ ê¸°ìš¸ê¸° ì—°ì‚° ì—†ì´ í•´ë‚¼ ìˆ˜ ìžˆë‹¤.truncate!ëŠ” ì—¬ëŸ¬ ê°œì˜ ì»¤ë‹¤ëž€ ì‹œí€€ìŠ¤ ë©ì–´ë¦¬ë¥¼ ë‹¤ë£° ë•Œ ìœ ìš©í•˜ì§€ë§Œ, ì„œë¡œ ë…ë¦½ì ì¸ ì‹œí€€ìŠ¤ë“¤ì„ ë‹¤ë£¨ê³  ì‹¶ì„ ë•Œë„ ìžˆë‹¤. ê·¸ ê²½ìš° ížˆë“  ìƒíƒœëŠ” ì›ëž˜ ê°’ìœ¼ë¡œ ì™„ì „ížˆ ì´ˆê¸°í™” ë˜ì–´ ëˆ„ì ëœ ì •ë³´ë¥¼ ë²„ë¦°ë‹¤. ê·¸ë ‡ê²Œ í•˜ê³  ì‹¶ìœ¼ë©´ reset!ì„ í•´ ì£¼ìž.julia> Flux.reset!(m)\n"
},

{
    "location": "Flux/models/regularisation/#",
    "page": "ì •ê·œí™”(Regularisation)",
    "title": "ì •ê·œí™”(Regularisation)",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/regularisation/#ì •ê·œí™”(Regularisation)-1",
    "page": "ì •ê·œí™”(Regularisation)",
    "title": "ì •ê·œí™”(Regularisation)",
    "category": "section",
    "text": "ì´ë²ˆì—ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì •ê·œí™” í•´ ë³´ìž. normê³¼ ê°™ì€ ì •ê·œí™”ë¥¼ í•´ì£¼ëŠ” ì ì ˆí•œ í•¨ìˆ˜ë¥¼ ê° ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì ìš©í•˜ì—¬ ê·¸ ê²°ê³¼ë¥¼ ëª¨ë“  lossì— ë”í•˜ë„ë¡ í•˜ìž.ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ ê°„ë‹¨í•œ regressionì„ ë³´ìž.julia> using Flux\n\njulia> m = Dense(10, 5)\nDense(10, 5)\n\njulia> loss(x, y) = Flux.crossentropy(softmax(m(x)), y)\nloss (generic function with 1 method)m.Wì™€ m.b íŒŒë¼ë¯¸í„°ì— L2 normì„ ì·¨í•˜ì—¬ ì •ê·œí™” í•´ë³´ìž.julia> penalty() = norm(m.W) + norm(m.b)\npenalty (generic function with 1 method)\n\njulia> loss(x, y) = Flux.crossentropy(softmax(m(x)), y) + penalty()\nloss (generic function with 1 method)ë ˆì´ì–´ë¥¼ ì´ìš©í•˜ëŠ” ê²½ìš°, FluxëŠ” params í•¨ìˆ˜ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìžˆë‹¤. sum(norm, params)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ë¥¼ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìžˆë‹¤.julia> params(m)\n2-element Array{Any,1}:\n param([-0.61839 -0.556047 â€¦ -0.460808 -0.107646; 0.346293 -0.375076 â€¦ -0.608704 -0.181025; â€¦ ; -0.2226 -0.0992159 â€¦ 0.0707984 -0.429173; -0.331058 -0.291995 â€¦ 0.383368 0.156716])\n param([0.0, 0.0, 0.0, 0.0, 0.0])\n\njulia> sum(norm, params(m))\n2.4130860599427706 (tracked)ì¢€ ë” í° ê·œëª¨ì˜ ì˜ˆë¡œ, ë©€í‹°-ë ˆì´ì–´ í¼ì…‰íŠ¸ë¡ (perceptron)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.julia> m = Chain(\n         Dense(28^2, 128, relu),\n         Dense(128, 32, relu),\n         Dense(32, 10), softmax)\nChain(Dense(784, 128, NNlib.relu), Dense(128, 32, NNlib.relu), Dense(32, 10), NNlib.softmax)\n\njulia> loss(x, y) = Flux.crossentropy(m(x), y) + sum(norm, params(m))\nloss (generic function with 1 method)\n\njulia> loss(rand(28^2), rand(10))\n39.128892409412174 (tracked)"
},

{
    "location": "Flux/models/layers/#",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/models/layers/#ê¸°ë³¸-ë ˆì´ì–´-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ê¸°ë³¸ ë ˆì´ì–´",
    "category": "section",
    "text": "ê±°ì˜ ëª¨ë“  ì‹ ê²½ë§(neural networks)ì˜ í† ëŒ€ë¥¼ ë‹¤ìŒì˜ í•µì‹¬ ë ˆì´ì–´ë¡œ êµ¬ì„±í•œë‹¤.Chain\nDense\nConv2D"
},

{
    "location": "Flux/models/layers/#ìˆœí™˜-ë ˆì´ì–´(Recurrent-Layers)-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ìˆœí™˜ ë ˆì´ì–´(Recurrent Layers)",
    "category": "section",
    "text": "ìœ„ì˜ í•µì‹¬ ë ˆì´ì–´ì™€ í•¨ê»˜, ì‹œí€€ìŠ¤ ë°ì´í„°(ë‹¤ë¥¸ ì¢…ë¥˜ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°)ë¥¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.RNN\nLSTM\nFlux.Recur"
},

{
    "location": "Flux/models/layers/#í™œì„±-í•¨ìˆ˜(Activation-Functions)-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "í™œì„± í•¨ìˆ˜(Activation Functions)",
    "category": "section",
    "text": "ëª¨ë¸ì˜ ë ˆì´ì–´ ì¤‘ê°„ì— ë¹„ì„ í˜•ì„±(Non-linearities)ì„ ê°–ì„ ë•Œ ì‚¬ìš©í•œë‹¤. í•¨ìˆ˜ì˜ ëŒ€ë¶€ë¶„ì€ NNlibì— ì •ì˜ë˜ì–´ ìžˆê³  Fluxì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.íŠ¹ë³„í•œ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ í™œì„± í•¨ìˆ˜ëŠ” ë³´í†µ ìŠ¤ì¹¼ë¼(scalars) ê°’ì„ ì²˜ë¦¬í•œë‹¤. ë°°ì—´ì— ì ìš©í•˜ë ¤ë©´ Ïƒ.(xs), relu.(xs) ì²˜ëŸ¼ .ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŒ… í•´ ì£¼ìž.Ïƒ\nrelu\nleakyrelu\nelu\nswish"
},

{
    "location": "Flux/models/layers/#ì •ìƒí™”(Normalisation)-and-ì •ê·œí™”(Regularisation)-1",
    "page": "ëª¨ë¸ ì°¸ì¡°(Model Reference)",
    "title": "ì •ìƒí™”(Normalisation) & ì •ê·œí™”(Regularisation)",
    "category": "section",
    "text": "ì´ ë ˆì´ì–´ë“¤ì€ ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì¡°ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ í›ˆë ¨ ì‹œê°„(training times)ì˜ ê°œì„  ê·¸ë¦¬ê³  overfitting(ì˜¤ë²„í”¼íŒ…, ê³¼ì í•©)ì„ ì¤„ì—¬ ì¤€ë‹¤.Flux.testmode!\nBatchNorm\nDropout\nLayerNorm"
},

{
    "location": "Flux/training/optimisers/#",
    "page": "Optimisers",
    "title": "Optimisers",
    "category": "page",
    "text": "ë‹¤ìŒ ë˜¥ íƒ€ìž„"
},

{
    "location": "Flux/training/optimisers/#Optimisers-1",
    "page": "Optimisers",
    "title": "Optimisers",
    "category": "section",
    "text": "Consider a simple linear regression. We create some dummy data, calculate a loss, and backpropagate to calculate gradients for the parameters W and b.W = param(rand(2, 5))\nb = param(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = rand(5), rand(2) # Dummy data\nl = loss(x, y) # ~ 3\nback!(l)We want to update each parameter, using the gradient, in order to improve (reduce) the loss. Here's one way to do that:function update()\n  Î· = 0.1 # Learning Rate\n  for p in (W, b)\n    p.data .-= Î· .* p.grad # Apply the update\n    p.grad .= 0            # Clear the gradient\n  end\nendIf we call update, the parameters W and b will change and our loss should go down.There are two pieces here: one is that we need a list of trainable parameters for the model ([W, b] in this case), and the other is the update step. In this case the update is simply gradient descent (x .-= Î· .* Î”), but we might choose to do something more advanced, like adding momentum.In this case, getting the variables is trivial, but you can imagine it'd be more of a pain with some complex stack of layers.m = Chain(\n  Dense(10, 5, Ïƒ),\n  Dense(5, 2), softmax)Instead of having to write [m[1].W, m[1].b, ...], Flux provides a params function params(m) that returns a list of all parameters in the model for you.For the update step, there's nothing whatsoever wrong with writing the loop above â€“ it'll work just fine â€“ but Flux provides various optimisers that make it more convenient.opt = SGD([W, b], 0.1) # Gradient descent with learning rate 0.1\n\nopt() # Carry out the update, modifying `W` and `b`.An optimiser takes a parameter list and returns a function that does the same thing as update above. We can pass either opt or update to our training loop, which will then run the optimiser after every mini-batch of data."
},

{
    "location": "Flux/training/optimisers/#Optimiser-Reference-1",
    "page": "Optimisers",
    "title": "Optimiser Reference",
    "category": "section",
    "text": "All optimisers return a function that, when called, will update the parameters passed to it.SGD\nMomentum\nNesterov\nADAM"
},

{
    "location": "Flux/training/training/#",
    "page": "Training",
    "title": "Training",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/training/training/#Training-1",
    "page": "Training",
    "title": "Training",
    "category": "section",
    "text": "To actually train a model we need three things:A objective function, that evaluates how well a model is doing given some input data.\nA collection of data points that will be provided to the objective function.\nAn optimiser that will update the model parameters appropriately.With these we can call Flux.train!:Flux.train!(objective, data, opt)There are plenty of examples in the model zoo."
},

{
    "location": "Flux/training/training/#Loss-Functions-1",
    "page": "Training",
    "title": "Loss Functions",
    "category": "section",
    "text": "The objective function must return a number representing how far the model is from its target â€“ the loss of the model. The loss function that we defined in basics will work as an objective. We can also define an objective in terms of some model:m = Chain(\n  Dense(784, 32, Ïƒ),\n  Dense(32, 10), softmax)\n\nloss(x, y) = Flux.mse(m(x), y)\n\n# later\nFlux.train!(loss, data, opt)The objective will almost always be defined in terms of some cost function that measures the distance of the prediction m(x) from the target y. Flux has several of these built in, like mse for mean squared error or crossentropy for cross entropy loss, but you can calculate it however you want."
},

{
    "location": "Flux/training/training/#Datasets-1",
    "page": "Training",
    "title": "Datasets",
    "category": "section",
    "text": "The data argument provides a collection of data to train with (usually a set of inputs x and target outputs y). For example, here's a dummy data set with only one data point:x = rand(784)\ny = rand(10)\ndata = [(x, y)]Flux.train! will call loss(x, y), calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:data = [(x, y), (x, y), (x, y)]\n# Or equivalently\ndata = Iterators.repeated((x, y), 3)It's common to load the xs and ys separately. In this case you can use zip:xs = [rand(784), rand(784), rand(784)]\nys = [rand( 10), rand( 10), rand( 10)]\ndata = zip(xs, ys)Note that, by default, train! only loops over the data once (a single \"epoch\"). A convenient way to run multiple epochs from the REPL is provided by @epochs.julia> using Flux: @epochs\n\njulia> @epochs 2 println(\"hello\")\nINFO: Epoch 1\nhello\nINFO: Epoch 2\nhello\n\njulia> @epochs 2 Flux.train!(...)\n# Train for two epochs"
},

{
    "location": "Flux/training/training/#Callbacks-1",
    "page": "Training",
    "title": "Callbacks",
    "category": "section",
    "text": "train! takes an additional argument, cb, that's used for callbacks so that you can observe the training process. For example:train!(objective, data, opt, cb = () -> println(\"training\"))Callbacks are called for every batch of training data. You can slow this down using Flux.throttle(f, timeout) which prevents f from being called more than once every timeout seconds.A more typical callback might look like this:test_x, test_y = # ... create single batch of test data ...\nevalcb() = @show(loss(test_x, test_y))\n\nFlux.train!(objective, data, opt,\n            cb = throttle(evalcb, 5))"
},

{
    "location": "Flux/data/onehot/#",
    "page": "One-Hot Encoding",
    "title": "One-Hot Encoding",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/data/onehot/#One-Hot-Encoding-1",
    "page": "One-Hot Encoding",
    "title": "One-Hot Encoding",
    "category": "section",
    "text": "It's common to encode categorical variables (like true, false or cat, dog) in \"one-of-k\" or \"one-hot\" form. Flux provides the onehot function to make this easy.julia> using Flux: onehot\n\njulia> onehot(:b, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n  true\n false\n\njulia> onehot(:c, [:a, :b, :c])\n3-element Flux.OneHotVector:\n false\n false\n  trueThe inverse is argmax (which can take a general probability distribution, as well as just booleans).julia> argmax(ans, [:a, :b, :c])\n:c\n\njulia> argmax([true, false, false], [:a, :b, :c])\n:a\n\njulia> argmax([0.3, 0.2, 0.5], [:a, :b, :c])\n:c"
},

{
    "location": "Flux/data/onehot/#Batches-1",
    "page": "One-Hot Encoding",
    "title": "Batches",
    "category": "section",
    "text": "onehotbatch creates a batch (matrix) of one-hot vectors, and argmax treats matrices as batches.julia> using Flux: onehotbatch\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3Ã—3 Flux.OneHotMatrix:\n false   true  false\n  true  false   true\n false  false  false\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Array{Symbol,1}:\n  :b\n  :a\n  :bNote that these operations returned OneHotVector and OneHotMatrix rather than Arrays. OneHotVectors behave like normal vectors but avoid any unnecessary cost compared to using an integer index directly. For example, multiplying a matrix with a one-hot vector simply slices out the relevant row of the matrix under the hood."
},

{
    "location": "Flux/gpu/#",
    "page": "GPU Support",
    "title": "GPU Support",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/gpu/#GPU-Support-1",
    "page": "GPU Support",
    "title": "GPU Support",
    "category": "section",
    "text": "Support for array operations on other hardware backends, like GPUs, is provided by external packages like CuArrays and CLArrays. Flux doesn't care what array type you use, so we can just plug these in without any other changes.For example, we can use CuArrays (with the cu converter) to run our basic example on an NVIDIA GPU.using CuArrays\n\nW = cu(rand(2, 5)) # a 2Ã—5 CuArray\nb = cu(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = cu(rand(5)), cu(rand(2)) # Dummy data\nloss(x, y) # ~ 3Note that we convert both the parameters (W, b) and the data set (x, y) to cuda arrays. Taking derivatives and training works exactly as before.If you define a structured model, like a Dense layer or Chain, you just need to convert the internal parameters. Flux provides mapleaves, which allows you to alter all parameters of a model at once.d = Dense(10, 5, Ïƒ)\nd = mapleaves(cu, d)\nd.W # Tracked CuArray\nd(cu(rand(10))) # CuArray output\n\nm = Chain(Dense(10, 5, Ïƒ), Dense(5, 2), softmax)\nm = mapleaves(cu, m)\nd(cu(rand(10)))The mnist example contains the code needed to run the model on the GPU; just uncomment the lines after using CuArrays."
},

{
    "location": "Flux/community/#",
    "page": "Community",
    "title": "Community",
    "category": "page",
    "text": ""
},

{
    "location": "Flux/community/#Community-1",
    "page": "Community",
    "title": "Community",
    "category": "section",
    "text": "All Flux users are welcome to join our community on the Julia forum, the slack (channel #machine-learning), or Flux's Gitter. If you have questions or issues we'll try to help you out.If you're interested in hacking on Flux, the source code is open and easy to understand â€“ it's all just the same Julia code you work with normally. You might be interested in our intro issues to get started."
},

]}
